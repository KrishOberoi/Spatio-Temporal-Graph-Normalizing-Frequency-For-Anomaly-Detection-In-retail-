{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9698db9e-9d91-4635-b4f0-072a900865b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "easyocr 1.7.2 requires opencv-python-headless, which is not installed.\n",
      "llama-index-llms-openai 0.3.44 requires openai<2,>=1.81.0, but you have openai 1.70.0 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # adjust CUDA if needed\n",
    "!pip install numpy matplotlib scikit-learn tqdm\n",
    "!pip install ultralytics  # if you later want to experiment with your own pose extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba0e9ec-8f9e-4fac-a7d9-e86c2d038494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krish\\PoseLift\\STG-NF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'STG-NF'...\n"
     ]
    }
   ],
   "source": [
    "# In a folder where you keep projects\n",
    "!git clone https://github.com/orhir/STG-NF.git\n",
    "%cd STG-NF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b93f63-f13f-4b22-9902-fb37e9194d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\krish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install numpy scikit-learn tqdm matplotlib networkx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7a7d8c9-fbb5-48fd-ab25-530151922e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\Users\\Krish\\PoseLift\\STG-NF\n",
      "['.git', '.gitignore', 'args.py', 'checkpoints', 'data', 'dataset.py', 'environment.yml', 'gen_data.py', 'LICENSE', 'models', 'README.md', 'train_eval.py', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# See where the notebook is running\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "# List contents here\n",
    "print(os.listdir(\".\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f28462ba-3551-43e9-af45-adf792807db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.gitignore', 'args.py', 'checkpoints', 'data', 'dataset.py', 'environment.yml', 'gen_data.py', 'LICENSE', 'models', 'README.md', 'STG-NF', 'train_eval.py', 'utils']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'STG-NF'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/orhir/STG-NF.git\n",
    "print(os.listdir(\".\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b46b4dfd-b2f3-4beb-a974-46c71fecde06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krish\\PoseLift\\STG-NF\\STG-NF\n",
      "Now in: C:\\Users\\Krish\\PoseLift\\STG-NF\\STG-NF\n",
      "['.git', '.gitignore', 'args.py', 'checkpoints', 'data', 'dataset.py', 'environment.yml', 'gen_data.py', 'LICENSE', 'models', 'README.md', 'train_eval.py', 'utils']\n"
     ]
    }
   ],
   "source": [
    "%cd STG-NF\n",
    "import os\n",
    "print(\"Now in:\", os.getcwd())\n",
    "print(os.listdir(\".\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edf0a3cd-d126-4f06-b0c5-2eba00bda965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krish\\PoseLift\\STG-NF\n"
     ]
    }
   ],
   "source": [
    "%cd C:/Users/Krish/PoseLift/STG-NF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbf122ba-be8c-4591-b92f-b5c25e7d52c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(r\"C:/Users/Krish/PoseLift/STG-NF\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50cc589b-d3c1-45f4-b6d2-389434ceeb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: C:\\Users\\Krish\\PoseLift\\STG-NF\\data\\PoseLift\\pose\n",
      "['arch.png', 'PoseLift', 'ShanghaiTech', 'UBnormal']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base = os.getcwd()  # should be STG-NF root\n",
    "pose_dir = os.path.join(base, \"data\", \"PoseLift\", \"pose\")\n",
    "\n",
    "os.makedirs(pose_dir, exist_ok=True)\n",
    "\n",
    "print(\"Created:\", pose_dir)\n",
    "print(os.listdir(os.path.join(base, \"data\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b42b2156-ccac-4050-b764-f14a368770b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAIN FOLDER\n",
      "============================================================\n",
      "Total files: 104\n",
      "First 5 files: ['01_097_alphapose_tracked_person.json', '01_126_alphapose_tracked_person.json', '01_127_alphapose_tracked_person.json', '01_132_alphapose_tracked_person.json', '01_134_alphapose_tracked_person.json']\n",
      "\n",
      "Sample file: 01_097_alphapose_tracked_person.json\n",
      "File size: 536.00 KB\n",
      "Type: <class 'dict'>\n",
      "Structure: ['167', '47', '54', '89', '93']\n",
      "\n",
      "First entry key: 167\n",
      "First entry value: {'1350': {'keypoints': [631.5, 1266.0, 0.712890625, 631.5, 1278.0, 0.71240234375, 624.0, 1270.0, 0.6513671875, 594.0, 1304.0, 0.1063232421875, 582.5, 1304.0, 0.1431884765625, 586.0, 1304.0, 0.054473876953125, 586.0, 1304.0, 0.07830810546875, 628.0, 1304.0, 0.59375, 631.5, 1304.0, 0.288818359375, 597.5, 1274.0, 0.751953125, 597.5, 1274.0, 0.193603515625, 696.0, 1289.0, 0.20947265625, 684.5, 1255.0, 0.338134765625, 752.5, 1274.0, 0.09234619140625, 756.5, 1274.0, 0.275634765625, 794.0, 1198.0, 0.057159423828125, 794.0, 1141.0, 0.09429931640625], 'scores': None}, '1351': {'keypoints': [626.5, 1255.0, 0.71142578125, 622.5, 1270.0, 0.71240234375, 619.0, 1262.0, 0.59619140625, 680.0, 1304.0, 0.1600341796875, 619.0, 1255.0, 0.12005615234375, 722.0, 1274.0, 0.1009521484375, 683.5, 1247.0, 0.2406005859375, 622.5, 1300.0, 0.258056640625, 767.5, 1278.0, 0.060394287109375, 596.0, 1266.0, 0.58056640625, 592.0, 1266.0, 0.166748046875, 683.5, 1297.0, 0.09991455078125, 752.0, 1183.0, 0.12164306640625, 794.0, 1137.0, 0.08917236328125, 790.5, 1137.0, 0.10699462890625, 794.0, 1244.0, 0.05029296875, 794.0, 1251.0, 0.07177734375], 'scores': None}, '1352': {'keypoints': [573.0, 1290.0, 0.250732421875, 550.0, 1305.0, 0.11322021484375, 554.0, 1290.0, 0.1700439453125, 588.0, 1305.0, 0.07843017578125, 554.0, 1305.0, 0.10198974609375, 557.5, 1305.0, 0.0521240234375, 550.0, 1301.0, 0.1949462890625, 618.5, 1294.0, 0.50146484375, 603.5, 1286.0, 0.1734619140625, 592.0, 1256.0, 0.728515625, 588.0, 1260.0, 0.1402587890625, 687.0, 1275.0, 0.3544921875, 668.0, 1237.0, 0.337158203125, 748.0, 1260.0, 0.09832763671875, 733.0, 1191.0, 0.2315673828125, 790.0, 1245.0, 0.07049560546875, 782.5, 1150.0, 0.126220703125], 'scores': None}, '1353': {'keypoints': [572.0, 1279.0, 0.6474609375, 572.0, 1283.0, 0.63037109375, 564.0, 1275.0, 0.53662109375, 580.0, 1302.0, 0.5126953125, 564.0, 1267.0, 0.36181640625, 615.0, 1306.0, 0.126708984375, 587.5, 1255.0, 0.2578125, 611.5, 1275.0, 0.55224609375, 611.5, 1275.0, 0.2388916015625, 587.5, 1243.0, 0.75537109375, 564.0, 1236.0, 0.315185546875, 686.0, 1263.0, 0.2880859375, 666.5, 1228.0, 0.453125, 706.0, 1181.0, 0.16455078125, 710.0, 1185.0, 0.395263671875, 741.0, 1153.0, 0.1849365234375, 741.0, 1149.0, 0.29345703125], 'scores': None}, '1354': {'keypoints': [556.0, 1264.0, 0.1763916015625, 560.5, 1269.0, 0.083251953125, 560.5, 1264.0, 0.1141357421875, 578.0, 1291.0, 0.0797119140625, 495.0, 1295.0, 0.058441162109375, 552.0, 1304.0, 0.404052734375, 543.0, 1295.0, 0.1861572265625, 604.5, 1269.0, 0.7099609375, 595.5, 1269.0, 0.162109375, 578.0, 1234.0, 0.7880859375, 560.5, 1230.0, 0.2183837890625, 657.0, 1230.0, 0.533203125, 665.5, 1264.0, 0.4794921875, 718.0, 1177.0, 0.457763671875, 731.0, 1243.0, 0.2427978515625, 735.5, 1155.0, 0.19140625, 735.5, 1260.0, 0.1572265625], 'scores': None}, '1355': {'keypoints': [443.75, 1294.0, 0.8388671875, 443.75, 1309.0, 0.82177734375, 433.75, 1299.0, 0.89599609375, 448.5, 1319.0, 0.381103515625, 424.0, 1299.0, 0.2476806640625, 508.0, 1304.0, 0.5390625, 508.0, 1319.0, 0.2269287109375, 577.0, 1260.0, 0.424560546875, 587.0, 1314.0, 0.129150390625, 562.0, 1221.0, 0.5341796875, 597.0, 1309.0, 0.2357177734375, 636.0, 1236.0, 0.443603515625, 636.0, 1265.0, 0.350830078125, 720.0, 1176.0, 0.37646484375, 720.0, 1206.0, 0.2225341796875, 730.0, 1162.0, 0.1519775390625, 695.5, 1245.0, 0.1973876953125], 'scores': None}, '1356': {'keypoints': [437.0, 1286.0, 0.87255859375, 432.0, 1296.0, 0.904296875, 426.75, 1286.0, 0.9091796875, 447.5, 1322.0, 0.85595703125, 452.5, 1322.0, 0.2802734375, 488.5, 1286.0, 0.5927734375, 498.75, 1322.0, 0.401123046875, 571.0, 1255.0, 0.377197265625, 555.5, 1296.0, 0.1190185546875, 550.0, 1209.0, 0.8330078125, 596.5, 1286.0, 0.1318359375, 622.0, 1224.0, 0.51708984375, 622.0, 1260.0, 0.41162109375, 699.5, 1173.0, 0.60986328125, 714.5, 1224.0, 0.320068359375, 720.0, 1158.0, 0.1842041015625, 714.5, 1224.0, 0.248046875], 'scores': None}, '1357': {'keypoints': [426.0, 1270.0, 0.89111328125, 420.75, 1281.0, 0.888671875, 415.5, 1276.0, 0.85986328125, 436.5, 1302.0, 0.79833984375, 436.5, 1307.0, 0.60791015625, 483.75, 1270.0, 0.7119140625, 478.5, 1312.0, 0.7314453125, 562.5, 1228.0, 0.70654296875, 541.5, 1286.0, 0.0859375, 531.0, 1197.0, 0.85595703125, 473.25, 1260.0, 0.2288818359375, 594.0, 1228.0, 0.53173828125, 599.0, 1260.0, 0.513671875, 683.0, 1181.0, 0.69384765625, 652.0, 1197.0, 0.371337890625, 715.0, 1155.0, 0.1290283203125, 709.5, 1218.0, 0.42919921875], 'scores': None}, '1358': {'keypoints': [416.0, 1259.0, 0.88916015625, 416.0, 1275.0, 0.90625, 405.25, 1264.0, 0.94580078125, 427.0, 1297.0, 0.796875, 421.5, 1291.0, 0.441162109375, 470.0, 1259.0, 0.8076171875, 464.75, 1297.0, 0.82861328125, 534.5, 1211.0, 0.693359375, 534.5, 1286.0, 0.50341796875, 518.5, 1184.0, 0.87744140625, 572.5, 1264.0, 0.1859130859375, 588.5, 1222.0, 0.5498046875, 588.5, 1248.0, 0.5712890625, 669.5, 1184.0, 0.736328125, 626.5, 1173.0, 0.434814453125, 712.5, 1162.0, 0.301025390625, 712.5, 1195.0, 0.3486328125], 'scores': None}, '1359': {'keypoints': [407.75, 1244.0, 0.89111328125, 407.75, 1255.0, 0.89453125, 402.25, 1250.0, 0.9306640625, 418.75, 1277.0, 0.76171875, 418.75, 1277.0, 0.5595703125, 462.5, 1244.0, 0.779296875, 462.5, 1282.0, 0.83203125, 533.5, 1200.0, 0.64794921875, 517.0, 1282.0, 0.28955078125, 506.25, 1168.0, 0.869140625, 572.0, 1239.0, 0.23291015625, 583.0, 1200.0, 0.54248046875, 583.0, 1233.0, 0.52685546875, 675.5, 1173.0, 0.62158203125, 615.5, 1157.0, 0.277587890625, 708.5, 1162.0, 0.2052001953125, 708.5, 1157.0, 0.285400390625], 'scores': None}, '1360': {'keypoints': [394.75, 1223.0, 0.89306640625, 394.75, 1234.0, 0.91748046875, 389.25, 1228.0, 0.91552734375, 406.0, 1256.0, 0.73974609375, 411.5, 1262.0, 0.6240234375, 455.75, 1223.0, 0.81689453125, 455.75, 1267.0, 0.78759765625, 528.0, 1190.0, 0.677734375, 522.5, 1250.0, 0.309326171875, 494.75, 1151.0, 0.85986328125, 561.0, 1228.0, 0.40478515625, 578.0, 1184.0, 0.54931640625, 572.5, 1223.0, 0.5107421875, 661.0, 1167.0, 0.53955078125, 611.0, 1151.0, 0.7734375, 705.5, 1162.0, 0.2098388671875, 666.5, 1134.0, 0.6396484375], 'scores': None}, '1361': {'keypoints': [382.75, 1203.0, 0.75830078125, 382.75, 1208.0, 0.83447265625, 382.75, 1208.0, 0.69287109375, 399.5, 1214.0, 0.56591796875, 405.25, 1242.0, 0.74853515625, 456.0, 1197.0, 0.759765625, 444.5, 1242.0, 0.859375, 529.0, 1175.0, 0.7705078125, 427.75, 1208.0, 0.2142333984375, 484.0, 1135.0, 0.91162109375, 433.5, 1203.0, 0.469970703125, 574.0, 1169.0, 0.58203125, 568.5, 1197.0, 0.58544921875, 653.0, 1158.0, 0.75, 613.5, 1130.0, 0.483154296875, 703.5, 1152.0, 0.253662109375, 641.5, 1107.0, 0.478271484375], 'scores': None}, '1362': {'keypoints': [378.0, 1183.0, 0.91259765625, 378.0, 1189.0, 0.9091796875, 372.25, 1189.0, 0.85498046875, 389.25, 1206.0, 0.77490234375, 389.25, 1217.0, 0.72998046875, 445.75, 1183.0, 0.77978515625, 440.25, 1217.0, 0.857421875, 513.5, 1155.0, 0.7001953125, 496.75, 1200.0, 0.3125, 479.75, 1121.0, 0.7734375, 542.0, 1189.0, 0.285888671875, 553.5, 1155.0, 0.64208984375, 559.0, 1178.0, 0.6015625, 638.0, 1149.0, 0.6767578125, 610.0, 1115.0, 0.52294921875, 700.5, 1144.0, 0.46826171875, 627.0, 1093.0, 0.61279296875], 'scores': None}, '1363': {'keypoints': [364.5, 1163.0, 0.904296875, 364.5, 1174.0, 0.89404296875, 364.5, 1169.0, 0.9033203125, 381.5, 1186.0, 0.78759765625, 381.5, 1197.0, 0.70654296875, 432.75, 1163.0, 0.76171875, 432.75, 1197.0, 0.8359375, 512.5, 1140.0, 0.8037109375, 489.5, 1186.0, 0.5166015625, 472.5, 1106.0, 0.85205078125, 535.0, 1169.0, 0.376220703125, 546.5, 1135.0, 0.58544921875, 546.5, 1157.0, 0.483154296875, 632.0, 1129.0, 0.5244140625, 609.0, 1101.0, 0.40234375, 694.5, 1129.0, 0.50146484375, 649.0, 1061.0, 0.303466796875], 'scores': None}, '1364': {'keypoints': [355.5, 1154.0, 0.92333984375, 355.5, 1160.0, 0.9189453125, 349.75, 1160.0, 0.748046875, 367.0, 1171.0, 0.75830078125, 372.5, 1188.0, 0.73583984375, 418.25, 1154.0, 0.85302734375, 424.0, 1188.0, 0.8466796875, 498.0, 1126.0, 0.8193359375, 475.25, 1171.0, 0.463623046875, 464.0, 1097.0, 0.87548828125, 452.5, 1114.0, 0.1717529296875, 532.5, 1120.0, 0.58154296875, 532.5, 1148.0, 0.50927734375, 629.5, 1108.0, 0.59765625, 623.5, 1086.0, 0.48828125, 686.5, 1120.0, 0.2568359375, 669.0, 1046.0, 0.18017578125], 'scores': None}, '1365': {'keypoints': [344.5, 1140.0, 0.9345703125, 344.5, 1146.0, 0.904296875, 338.75, 1140.0, 0.92578125, 355.75, 1163.0, 0.83349609375, 355.75, 1169.0, 0.6259765625, 407.0, 1146.0, 0.76611328125, 407.0, 1174.0, 0.85546875, 486.5, 1118.0, 0.79833984375, 458.25, 1140.0, 0.39306640625, 458.25, 1089.0, 0.865234375, 452.5, 1112.0, 0.45947265625, 526.5, 1118.0, 0.61376953125, 526.5, 1135.0, 0.509765625, 612.0, 1084.0, 0.7705078125, 612.0, 1084.0, 0.55224609375, 674.5, 1106.0, 0.3359375, 674.5, 1101.0, 0.1444091796875], 'scores': None}, '1366': {'keypoints': [334.75, 1127.0, 0.9375, 334.75, 1139.0, 0.92138671875, 329.0, 1133.0, 0.8701171875, 346.25, 1150.0, 0.85693359375, 340.5, 1156.0, 0.654296875, 403.5, 1139.0, 0.8037109375, 397.75, 1162.0, 0.81201171875, 478.0, 1122.0, 0.7861328125, 460.75, 1122.0, 0.367431640625, 443.5, 1082.0, 0.83251953125, 443.5, 1093.0, 0.390380859375, 518.0, 1110.0, 0.61376953125, 512.5, 1122.0, 0.53076171875, 592.5, 1059.0, 0.8134765625, 598.5, 1076.0, 0.63037109375, 661.5, 1093.0, 0.650390625, 661.5, 1087.0, 0.4306640625], 'scores': None}, '1367': {'keypoints': [329.75, 1114.0, 0.90185546875, 324.25, 1125.0, 0.93408203125, 324.25, 1120.0, 0.8916015625, 335.25, 1142.0, 0.84716796875, 340.5, 1147.0, 0.62841796875, 389.75, 1131.0, 0.76220703125, 384.25, 1142.0, 0.72021484375, 466.0, 1109.0, 0.65869140625, 444.25, 1109.0, 0.350341796875, 438.75, 1076.0, 0.87353515625, 438.75, 1076.0, 0.451171875, 515.0, 1093.0, 0.66357421875, 515.0, 1103.0, 0.54052734375, 575.0, 1043.0, 0.70947265625, 580.5, 1082.0, 0.5751953125, 640.5, 1049.0, 0.298583984375, 635.0, 1071.0, 0.56982421875], 'scores': None}, '1368': {'keypoints': [321.5, 1105.0, 0.85595703125, 316.0, 1110.0, 0.93896484375, 316.0, 1105.0, 0.74560546875, 326.75, 1126.0, 0.849609375, 326.75, 1126.0, 0.59423828125, 391.75, 1116.0, 0.74169921875, 381.0, 1132.0, 0.7470703125, 451.25, 1094.0, 0.61328125, 445.75, 1105.0, 0.413818359375, 435.0, 1061.0, 0.80712890625, 429.5, 1061.0, 0.5185546875, 510.75, 1083.0, 0.7021484375, 510.75, 1099.0, 0.564453125, 570.0, 1029.0, 0.77001953125, 581.0, 1078.0, 0.751953125, 629.5, 1034.0, 0.251953125, 629.5, 1067.0, 0.41015625], 'scores': None}, '1369': {'keypoints': [316.5, 1089.0, 0.8740234375, 311.0, 1100.0, 0.919921875, 311.0, 1095.0, 0.84814453125, 321.75, 1116.0, 0.8544921875, 321.75, 1116.0, 0.6103515625, 386.75, 1106.0, 0.75146484375, 370.5, 1116.0, 0.701171875, 451.5, 1089.0, 0.68408203125, 430.0, 1084.0, 0.41162109375, 424.5, 1046.0, 0.76025390625, 413.75, 1030.0, 0.492919921875, 505.75, 1073.0, 0.662109375, 505.75, 1089.0, 0.61328125, 565.0, 1019.0, 0.783203125, 576.0, 1073.0, 0.7724609375, 624.5, 997.0, 0.151611328125, 624.5, 1068.0, 0.421875], 'scores': None}, '1370': {'keypoints': [312.5, 1080.0, 0.8916015625, 307.0, 1086.0, 0.9375, 307.0, 1080.0, 0.82275390625, 318.0, 1096.0, 0.87060546875, 312.5, 1102.0, 0.55322265625, 366.75, 1096.0, 0.8330078125, 361.25, 1091.0, 0.58984375, 442.75, 1069.0, 0.65478515625, 426.5, 1064.0, 0.355712890625, 426.5, 1037.0, 0.8505859375, 421.0, 1032.0, 0.6044921875, 502.25, 1059.0, 0.63623046875, 497.0, 1075.0, 0.498291015625, 562.0, 1015.0, 0.7724609375, 573.0, 1069.0, 0.78125, 621.5, 1037.0, 0.1688232421875, 621.5, 1064.0, 0.461181640625], 'scores': None}, '1371': {'keypoints': [302.25, 1060.0, 0.91259765625, 302.25, 1071.0, 0.91650390625, 297.0, 1066.0, 0.9072265625, 313.25, 1082.0, 0.85791015625, 307.75, 1088.0, 0.6513671875, 368.0, 1082.0, 0.7529296875, 362.5, 1099.0, 0.51708984375, 444.5, 1055.0, 0.6708984375, 439.0, 1055.0, 0.474365234375, 422.75, 1022.0, 0.8154296875, 428.0, 1022.0, 0.5556640625, 493.75, 1044.0, 0.654296875, 493.75, 1060.0, 0.53662109375, 559.5, 1006.0, 0.6171875, 570.5, 1060.0, 0.72021484375, 619.5, 989.5, 0.2232666015625, 619.5, 1060.0, 0.398193359375], 'scores': None}, '1372': {'keypoints': [299.0, 1049.0, 0.81982421875, 293.75, 1055.0, 0.7978515625, 293.75, 1049.0, 0.64990234375, 304.5, 1060.0, 0.75, 310.0, 1076.0, 0.56396484375, 358.75, 1055.0, 0.7900390625, 358.75, 1076.0, 0.72412109375, 440.0, 1044.0, 0.66650390625, 434.75, 1049.0, 0.40087890625, 418.5, 1011.5, 0.7998046875, 445.5, 1011.5, 0.66796875, 494.25, 1028.0, 0.6748046875, 489.0, 1055.0, 0.68701171875, 554.0, 979.0, 0.50732421875, 554.0, 1055.0, 0.74755859375, 613.5, 935.5, 0.2218017578125, 613.5, 1055.0, 0.380859375], 'scores': None}, '1373': {'keypoints': [294.25, 1033.0, 0.73291015625, 288.75, 1039.0, 0.7265625, 288.75, 1039.0, 0.51708984375, 299.75, 1044.0, 0.74072265625, 299.75, 1061.0, 0.53662109375, 354.25, 1039.0, 0.70849609375, 354.25, 1066.0, 0.69580078125, 419.75, 1017.0, 0.63525390625, 403.25, 1072.0, 0.2734375, 414.25, 995.0, 0.7314453125, 463.25, 1061.0, 0.260498046875, 485.0, 1011.5, 0.53125, 474.25, 1050.0, 0.5791015625, 539.5, 984.5, 0.460693359375, 545.0, 1039.0, 0.77587890625, 599.5, 940.5, 0.1240234375, 610.5, 1050.0, 0.5478515625], 'scores': None}, '1374': {'keypoints': [286.0, 1019.5, 0.6943359375, 280.75, 1019.5, 0.67578125, 280.75, 1019.5, 0.69287109375, 291.5, 1019.5, 0.7529296875, 297.0, 1046.0, 0.74609375, 340.25, 1019.5, 0.78125, 340.25, 1057.0, 0.83544921875, 400.0, 998.0, 0.744140625, 389.0, 1057.0, 0.685546875, 394.5, 960.0, 0.67041015625, 427.0, 1046.0, 0.37890625, 454.25, 998.0, 0.73193359375, 459.5, 1025.0, 0.5634765625, 524.5, 970.5, 0.493896484375, 530.0, 1014.0, 0.7861328125, 600.5, 943.5, 0.2440185546875, 595.0, 1036.0, 0.50830078125], 'scores': None}, '1375': {'keypoints': [273.5, 1007.5, 0.6357421875, 268.0, 1007.5, 0.72900390625, 273.5, 1007.5, 0.68701171875, 289.5, 1002.0, 0.81103515625, 289.5, 1029.0, 0.7919921875, 332.5, 1002.0, 0.80517578125, 332.5, 1045.0, 0.89208984375, 391.75, 986.0, 0.6162109375, 375.5, 1050.0, 0.8544921875, 391.75, 959.0, 0.50244140625, 418.5, 1034.0, 0.292236328125, 445.5, 980.5, 0.69384765625, 451.0, 1018.0, 0.49365234375, 521.0, 991.0, 0.42626953125, 510.0, 996.5, 0.490478515625, 585.0, 1029.0, 0.359130859375, 585.0, 1029.0, 0.2249755859375], 'scores': None}, '1376': {'keypoints': [269.75, 986.5, 0.47119140625, 260.5, 991.0, 0.444580078125, 260.5, 991.0, 0.367431640625, 279.0, 991.0, 0.81494140625, 279.0, 1023.5, 0.84423828125, 325.25, 991.0, 0.826171875, 325.25, 1033.0, 0.85205078125, 390.0, 977.0, 0.6337890625, 367.0, 1042.0, 0.80126953125, 380.75, 949.5, 0.64013671875, 343.75, 1014.0, 0.12890625, 445.5, 972.5, 0.626953125, 445.5, 1005.0, 0.50537109375, 510.25, 958.5, 0.4423828125, 510.25, 972.5, 0.469970703125, 533.5, 949.5, 0.10986328125, 529.0, 972.5, 0.077880859375], 'scores': None}, '1377': {'keypoints': [256.25, 977.0, 0.4951171875, 256.25, 985.0, 0.54345703125, 256.25, 985.0, 0.423095703125, 273.5, 981.0, 0.65234375, 273.5, 1011.0, 0.80517578125, 320.25, 977.0, 0.755859375, 320.25, 1023.5, 0.8310546875, 380.0, 960.0, 0.6240234375, 363.0, 1032.0, 0.84521484375, 358.75, 934.0, 0.51025390625, 346.0, 1002.0, 0.28173828125, 435.5, 964.0, 0.56494140625, 435.5, 994.0, 0.58203125, 499.5, 955.5, 0.57568359375, 499.5, 955.5, 0.568359375, 499.5, 955.5, 0.08770751953125, 499.5, 943.0, 0.062042236328125], 'scores': None}, '1378': {'keypoints': [257.75, 975.0, 0.457763671875, 253.5, 975.0, 0.4912109375, 253.5, 975.0, 0.409423828125, 266.25, 971.0, 0.82861328125, 266.25, 1001.0, 0.83642578125, 313.0, 962.5, 0.7607421875, 313.0, 1013.5, 0.87060546875, 364.0, 945.5, 0.55517578125, 368.25, 1018.0, 0.6796875, 393.75, 933.0, 0.3466796875, 389.5, 992.0, 0.57568359375, 419.25, 954.0, 0.62109375, 423.5, 988.0, 0.6650390625, 491.5, 950.0, 0.68603515625, 491.5, 954.0, 0.61083984375, 495.75, 954.0, 0.2415771484375, 491.5, 954.0, 0.187744140625], 'scores': None}, '1379': {'keypoints': [258.0, 957.0, 0.463134765625, 253.875, 965.0, 0.50830078125, 253.875, 965.0, 0.407958984375, 258.0, 957.0, 0.86767578125, 266.75, 991.0, 0.84765625, 309.25, 944.0, 0.77099609375, 309.25, 1003.5, 0.8564453125, 373.25, 923.0, 0.82275390625, 364.75, 1008.0, 0.68212890625, 339.25, 923.0, 0.55078125, 369.0, 982.0, 0.505859375, 411.75, 944.0, 0.630859375, 420.25, 978.0, 0.67822265625, 488.5, 948.0, 0.6181640625, 475.75, 948.0, 0.365966796875, 492.75, 948.0, 0.1484375, 492.75, 948.0, 0.11767578125], 'scores': None}, '1380': {'keypoints': [255.375, 954.5, 0.3916015625, 251.125, 954.5, 0.427978515625, 251.125, 954.5, 0.36181640625, 255.375, 954.5, 0.8232421875, 264.0, 980.0, 0.83056640625, 307.0, 933.0, 0.80224609375, 307.0, 993.0, 0.83447265625, 354.25, 915.5, 0.65185546875, 354.25, 997.0, 0.7802734375, 328.5, 907.0, 0.48046875, 358.5, 963.0, 0.250732421875, 410.0, 937.0, 0.63818359375, 410.0, 971.5, 0.6552734375, 487.5, 941.5, 0.56787109375, 474.5, 937.0, 0.39453125, 491.75, 950.0, 0.036529541015625, 478.75, 928.5, 0.0986328125], 'scores': None}, '1381': {'keypoints': [257.5, 930.0, 0.66259765625, 257.5, 930.0, 0.63037109375, 257.5, 930.0, 0.430419921875, 257.5, 934.5, 0.75048828125, 261.75, 969.5, 0.82568359375, 301.0, 921.5, 0.78564453125, 305.5, 983.0, 0.83740234375, 358.0, 904.0, 0.73486328125, 345.0, 987.0, 0.78955078125, 331.75, 899.5, 0.51123046875, 353.5, 965.0, 0.293701171875, 406.0, 926.0, 0.64404296875, 406.0, 961.0, 0.6630859375, 485.0, 930.0, 0.6318359375, 458.5, 926.0, 0.3828125, 493.5, 939.0, 0.07720947265625, 493.5, 930.0, 0.10406494140625], 'scores': None}, '1382': {'keypoints': [252.5, 935.5, 0.2220458984375, 257.0, 931.0, 0.276123046875, 252.5, 975.0, 0.236572265625, 252.5, 931.0, 0.8203125, 257.0, 962.0, 0.80810546875, 292.5, 918.0, 0.802734375, 297.0, 975.0, 0.8291015625, 341.25, 904.5, 0.6650390625, 336.75, 975.0, 0.81884765625, 328.0, 891.0, 0.48388671875, 297.0, 975.0, 0.2095947265625, 394.5, 918.0, 0.6416015625, 394.5, 948.5, 0.671875, 470.0, 918.0, 0.673828125, 461.0, 918.0, 0.54345703125, 496.5, 931.0, 0.0712890625, 487.75, 869.0, 0.11676025390625], 'scores': None}, '1383': {'keypoints': [231.75, 929.5, 0.4560546875, 231.75, 929.5, 0.445068359375, 227.375, 929.5, 0.427978515625, 236.0, 925.0, 0.78466796875, 244.75, 955.5, 0.8017578125, 283.75, 907.5, 0.82275390625, 288.25, 959.5, 0.8232421875, 340.25, 899.0, 0.705078125, 327.25, 968.5, 0.810546875, 318.5, 886.0, 0.6630859375, 336.0, 938.0, 0.30029296875, 383.75, 907.5, 0.67138671875, 388.0, 942.5, 0.61962890625, 457.5, 894.5, 0.64697265625, 453.25, 903.5, 0.58056640625, 483.75, 886.0, 0.0501708984375, 483.75, 886.0, 0.08172607421875], 'scores': None}, '1384': {'keypoints': [223.125, 919.0, 0.541015625, 223.125, 919.0, 0.5556640625, 219.0, 919.0, 0.5693359375, 227.375, 919.0, 0.81103515625, 235.875, 944.5, 0.828125, 278.25, 902.5, 0.81396484375, 282.5, 953.0, 0.83447265625, 337.5, 894.0, 0.794921875, 329.0, 957.0, 0.61962890625, 316.25, 877.0, 0.84814453125, 299.5, 927.5, 0.212646484375, 371.5, 898.0, 0.58642578125, 379.75, 932.0, 0.61328125, 443.25, 877.0, 0.669921875, 443.25, 902.5, 0.67724609375, 468.75, 868.5, 0.08953857421875, 464.5, 894.0, 0.06524658203125], 'scores': None}, '1385': {'keypoints': [216.0, 904.0, 0.5703125, 211.75, 908.0, 0.69384765625, 211.75, 908.0, 0.5673828125, 224.375, 908.0, 0.82958984375, 228.625, 933.5, 0.802734375, 270.75, 900.0, 0.81689453125, 270.75, 946.0, 0.8671875, 333.75, 891.5, 0.822265625, 321.0, 946.0, 0.67529296875, 308.5, 870.5, 0.9140625, 304.25, 887.0, 0.2724609375, 375.75, 891.5, 0.58544921875, 375.75, 921.0, 0.60302734375, 434.5, 870.5, 0.57568359375, 438.75, 900.0, 0.7119140625, 459.75, 870.5, 0.10260009765625, 459.75, 891.5, 0.0665283203125], 'scores': None}, '1386': {'keypoints': [215.25, 898.0, 0.4443359375, 206.875, 898.0, 0.529296875, 206.875, 898.0, 0.406494140625, 219.5, 898.0, 0.86083984375, 223.75, 923.5, 0.87060546875, 266.0, 890.0, 0.79052734375, 261.75, 936.0, 0.87841796875, 325.0, 885.5, 0.78271484375, 308.0, 940.5, 0.75537109375, 304.0, 864.5, 0.84326171875, 299.75, 881.5, 0.223876953125, 367.25, 881.5, 0.625, 367.25, 915.0, 0.6396484375, 417.75, 843.5, 0.6689453125, 439.0, 898.0, 0.66455078125, 455.75, 814.0, 0.2396240234375, 455.75, 890.0, 0.0823974609375], 'scores': None}, '1387': {'keypoints': [207.375, 888.0, 0.53271484375, 203.0, 888.0, 0.66357421875, 203.0, 888.0, 0.57666015625, 211.625, 888.0, 0.8017578125, 215.875, 914.0, 0.81982421875, 262.75, 884.0, 0.7861328125, 258.5, 926.5, 0.89306640625, 322.5, 880.0, 0.74462890625, 305.5, 931.0, 0.66162109375, 305.5, 858.5, 0.7861328125, 331.0, 914.0, 0.358154296875, 365.25, 871.0, 0.62158203125, 365.25, 905.0, 0.6396484375, 420.5, 833.0, 0.7275390625, 433.5, 892.5, 0.78515625, 454.75, 812.0, 0.1519775390625, 454.75, 901.0, 0.0226593017578125], 'scores': None}, '1388': {'keypoints': [200.125, 878.0, 0.580078125, 195.875, 878.0, 0.7041015625, 200.125, 878.0, 0.466064453125, 213.0, 878.0, 0.82080078125, 213.0, 904.0, 0.79541015625, 255.75, 873.5, 0.77685546875, 255.75, 921.0, 0.89990234375, 311.5, 869.5, 0.67724609375, 303.0, 921.0, 0.7099609375, 307.25, 843.5, 0.72900390625, 328.5, 908.0, 0.266845703125, 362.75, 865.0, 0.65234375, 362.75, 899.5, 0.65478515625, 418.5, 835.0, 0.71337890625, 427.0, 891.0, 0.7548828125, 452.75, 818.0, 0.156494140625, 452.75, 895.0, 0.06689453125], 'scores': None}, '1389': {'keypoints': [213.625, 863.5, 0.5859375, 209.375, 868.0, 0.611328125, 209.375, 863.5, 0.46923828125, 213.625, 868.0, 0.8681640625, 217.875, 893.0, 0.75146484375, 251.75, 863.5, 0.78564453125, 251.75, 905.5, 0.85107421875, 311.0, 859.5, 0.79150390625, 285.5, 914.0, 0.72021484375, 298.25, 838.5, 0.7734375, 285.5, 884.5, 0.147216796875, 353.5, 859.5, 0.64013671875, 353.5, 893.0, 0.60693359375, 412.75, 834.0, 0.72802734375, 425.25, 884.5, 0.73779296875, 450.75, 817.5, 0.1671142578125, 450.75, 884.5, 0.039886474609375], 'scores': None}, '1390': {'keypoints': [207.5, 854.5, 0.411865234375, 203.25, 859.0, 0.51220703125, 203.25, 859.0, 0.305419921875, 207.5, 859.0, 0.8388671875, 207.5, 880.0, 0.7890625, 250.0, 850.0, 0.8115234375, 245.75, 893.0, 0.80126953125, 305.25, 850.0, 0.8017578125, 284.0, 910.0, 0.7373046875, 296.75, 825.0, 0.79931640625, 288.25, 876.0, 0.1689453125, 352.0, 850.0, 0.6142578125, 347.75, 884.0, 0.64013671875, 407.25, 825.0, 0.67041015625, 420.0, 880.0, 0.77783203125, 445.5, 795.0, 0.1422119140625, 449.75, 888.5, 0.044677734375], 'scores': None}, '1391': {'keypoints': [202.75, 843.0, 0.37109375, 202.75, 843.0, 0.452392578125, 202.75, 843.0, 0.350830078125, 207.0, 843.0, 0.84619140625, 207.0, 869.0, 0.849609375, 245.5, 839.0, 0.8017578125, 241.25, 886.0, 0.82861328125, 301.25, 834.5, 0.73583984375, 279.75, 899.0, 0.8173828125, 292.5, 817.5, 0.54150390625, 301.25, 882.0, 0.4169921875, 339.75, 839.0, 0.61767578125, 339.75, 877.5, 0.658203125, 395.25, 821.5, 0.7177734375, 408.25, 869.0, 0.783203125, 446.75, 813.0, 0.1741943359375, 446.75, 882.0, 0.0693359375], 'scores': None}, '1392': {'keypoints': [192.375, 835.0, 0.2529296875, 188.125, 835.0, 0.402587890625, 196.75, 874.0, 0.31689453125, 196.75, 835.0, 0.85205078125, 201.0, 861.0, 0.8388671875, 239.5, 826.5, 0.83447265625, 235.25, 874.0, 0.8388671875, 291.0, 822.0, 0.70556640625, 278.0, 886.5, 0.82568359375, 303.75, 818.0, 0.49560546875, 295.25, 869.5, 0.6376953125, 333.75, 826.5, 0.611328125, 333.75, 861.0, 0.68701171875, 389.25, 813.5, 0.72998046875, 398.0, 856.5, 0.88720703125, 436.5, 788.0, 0.493408203125, 440.75, 874.0, 0.330078125], 'scores': None}, '1393': {'keypoints': [194.625, 835.0, 0.39599609375, 186.125, 826.5, 0.44775390625, 186.125, 835.0, 0.4267578125, 190.375, 826.5, 0.8603515625, 194.625, 847.5, 0.8525390625, 232.75, 814.0, 0.86279296875, 232.75, 868.5, 0.8349609375, 275.0, 805.5, 0.56787109375, 270.75, 877.0, 0.740234375, 313.25, 809.5, 0.360107421875, 287.75, 864.0, 0.52880859375, 326.0, 818.0, 0.68603515625, 330.25, 851.5, 0.72607421875, 385.25, 814.0, 0.6904296875, 385.25, 847.5, 0.87890625, 431.75, 851.5, 0.365234375, 431.75, 856.0, 0.470947265625], 'scores': None}, '1394': {'keypoints': [188.0, 817.5, 0.37841796875, 183.875, 817.5, 0.46240234375, 183.875, 834.0, 0.323486328125, 188.0, 817.5, 0.86474609375, 188.0, 842.5, 0.85791015625, 229.375, 805.0, 0.81884765625, 225.25, 859.0, 0.85107421875, 275.0, 793.0, 0.65185546875, 266.75, 871.0, 0.77978515625, 295.75, 805.0, 0.431640625, 279.0, 855.0, 0.59716796875, 316.25, 809.5, 0.66748046875, 316.25, 842.5, 0.7138671875, 374.25, 809.5, 0.732421875, 370.25, 838.0, 0.85498046875, 419.75, 797.0, 0.471435546875, 419.75, 846.5, 0.61962890625], 'scores': None}, '1395': {'keypoints': [185.0, 808.0, 0.31494140625, 181.0, 808.0, 0.396728515625, 177.0, 812.0, 0.2012939453125, 181.0, 812.0, 0.87109375, 181.0, 836.0, 0.85986328125, 221.0, 796.0, 0.859375, 221.0, 852.0, 0.861328125, 265.0, 788.0, 0.75146484375, 265.0, 864.0, 0.85009765625, 289.0, 796.0, 0.3369140625, 277.0, 844.0, 0.64208984375, 313.0, 804.0, 0.68603515625, 313.0, 836.0, 0.76220703125, 369.0, 804.0, 0.74951171875, 365.0, 828.0, 0.8212890625, 405.0, 804.0, 0.32763671875, 405.0, 828.0, 0.4033203125], 'scores': None}, '1396': {'keypoints': [179.75, 817.0, 0.273193359375, 172.0, 809.0, 0.297607421875, 172.0, 817.0, 0.2423095703125, 175.875, 805.5, 0.82421875, 175.875, 828.5, 0.833984375, 214.375, 790.0, 0.83642578125, 218.25, 843.5, 0.84716796875, 260.5, 778.5, 0.8388671875, 260.5, 855.0, 0.85546875, 287.5, 782.5, 0.439453125, 272.0, 836.0, 0.69482421875, 299.0, 797.5, 0.71923828125, 302.75, 832.0, 0.75048828125, 364.25, 801.5, 0.62158203125, 345.0, 820.5, 0.7177734375, 391.25, 805.5, 0.2568359375, 391.25, 817.0, 0.484130859375], 'scores': None}, '1397': {'keypoints': [176.25, 809.0, 0.1966552734375, 176.25, 801.0, 0.20166015625, 164.875, 824.0, 0.3125, 176.25, 801.0, 0.7998046875, 176.25, 824.0, 0.85791015625, 214.0, 786.0, 0.84033203125, 214.0, 839.0, 0.8330078125, 259.5, 771.0, 0.8740234375, 255.625, 846.5, 0.79931640625, 251.875, 793.5, 0.326416015625, 263.25, 828.0, 0.401123046875, 297.25, 793.5, 0.72900390625, 293.5, 828.0, 0.6943359375, 357.75, 801.0, 0.73291015625, 354.0, 805.0, 0.376220703125, 384.25, 805.0, 0.1995849609375, 369.0, 801.0, 0.2200927734375], 'scores': None}, '1398': {'keypoints': [173.75, 802.5, 0.25634765625, 166.25, 802.5, 0.2509765625, 162.625, 817.5, 0.37158203125, 170.0, 795.5, 0.8330078125, 173.75, 817.5, 0.8095703125, 211.0, 780.5, 0.84228515625, 214.625, 832.5, 0.8408203125, 255.5, 765.5, 0.87890625, 259.25, 843.5, 0.80712890625, 255.5, 791.5, 0.5634765625, 263.0, 817.5, 0.3876953125, 292.75, 788.0, 0.68603515625, 296.5, 821.0, 0.6943359375, 359.75, 799.0, 0.7275390625, 348.5, 799.0, 0.405517578125, 378.25, 799.0, 0.0767822265625, 378.25, 795.5, 0.147216796875], 'scores': None}, '1399': {'keypoints': [162.125, 811.0, 0.3662109375, 162.125, 807.5, 0.3212890625, 162.125, 811.0, 0.425537109375, 169.375, 789.5, 0.85546875, 173.0, 811.0, 0.84375, 209.25, 775.0, 0.8359375, 212.875, 825.5, 0.83837890625, 256.5, 760.5, 0.88671875, 256.5, 833.0, 0.80078125, 234.625, 778.5, 0.33837890625, 278.0, 818.5, 0.572265625, 296.25, 786.0, 0.66650390625, 292.5, 815.0, 0.68310546875, 358.0, 793.0, 0.6484375, 343.5, 793.0, 0.346923828125, 372.5, 800.0, 0.12078857421875, 372.5, 789.5, 0.1468505859375], 'scores': None}, '1400': {'keypoints': [169.0, 796.0, 0.404296875, 162.0, 796.0, 0.345458984375, 162.0, 792.5, 0.420166015625, 165.5, 785.5, 0.845703125, 169.0, 806.5, 0.85986328125, 204.0, 768.0, 0.85400390625, 207.5, 820.5, 0.8349609375, 249.5, 757.5, 0.85400390625, 249.5, 827.5, 0.76806640625, 239.0, 782.0, 0.317138671875, 263.5, 813.5, 0.38623046875, 291.5, 778.5, 0.6787109375, 288.0, 806.5, 0.6162109375, 351.0, 785.5, 0.689453125, 347.5, 789.0, 0.3193359375, 361.5, 789.0, 0.080078125, 358.0, 785.5, 0.2242431640625], 'scores': None}, '1401': {'keypoints': [168.0, 785.0, 0.1485595703125, 163.0, 780.0, 0.1898193359375, 160.5, 802.5, 0.1595458984375, 165.5, 780.0, 0.7314453125, 163.0, 800.0, 0.7392578125, 198.0, 765.0, 0.7919921875, 200.5, 815.0, 0.81494140625, 245.5, 757.5, 0.77392578125, 240.5, 820.0, 0.58349609375, 233.0, 752.5, 0.443115234375, 255.5, 810.0, 0.1982421875, 275.5, 772.5, 0.72216796875, 275.5, 807.5, 0.70703125, 258.0, 765.0, 0.07928466796875, 265.5, 792.5, 0.06622314453125, 263.0, 762.5, 0.3310546875, 295.5, 802.5, 0.04180908203125], 'scores': None}, '1402': {'keypoints': [148.25, 774.5, 0.53271484375, 146.125, 776.5, 0.5849609375, 144.0, 779.0, 0.55126953125, 157.0, 774.5, 0.75732421875, 157.0, 794.0, 0.77392578125, 192.0, 761.5, 0.7509765625, 192.0, 809.0, 0.8134765625, 244.5, 757.0, 0.81103515625, 233.625, 811.5, 0.63720703125, 229.25, 746.5, 0.8076171875, 229.25, 772.5, 0.1380615234375, 268.5, 770.0, 0.5205078125, 268.5, 800.5, 0.5361328125, 264.25, 763.5, 0.0484619140625, 244.5, 787.5, 0.040496826171875, 270.75, 737.5, 0.05267333984375, 270.75, 824.5, 0.034027099609375], 'scores': None}, '1403': {'keypoints': [153.375, 778.0, 0.42431640625, 150.25, 772.0, 0.3349609375, 150.25, 778.0, 0.377197265625, 153.375, 768.5, 0.83056640625, 153.375, 787.5, 0.83984375, 188.25, 756.0, 0.82275390625, 188.25, 803.5, 0.8349609375, 235.875, 756.0, 0.84716796875, 229.5, 806.5, 0.7880859375, 226.375, 740.0, 0.81298828125, 248.5, 794.0, 0.646484375, 274.0, 762.5, 0.487548828125, 274.0, 791.0, 0.53369140625, 327.75, 756.0, 0.2861328125, 327.75, 778.0, 0.44970703125, 327.75, 762.5, 0.06768798828125, 327.75, 772.0, 0.032318115234375], 'scores': None}, '1404': {'keypoints': [150.625, 762.0, 0.384765625, 147.125, 762.0, 0.415771484375, 143.75, 762.0, 0.30517578125, 150.625, 762.0, 0.8740234375, 147.125, 783.0, 0.85107421875, 185.125, 755.0, 0.84228515625, 181.75, 796.5, 0.85986328125, 233.5, 751.5, 0.87353515625, 223.125, 800.0, 0.74169921875, 223.125, 738.0, 0.76025390625, 247.25, 793.0, 0.583984375, 261.0, 762.0, 0.58984375, 257.75, 790.0, 0.68505859375, 299.0, 748.0, 0.2232666015625, 323.25, 776.0, 0.7109375, 340.5, 751.5, 0.1495361328125, 340.5, 776.0, 0.1036376953125], 'scores': None}, '1405': {'keypoints': [151.5, 748.5, 0.54931640625, 147.875, 752.0, 0.56396484375, 147.875, 752.0, 0.343505859375, 147.875, 756.0, 0.873046875, 147.875, 777.5, 0.8701171875, 184.0, 748.5, 0.85498046875, 180.375, 791.5, 0.87939453125, 230.875, 748.5, 0.88671875, 216.5, 799.0, 0.72021484375, 220.0, 734.5, 0.86083984375, 245.375, 791.5, 0.5048828125, 259.75, 759.5, 0.62939453125, 259.75, 784.5, 0.669921875, 303.0, 731.0, 0.35595703125, 321.0, 774.0, 0.7138671875, 314.0, 741.5, 0.154541015625, 346.5, 774.0, 0.086181640625], 'scores': None}, '1406': {'keypoints': [142.75, 749.0, 0.39501953125, 142.75, 749.0, 0.4765625, 142.75, 749.0, 0.472900390625, 146.375, 749.0, 0.92822265625, 146.375, 771.0, 0.88037109375, 179.375, 741.5, 0.8115234375, 175.75, 785.5, 0.8564453125, 227.125, 741.5, 0.88232421875, 216.125, 793.0, 0.76904296875, 219.75, 727.0, 0.82275390625, 227.125, 778.5, 0.399169921875, 256.5, 752.5, 0.63134765625, 252.875, 782.0, 0.681640625, 304.25, 745.5, 0.475341796875, 315.25, 771.0, 0.7275390625, 348.25, 756.5, 0.28662109375, 348.25, 771.0, 0.08349609375], 'scores': None}, '1407': {'keypoints': [142.125, 745.5, nan, 144.875, 743.0, nan, 141.25, 748.0, nan, 145.5, 743.0, nan, 145.5, 763.0, nan, 177.25, 734.5, nan, 174.0, 778.0, nan, 222.5, 734.5, nan, 213.375, 785.5, nan, 223.75, 725.5, nan, 227.875, 774.0, nan, 251.875, 745.0, nan, 248.5, 774.0, nan, 298.0, 739.5, nan, 305.25, 765.5, nan, 327.25, 749.0, nan, 326.5, 764.5, nan], 'scores': None}, '1408': {'keypoints': [141.625, 742.0, nan, 146.875, 736.5, nan, 139.875, 747.5, nan, 144.625, 736.5, nan, 144.625, 754.5, nan, 175.25, 727.0, nan, 172.25, 770.0, nan, 217.875, 727.0, nan, 210.625, 778.0, nan, 227.875, 724.0, nan, 228.5, 770.0, nan, 247.25, 738.0, nan, 244.25, 765.5, nan, 291.5, 734.0, nan, 295.25, 760.0, nan, 306.25, 741.0, nan, 304.5, 758.5, nan], 'scores': None}, '1409': {'keypoints': [141.0, 738.5, 0.143798828125, 149.0, 730.5, 0.1622314453125, 138.375, 746.5, 0.232666015625, 143.75, 730.5, 0.7705078125, 143.75, 746.5, 0.7197265625, 173.125, 720.0, 0.76171875, 170.5, 762.5, 0.7900390625, 213.25, 720.0, 0.6767578125, 207.875, 770.5, 0.71630859375, 231.875, 722.5, 0.6533203125, 229.25, 765.5, 0.666015625, 242.625, 730.5, 0.63330078125, 239.875, 757.5, 0.66748046875, 285.25, 728.0, 0.41748046875, 285.25, 754.5, 0.427734375, 285.25, 733.5, 0.06951904296875, 282.75, 752.0, 0.0179901123046875], 'scores': None}, '1410': {'keypoints': [132.75, 731.0, 0.34375, 132.75, 723.5, 0.36962890625, 130.25, 723.5, 0.3359375, 137.75, 721.0, 0.861328125, 137.75, 738.5, 0.7978515625, 170.75, 710.5, 0.7939453125, 168.125, 759.0, 0.76513671875, 213.75, 710.5, 0.78857421875, 203.625, 766.5, 0.7255859375, 208.625, 705.5, 0.1668701171875, 218.75, 751.0, 0.392578125, 244.125, 723.5, 0.52783203125, 244.125, 751.0, 0.541015625, 274.5, 721.0, 0.11309814453125, 274.5, 748.5, 0.14306640625, 274.5, 726.0, 0.029052734375, 135.25, 695.5, 0.01910400390625], 'scores': None}, '1411': {'keypoints': [136.625, 712.0, 0.1812744140625, 134.0, 715.0, 0.3017578125, 134.0, 732.5, 0.253662109375, 136.625, 715.0, 0.79443359375, 136.625, 735.0, 0.73974609375, 167.375, 704.5, 0.818359375, 164.75, 753.0, 0.79052734375, 205.75, 699.5, 0.82861328125, 198.125, 758.0, 0.71240234375, 216.0, 709.5, 0.32275390625, 221.25, 750.5, 0.52587890625, 236.5, 717.5, 0.51416015625, 234.0, 745.0, 0.54248046875, 272.5, 715.0, 0.30029296875, 272.5, 742.5, 0.41748046875, 272.5, 720.0, 0.071533203125, 270.0, 737.5, 0.04345703125], 'scores': None}, '1412': {'keypoints': [125.375, 718.0, 0.359619140625, 122.75, 718.0, 0.33349609375, 122.75, 715.0, 0.409423828125, 128.0, 710.0, 0.81494140625, 128.0, 728.0, 0.83203125, 162.125, 699.5, 0.80419921875, 159.5, 749.0, 0.80224609375, 201.5, 694.0, 0.85302734375, 196.25, 752.0, 0.755859375, 191.0, 712.5, 0.48486328125, 217.25, 744.0, 0.398681640625, 227.75, 710.0, 0.66796875, 230.375, 739.0, 0.6767578125, 272.5, 712.5, 0.587890625, 272.5, 736.0, 0.54248046875, 272.5, 718.0, 0.09637451171875, 269.75, 731.0, 0.062255859375], 'scores': None}, '1413': {'keypoints': [125.75, 717.0, 0.232421875, 131.25, 711.5, 0.2054443359375, 125.75, 720.0, 0.2161865234375, 131.25, 709.0, 0.73486328125, 128.5, 725.0, 0.7626953125, 156.0, 695.0, 0.79833984375, 156.0, 744.5, 0.79736328125, 194.5, 687.0, 0.77587890625, 191.75, 750.0, 0.79443359375, 200.0, 698.0, 0.54833984375, 205.5, 739.0, 0.3798828125, 227.5, 706.0, 0.57763671875, 227.5, 733.5, 0.5849609375, 274.25, 711.5, 0.62646484375, 271.5, 731.0, 0.59716796875, 274.25, 717.0, 0.09979248046875, 271.5, 725.0, 0.042327880859375], 'scores': None}, '1414': {'keypoints': [124.0, 720.5, 0.286865234375, 124.0, 712.0, 0.245849609375, 124.0, 720.5, 0.3623046875, 126.8125, 706.5, 0.841796875, 126.8125, 723.0, 0.84521484375, 154.625, 692.5, 0.8125, 154.625, 740.0, 0.79736328125, 188.0, 681.5, 0.86181640625, 190.75, 745.5, 0.76025390625, 174.125, 695.0, 0.29296875, 207.5, 734.5, 0.372802734375, 224.125, 703.5, 0.5712890625, 224.125, 729.0, 0.59326171875, 274.25, 709.0, 0.63818359375, 271.5, 726.0, 0.67822265625, 274.25, 712.0, 0.08990478515625, 274.25, 726.0, 0.0706787109375], 'scores': None}, '1415': {'keypoints': [124.3125, 708.0, 0.321044921875, 121.5, 702.5, 0.35400390625, 121.5, 711.0, 0.279052734375, 124.3125, 702.5, 0.88720703125, 124.3125, 719.0, 0.841796875, 155.25, 688.5, 0.82080078125, 152.5, 736.0, 0.83251953125, 189.0, 680.0, 0.88232421875, 189.0, 744.5, 0.76416015625, 175.0, 691.0, 0.3427734375, 208.75, 727.5, 0.4150390625, 225.5, 702.5, 0.65185546875, 225.5, 727.5, 0.650390625, 273.5, 708.0, 0.6513671875, 273.5, 727.5, 0.67333984375, 276.25, 713.5, 0.0828857421875, 276.25, 727.5, 0.042327880859375], 'scores': None}, '1416': {'keypoints': [124.3125, 703.0, 0.375, 124.3125, 703.0, 0.431396484375, 121.5, 703.0, 0.362548828125, 127.125, 700.5, 0.84912109375, 127.125, 720.0, 0.8359375, 155.25, 689.0, 0.8203125, 152.5, 734.0, 0.8369140625, 189.0, 678.0, 0.83642578125, 189.0, 739.5, 0.75537109375, 180.5, 695.0, 0.3720703125, 205.875, 725.5, 0.533203125, 225.5, 700.5, 0.57568359375, 225.5, 725.5, 0.591796875, 276.25, 709.0, 0.63330078125, 273.5, 720.0, 0.6376953125, 276.25, 711.5, 0.09075927734375, 276.25, 714.5, 0.07684326171875], 'scores': None}, '1417': {'keypoints': [125.3125, 702.0, 0.366455078125, 119.6875, 702.0, 0.45947265625, 119.6875, 702.0, 0.341064453125, 125.3125, 699.5, 0.8564453125, 125.3125, 719.0, 0.83740234375, 153.5, 688.0, 0.8271484375, 150.625, 733.0, 0.83056640625, 187.25, 680.0, 0.8798828125, 187.25, 735.5, 0.77001953125, 178.75, 699.5, 0.22607421875, 206.875, 724.5, 0.470703125, 221.0, 699.5, 0.60888671875, 221.0, 724.5, 0.60107421875, 274.5, 705.0, 0.63427734375, 274.5, 716.0, 0.458740234375, 277.25, 710.5, 0.08319091796875, 277.25, 713.5, 0.0704345703125], 'scores': None}, '1418': {'keypoints': [122.625, 707.0, 0.241455078125, 119.8125, 698.5, 0.319091796875, 117.0, 715.5, 0.22021484375, 122.625, 698.5, 0.8330078125, 119.8125, 715.5, 0.8251953125, 150.875, 687.0, 0.83935546875, 148.125, 729.5, 0.8173828125, 184.875, 678.5, 0.83056640625, 184.875, 732.5, 0.7841796875, 176.375, 690.0, 0.29638671875, 201.75, 724.0, 0.576171875, 218.75, 695.5, 0.56982421875, 218.75, 721.0, 0.57861328125, 269.75, 704.0, 0.61962890625, 266.75, 709.5, 0.56494140625, 278.25, 704.0, 0.0772705078125, 275.25, 712.5, 0.07574462890625], 'scores': None}, '1419': {'keypoints': [118.25, 707.0, 0.257080078125, 118.25, 698.0, 0.309814453125, 115.375, 713.0, 0.255126953125, 118.25, 698.0, 0.8203125, 115.375, 715.5, 0.86083984375, 144.375, 686.5, 0.84765625, 144.375, 727.5, 0.82861328125, 182.25, 680.5, 0.80029296875, 182.25, 730.0, 0.697265625, 190.875, 686.5, 0.449462890625, 199.625, 721.5, 0.452392578125, 214.125, 695.0, 0.60986328125, 217.0, 718.5, 0.63232421875, 260.75, 695.0, 0.78173828125, 263.5, 710.0, 0.68505859375, 275.25, 704.0, 0.06488037109375, 278.0, 710.0, 0.1524658203125], 'scores': None}, '1420': {'keypoints': [109.8125, 704.0, 0.320068359375, 109.8125, 704.0, 0.288818359375, 109.8125, 713.0, 0.373046875, 109.8125, 698.0, 0.89111328125, 109.8125, 716.0, 0.83447265625, 139.5, 686.0, 0.8564453125, 139.5, 727.5, 0.8017578125, 178.125, 680.5, 0.79638671875, 172.125, 730.5, 0.771484375, 184.0, 689.0, 0.319091796875, 184.0, 719.0, 0.533203125, 210.75, 695.0, 0.61767578125, 210.75, 719.0, 0.6611328125, 255.25, 689.0, 0.814453125, 258.25, 710.0, 0.7587890625, 279.0, 695.0, 0.150634765625, 279.0, 707.0, 0.1329345703125], 'scores': None}, '1421': {'keypoints': [110.125, 703.0, 0.3583984375, 107.125, 703.0, 0.349609375, 107.125, 706.0, 0.39208984375, 110.125, 697.0, 0.88134765625, 110.125, 712.0, 0.80712890625, 137.25, 685.0, 0.87353515625, 137.25, 724.0, 0.84033203125, 173.5, 682.0, 0.80517578125, 170.375, 730.0, 0.7392578125, 182.5, 688.0, 0.5546875, 185.5, 715.0, 0.446044921875, 203.625, 694.0, 0.5859375, 203.625, 718.0, 0.6259765625, 245.75, 685.0, 0.72265625, 251.875, 709.0, 0.810546875, 279.0, 691.0, 0.58056640625, 279.0, 706.0, 0.352294921875], 'scores': None}, '1422': {'keypoints': [112.25, 703.0, 0.320068359375, 106.1875, 703.0, 0.3310546875, 106.1875, 706.0, 0.35546875, 109.25, 697.0, 0.8740234375, 109.25, 712.0, 0.81640625, 136.5, 685.0, 0.8642578125, 136.5, 724.5, 0.83056640625, 172.875, 681.5, 0.8447265625, 169.875, 730.5, 0.740234375, 169.875, 681.5, 0.361572265625, 182.0, 721.0, 0.513671875, 209.25, 691.0, 0.63818359375, 206.25, 715.0, 0.64453125, 236.5, 685.0, 0.8154296875, 251.75, 709.0, 0.7666015625, 276.0, 691.0, 0.703125, 279.0, 706.0, 0.45947265625], 'scores': None}, '1423': {'keypoints': [111.375, 700.0, 0.305419921875, 108.3125, 694.0, 0.3271484375, 105.25, 700.0, 0.33642578125, 108.3125, 694.0, 0.869140625, 108.3125, 709.0, 0.8681640625, 132.75, 685.0, 0.84814453125, 132.75, 721.0, 0.81396484375, 172.25, 681.5, 0.74658203125, 166.25, 730.5, 0.7548828125, 178.375, 675.5, 0.450439453125, 178.375, 721.0, 0.46337890625, 196.75, 691.0, 0.6728515625, 196.75, 715.0, 0.66357421875, 233.25, 685.0, 0.8037109375, 245.5, 709.0, 0.7294921875, 272.75, 688.0, 0.6767578125, 279.0, 706.0, 0.57568359375], 'scores': None}, '1424': {'keypoints': [108.25, 700.0, 0.2452392578125, 108.25, 694.0, 0.2568359375, 102.125, 706.0, 0.307373046875, 108.25, 694.0, 0.83056640625, 108.25, 709.0, 0.84521484375, 132.5, 681.5, 0.84375, 132.5, 721.0, 0.8271484375, 165.75, 678.5, 0.66357421875, 162.75, 730.5, 0.80810546875, 181.0, 681.5, 0.65673828125, 174.875, 721.0, 0.6591796875, 190.0, 691.0, 0.6875, 193.125, 715.0, 0.6875, 232.5, 685.0, 0.79833984375, 241.625, 706.0, 0.71240234375, 269.0, 681.5, 0.646484375, 278.0, 703.0, 0.56298828125], 'scores': None}, '1425': {'keypoints': [103.0625, 706.0, 0.221435546875, 109.125, 694.0, 0.233642578125, 103.0625, 706.0, 0.3154296875, 106.125, 694.0, 0.82080078125, 106.125, 709.0, 0.8310546875, 130.25, 682.0, 0.82421875, 130.25, 721.0, 0.85986328125, 163.375, 676.0, 0.74755859375, 163.375, 727.0, 0.77587890625, 181.5, 679.0, 0.654296875, 178.5, 721.0, 0.53759765625, 193.5, 691.0, 0.64990234375, 193.5, 712.0, 0.67626953125, 235.75, 685.0, 0.7275390625, 244.75, 706.0, 0.77685546875, 269.0, 679.0, 0.48583984375, 278.0, 703.0, 0.40380859375], 'scores': None}, '1426': {'keypoints': [106.1875, 699.0, 0.2293701171875, 106.1875, 690.0, 0.274658203125, 103.125, 705.0, 0.30029296875, 106.1875, 693.0, 0.79248046875, 106.1875, 708.0, 0.8291015625, 130.5, 677.5, 0.8505859375, 130.5, 720.0, 0.8603515625, 163.75, 674.5, 0.78955078125, 160.75, 729.5, 0.84619140625, 182.0, 680.5, 0.52294921875, 175.875, 717.0, 0.52685546875, 191.0, 690.0, 0.68994140625, 194.125, 711.0, 0.68994140625, 236.5, 687.0, 0.748046875, 242.625, 705.0, 0.7724609375, 270.0, 680.5, 0.5712890625, 279.0, 702.0, 0.50830078125], 'scores': None}, '1427': {'keypoints': [104.375, 697.0, 0.24658203125, 104.375, 688.0, 0.2666015625, 101.3125, 703.0, 0.332275390625, 104.375, 691.0, 0.81396484375, 104.375, 706.0, 0.822265625, 128.875, 678.5, 0.853515625, 128.875, 718.0, 0.86181640625, 159.5, 672.5, 0.82763671875, 162.5, 727.5, 0.85009765625, 181.0, 678.5, 0.429931640625, 174.75, 718.0, 0.443115234375, 190.125, 688.0, 0.73291015625, 190.125, 709.0, 0.68603515625, 233.0, 685.0, 0.8134765625, 239.125, 706.0, 0.78955078125, 269.75, 682.0, 0.7294921875, 279.0, 700.0, 0.70947265625], 'scores': None}, '1428': {'keypoints': [98.5625, 699.0, 0.364990234375, 95.4375, 699.0, 0.4189453125, 95.4375, 702.0, 0.4638671875, 101.625, 690.0, 0.76708984375, 101.625, 705.0, 0.82568359375, 126.5, 677.5, 0.86376953125, 126.5, 717.5, 0.84033203125, 157.625, 671.0, 0.84423828125, 157.625, 723.5, 0.859375, 170.0, 677.5, 0.486572265625, 160.75, 711.5, 0.42041015625, 188.75, 686.5, 0.6962890625, 188.75, 708.0, 0.7080078125, 229.125, 686.5, 0.8369140625, 232.25, 705.0, 0.8193359375, 269.5, 683.5, 0.71484375, 279.0, 702.0, 0.7294921875], 'scores': None}, '1429': {'keypoints': [99.0, 697.0, 0.355224609375, 95.875, 690.5, 0.40478515625, 95.875, 706.5, 0.40673828125, 99.0, 690.5, 0.86083984375, 99.0, 706.5, 0.9013671875, 121.25, 678.0, 0.84033203125, 124.375, 716.0, 0.8603515625, 153.0, 671.5, 0.80078125, 156.125, 722.5, 0.876953125, 165.625, 678.0, 0.63427734375, 156.125, 709.5, 0.60302734375, 181.5, 684.5, 0.7021484375, 181.5, 709.5, 0.7080078125, 225.875, 687.5, 0.88037109375, 229.125, 713.0, 0.8154296875, 264.0, 684.5, 0.7470703125, 270.25, 700.0, 0.8017578125], 'scores': None}, '1430': {'keypoints': [96.25, 700.0, 0.420654296875, 96.25, 697.0, 0.391845703125, 93.0, 706.5, 0.4248046875, 96.25, 694.0, 0.8876953125, 96.25, 706.5, 0.8994140625, 118.625, 678.0, 0.87158203125, 121.875, 716.0, 0.8623046875, 150.625, 671.5, 0.80126953125, 157.125, 722.5, 0.88232421875, 163.5, 678.0, 0.373046875, 150.625, 710.0, 0.55712890625, 179.5, 687.5, 0.73779296875, 182.75, 710.0, 0.72314453125, 224.375, 687.5, 0.8896484375, 221.125, 716.0, 0.86962890625, 262.75, 684.0, 0.81689453125, 262.75, 700.0, 0.76171875], 'scores': None}, '1431': {'keypoints': [93.375, 708.5, 0.410888671875, 90.1875, 705.5, 0.333251953125, 90.1875, 708.5, 0.51025390625, 93.375, 696.0, 0.8896484375, 93.375, 712.0, 0.89794921875, 116.0625, 683.0, 0.84521484375, 119.25, 715.0, 0.85009765625, 145.125, 673.5, 0.8359375, 154.875, 721.5, 0.8828125, 148.375, 689.5, 0.425048828125, 151.625, 712.0, 0.4189453125, 177.5, 686.0, 0.7177734375, 180.75, 708.5, 0.705078125, 222.75, 689.5, 0.8955078125, 219.5, 721.5, 0.85595703125, 261.5, 683.0, 0.77294921875, 261.5, 705.5, 0.8203125], 'scores': None}, '1432': {'keypoints': [91.5, 717.0, 0.45361328125, 91.5, 707.5, 0.338134765625, 88.25, 717.0, 0.59130859375, 91.5, 698.0, 0.8984375, 91.5, 714.0, 0.89453125, 114.25, 685.0, 0.896484375, 117.5, 714.0, 0.82421875, 140.25, 678.0, 0.8095703125, 153.25, 724.0, 0.83154296875, 146.75, 694.5, 0.344482421875, 153.25, 727.0, 0.32421875, 172.75, 688.0, 0.71337890625, 176.0, 711.0, 0.69580078125, 221.5, 688.0, 0.8564453125, 218.25, 724.0, 0.8583984375, 263.75, 685.0, 0.794921875, 260.5, 714.0, 0.83544921875], 'scores': None}, '1433': {'keypoints': [91.375, 717.0, 0.4599609375, 84.9375, 717.0, 0.3330078125, 88.1875, 717.0, 0.5244140625, 91.375, 701.0, 0.91455078125, 91.375, 717.0, 0.89404296875, 114.0625, 688.0, 0.9169921875, 117.25, 720.0, 0.8349609375, 139.875, 681.5, 0.814453125, 149.625, 723.0, 0.830078125, 146.375, 697.5, 0.38330078125, 149.625, 733.0, 0.5087890625, 175.5, 691.0, 0.70751953125, 178.75, 713.5, 0.70703125, 220.75, 688.0, 0.8203125, 217.5, 723.0, 0.81103515625, 262.75, 685.0, 0.75830078125, 262.75, 723.0, 0.7900390625], 'scores': None}, '1434': {'keypoints': [88.1875, 720.5, 0.59912109375, 84.9375, 720.5, 0.418701171875, 88.1875, 720.5, 0.630859375, 91.375, 704.5, 0.89453125, 91.375, 720.5, 0.88671875, 114.0625, 692.0, 0.88037109375, 120.5, 724.0, 0.83642578125, 139.875, 685.5, 0.771484375, 152.875, 727.0, 0.826171875, 146.375, 704.5, 0.281005859375, 149.625, 733.5, 0.488525390625, 172.25, 695.0, 0.7705078125, 175.5, 717.5, 0.7080078125, 220.75, 692.0, 0.82080078125, 217.5, 720.5, 0.82666015625, 262.75, 685.5, 0.76904296875, 249.875, 733.5, 0.826171875], 'scores': None}, '1435': {'keypoints': [92.25, 726.0, 0.49169921875, 89.0, 726.0, 0.337158203125, 89.0, 726.0, 0.54345703125, 92.25, 710.0, 0.90673828125, 92.25, 726.0, 0.86328125, 114.625, 697.0, 0.89208984375, 117.875, 726.0, 0.86962890625, 140.25, 694.0, 0.77978515625, 149.875, 729.0, 0.8115234375, 156.25, 706.5, 0.2298583984375, 149.875, 735.5, 0.67724609375, 169.125, 697.0, 0.72216796875, 172.25, 719.5, 0.69873046875, 220.375, 697.0, 0.79833984375, 217.125, 726.0, 0.8623046875, 262.0, 687.5, 0.80859375, 252.375, 735.5, 0.7548828125], 'scores': None}, '1436': {'keypoints': [93.0, 724.5, 0.3408203125, 93.0, 718.0, 0.356201171875, 89.875, 731.0, 0.4462890625, 89.875, 718.0, 0.8876953125, 93.0, 734.0, 0.861328125, 112.0625, 702.5, 0.857421875, 118.375, 734.0, 0.85791015625, 137.5, 696.0, 0.822265625, 147.0, 737.0, 0.7373046875, 147.0, 708.5, 0.384033203125, 159.625, 737.0, 0.546875, 169.125, 702.5, 0.70068359375, 172.375, 724.5, 0.71044921875, 216.75, 699.0, 0.7919921875, 216.75, 731.0, 0.8427734375, 261.25, 686.5, 0.78515625, 251.625, 734.0, 0.7548828125], 'scores': None}, '1437': {'keypoints': [87.625, 739.0, 0.548828125, 84.5, 739.0, 0.4140625, 84.5, 739.0, 0.58056640625, 87.625, 723.0, 0.8818359375, 87.625, 739.0, 0.86474609375, 109.5, 710.5, 0.8857421875, 115.75, 739.0, 0.82275390625, 134.5, 704.5, 0.7890625, 147.0, 745.0, 0.78955078125, 143.875, 714.0, 0.40185546875, 128.25, 751.0, 0.454833984375, 172.0, 710.5, 0.69140625, 175.125, 732.5, 0.7158203125, 212.625, 707.5, 0.78955078125, 212.625, 735.5, 0.78857421875, 256.5, 689.0, 0.83935546875, 250.125, 735.5, 0.8408203125], 'scores': None}, '1438': {'keypoints': [85.5, 742.5, 0.51416015625, 82.375, 742.5, 0.372802734375, 82.375, 742.5, 0.61083984375, 85.5, 727.0, 0.833984375, 88.5625, 742.5, 0.83251953125, 107.125, 715.0, 0.826171875, 113.3125, 745.5, 0.82470703125, 135.0, 708.5, 0.841796875, 144.25, 748.5, 0.74755859375, 138.0, 730.0, 0.2391357421875, 122.625, 755.0, 0.619140625, 169.0, 718.0, 0.751953125, 172.125, 739.5, 0.720703125, 206.125, 715.0, 0.8212890625, 212.25, 736.5, 0.7900390625, 246.375, 696.0, 0.77197265625, 252.5, 736.5, 0.81982421875], 'scores': None}, '1439': {'keypoints': [86.375, 753.5, 0.59033203125, 83.3125, 753.5, 0.348876953125, 83.3125, 750.5, 0.6826171875, 86.375, 735.0, 0.8779296875, 86.375, 750.5, 0.880859375, 107.8125, 720.0, 0.896484375, 113.9375, 747.0, 0.7998046875, 132.25, 714.0, 0.7451171875, 141.5, 753.5, 0.70556640625, 147.625, 723.0, 0.38671875, 126.1875, 756.5, 0.360595703125, 166.0, 723.0, 0.6953125, 166.0, 741.0, 0.697265625, 202.75, 723.0, 0.81982421875, 208.875, 741.0, 0.76220703125, 233.375, 707.5, 0.82421875, 248.75, 735.0, 0.79248046875], 'scores': None}, '1440': {'keypoints': [82.3125, 752.0, 0.79736328125, 79.25, 752.0, 0.638671875, 79.25, 752.0, 0.787109375, 82.3125, 737.0, 0.83837890625, 85.375, 752.0, 0.88134765625, 106.8125, 725.0, 0.85302734375, 109.875, 755.0, 0.798828125, 131.25, 718.5, 0.76806640625, 140.5, 761.5, 0.638671875, 146.625, 734.0, 0.361328125, 155.75, 755.0, 0.2900390625, 162.0, 728.0, 0.7177734375, 162.0, 746.0, 0.64794921875, 198.75, 731.0, 0.74267578125, 204.75, 740.0, 0.6953125, 235.5, 718.5, 0.75146484375, 247.75, 740.0, 0.63427734375], 'scores': None}, '1441': {'keypoints': [84.5, 748.0, 0.319580078125, 84.5, 742.0, 0.3994140625, 81.375, 751.0, 0.375, 84.5, 742.0, 0.88720703125, 84.5, 757.0, 0.888671875, 106.0, 729.5, 0.86181640625, 109.125, 757.0, 0.798828125, 130.625, 723.5, 0.79541015625, 139.875, 763.5, 0.7919921875, 143.0, 735.5, 0.3720703125, 149.125, 760.5, 0.38232421875, 161.375, 732.5, 0.744140625, 161.375, 751.0, 0.70361328125, 204.5, 742.0, 0.61572265625, 204.5, 742.0, 0.72216796875, 244.5, 735.5, 0.56201171875, 241.5, 732.5, 0.56787109375], 'scores': None}, '1442': {'keypoints': [81.3125, 758.0, 0.421875, 78.25, 758.0, 0.351806640625, 81.3125, 761.0, 0.51171875, 84.375, 746.0, 0.8076171875, 84.375, 761.0, 0.83984375, 105.8125, 737.0, 0.84375, 108.875, 761.0, 0.779296875, 130.25, 730.5, 0.8076171875, 139.5, 770.0, 0.7744140625, 148.75, 740.0, 0.479248046875, 148.75, 767.0, 0.493408203125, 161.0, 737.0, 0.751953125, 161.0, 755.0, 0.75390625, 203.75, 746.0, 0.58447265625, 203.75, 746.0, 0.759765625, 243.625, 737.0, 0.64404296875, 246.75, 740.0, 0.68115234375], 'scores': None}, '1443': {'keypoints': [82.75, 764.5, 0.478759765625, 80.375, 764.5, 0.417724609375, 80.375, 764.5, 0.59130859375, 85.125, 752.5, 0.8076171875, 85.125, 764.5, 0.84130859375, 104.25, 738.5, 0.82080078125, 109.0, 767.0, 0.7587890625, 130.5, 733.5, 0.8369140625, 137.75, 771.5, 0.70849609375, 142.5, 748.0, 0.36962890625, 152.0, 771.5, 0.5654296875, 156.875, 743.0, 0.64453125, 159.25, 757.5, 0.69140625, 197.5, 750.0, 0.50927734375, 197.5, 750.0, 0.56298828125, 216.625, 755.0, 0.3349609375, 216.625, 743.0, 0.2086181640625], 'scores': None}, '1444': {'keypoints': [83.125, 768.5, 0.79736328125, 78.875, 768.5, 0.66455078125, 81.0, 766.5, 0.81396484375, 85.25, 756.0, 0.76123046875, 83.125, 766.5, 0.73779296875, 104.5, 745.0, 0.759765625, 108.8125, 768.5, 0.7412109375, 132.375, 739.0, 0.796875, 134.5, 768.5, 0.6376953125, 149.5, 745.0, 0.463134765625, 153.75, 771.0, 0.58251953125, 155.875, 747.5, 0.603515625, 158.0, 760.0, 0.6142578125, 192.25, 758.0, 0.3369140625, 200.875, 756.0, 0.5068359375, 200.875, 758.0, 0.2127685546875, 200.875, 756.0, 0.1025390625], 'scores': None}, '1445': {'keypoints': [83.375, 771.0, 0.491455078125, 79.25, 771.0, 0.50244140625, 79.25, 771.0, 0.546875, 83.375, 759.0, 0.77587890625, 83.375, 773.0, 0.73095703125, 103.875, 748.5, 0.84130859375, 107.9375, 775.0, 0.73876953125, 132.5, 744.5, 0.60009765625, 130.5, 773.0, 0.6748046875, 153.0, 748.5, 0.5341796875, 159.125, 775.0, 0.255126953125, 157.0, 753.0, 0.65478515625, 157.0, 765.0, 0.5927734375, 187.75, 763.0, 0.2666015625, 196.0, 761.0, 0.43408203125, 193.875, 763.0, 0.1934814453125, 196.0, 761.0, 0.1129150390625], 'scores': None}, '1446': {'keypoints': [81.25, 778.0, 0.456787109375, 75.125, 778.0, 0.396484375, 77.1875, 778.0, 0.61572265625, 81.25, 764.0, 0.76171875, 81.25, 776.0, 0.77587890625, 101.5625, 756.0, 0.7900390625, 103.625, 776.0, 0.7578125, 128.0, 749.5, 0.66015625, 130.0, 776.0, 0.6767578125, 146.25, 756.0, 0.42626953125, 150.25, 780.5, 0.321044921875, 152.375, 756.0, 0.66552734375, 152.375, 770.0, 0.68505859375, 188.875, 766.0, 0.423583984375, 191.0, 772.0, 0.53857421875, 193.0, 766.0, 0.10552978515625, 193.0, 772.0, 0.07464599609375], 'scores': None}, '1447': {'keypoints': [78.25, 780.0, 0.2239990234375, 72.875, 776.0, 0.1942138671875, 72.875, 780.0, 0.333251953125, 78.25, 769.0, 0.72509765625, 78.25, 780.0, 0.62451171875, 99.625, 760.0, 0.8056640625, 101.375, 778.0, 0.75146484375, 122.75, 754.5, 0.5673828125, 121.0, 776.0, 0.6337890625, 133.5, 760.0, 0.1788330078125, 137.0, 769.0, 0.114501953125, 147.75, 760.0, 0.6689453125, 147.75, 771.0, 0.619140625, 174.5, 765.5, 0.307861328125, 176.25, 771.0, 0.272705078125, 174.5, 767.0, 0.06341552734375, 176.25, 772.5, 0.08477783203125], 'scores': None}, '1448': {'keypoints': [72.0, 780.0, 0.1624755859375, 68.0, 780.0, 0.221923828125, 72.0, 780.0, 0.247802734375, 78.0, 774.0, 0.7021484375, 78.0, 786.0, 0.5673828125, 96.0, 766.0, 0.8125, 98.0, 782.0, 0.654296875, 122.0, 760.0, 0.61083984375, 122.0, 782.0, 0.45849609375, 140.0, 768.0, 0.242919921875, 144.0, 772.0, 0.2120361328125, 152.0, 766.0, 0.76416015625, 152.0, 776.0, 0.61962890625, 182.0, 774.0, 0.56005859375, 182.0, 780.0, 0.364990234375, 188.0, 776.0, 0.11944580078125, 186.0, 780.0, 0.1334228515625], 'scores': None}, '1449': {'keypoints': [70.625, 782.5, 0.662109375, 67.0625, 786.0, 0.65478515625, 68.8125, 781.0, 0.71337890625, 67.0625, 789.5, 0.349609375, 70.625, 779.0, 0.302978515625, 95.3125, 770.0, 0.56787109375, 95.3125, 784.0, 0.58544921875, 123.5625, 765.0, 0.62939453125, 144.75, 772.0, 0.2900390625, 135.875, 770.0, 0.1748046875, 90.0, 793.0, 0.07049560546875, 146.5, 774.0, 0.1944580078125, 146.5, 779.0, 0.135009765625, 167.75, 775.5, 0.060516357421875, 95.3125, 777.0, 0.05413818359375, 144.75, 772.0, 0.23095703125, 141.25, 772.0, 0.311279296875], 'scores': None}}\n",
      "\n",
      "============================================================\n",
      "TEST FOLDER\n",
      "============================================================\n",
      "Total files: 47\n",
      "First 5 files: ['01_0222_alphapose_tracked_person.json', '01_0225_alphapose_tracked_person.json', '01_0240_alphapose_tracked_person.json', '01_0242_alphapose_tracked_person.json', '01_0245_alphapose_tracked_person.json']\n",
      "\n",
      "Sample file: 01_0222_alphapose_tracked_person.json\n",
      "File size: 45.65 KB\n",
      "Type: <class 'dict'>\n",
      "Structure: ['3']\n",
      "\n",
      "First entry key: 3\n",
      "First entry value: {'7': {'keypoints': [517.5, 1282.0, 0.0047607421875, 455.0, 1282.0, 0.00984954833984375, 493.75, 1252.0, 0.004199981689453125, 443.0, 1258.0, 0.233154296875, 446.0, 1282.0, 0.100830078125, 464.0, 1261.0, 0.486083984375, 496.75, 1261.0, 0.6181640625, 541.5, 1238.0, 0.110595703125, 550.5, 1223.0, 0.73046875, 589.0, 1229.0, 0.11627197265625, 586.0, 1250.0, 0.7431640625, 547.5, 1176.0, 0.1685791015625, 583.5, 1184.0, 0.27099609375, 631.0, 1152.0, 0.09625244140625, 631.0, 1158.0, 0.10931396484375, 601.0, 1146.0, 0.0645751953125, 628.0, 1161.0, 0.046295166015625], 'scores': None}, '8': {'keypoints': [548.0, 1279.0, 0.0024433135986328125, 548.0, 1279.0, 0.005443572998046875, 500.5, 1252.0, 0.006832122802734375, 450.0, 1282.0, 0.047088623046875, 453.0, 1282.0, 0.099853515625, 470.75, 1270.0, 0.494873046875, 506.5, 1264.0, 0.5830078125, 545.0, 1238.0, 0.1383056640625, 554.0, 1220.0, 0.81201171875, 545.0, 1282.0, 0.10589599609375, 592.5, 1247.0, 0.71923828125, 542.0, 1173.0, 0.340087890625, 566.0, 1184.0, 0.348876953125, 589.5, 1208.0, 0.076904296875, 619.0, 1178.0, 0.10675048828125, 589.5, 1143.0, 0.08258056640625, 637.0, 1176.0, 0.068603515625], 'scores': None}, '9': {'keypoints': [538.5, 1282.0, 0.019561767578125, 467.75, 1276.0, 0.024993896484375, 538.5, 1282.0, 0.01023101806640625, 473.75, 1282.0, 0.11944580078125, 462.0, 1282.0, 0.068359375, 476.75, 1267.0, 0.4384765625, 518.0, 1261.0, 0.63525390625, 556.5, 1244.0, 0.2568359375, 562.5, 1217.0, 0.7998046875, 580.0, 1267.0, 0.156982421875, 603.5, 1247.0, 0.75048828125, 538.5, 1170.0, 0.2275390625, 565.5, 1184.0, 0.265869140625, 583.0, 1270.0, 0.09417724609375, 621.5, 1178.0, 0.1080322265625, 606.5, 1214.0, 0.0755615234375, 627.5, 1181.0, 0.0625], 'scores': None}, '10': {'keypoints': [550.0, 1280.0, 0.153076171875, 547.5, 1280.0, 0.072998046875, 544.5, 1280.0, 0.09814453125, 473.0, 1260.0, 0.0057525634765625, 539.0, 1280.0, 0.09149169921875, 487.25, 1266.0, 0.368896484375, 527.5, 1258.0, 0.5849609375, 561.5, 1243.0, 0.432861328125, 570.0, 1215.0, 0.73779296875, 590.0, 1269.0, 0.56640625, 610.0, 1243.0, 0.6181640625, 539.0, 1170.0, 0.172119140625, 553.0, 1178.0, 0.2236328125, 621.5, 1204.0, 0.1466064453125, 644.5, 1172.0, 0.1434326171875, 599.0, 1164.0, 0.052947998046875, 653.0, 1167.0, 0.07794189453125], 'scores': None}, '11': {'keypoints': [563.0, 1278.0, 0.08074951171875, 557.5, 1278.0, 0.069091796875, 555.0, 1278.0, 0.05792236328125, 503.0, 1278.0, 0.022308349609375, 544.0, 1278.0, 0.09320068359375, 497.5, 1265.0, 0.4775390625, 535.5, 1254.0, 0.60107421875, 563.0, 1246.0, 0.1798095703125, 576.5, 1211.0, 0.75341796875, 609.5, 1265.0, 0.305419921875, 615.0, 1235.0, 0.615234375, 533.0, 1175.0, 0.144287109375, 546.5, 1175.0, 0.1961669921875, 623.0, 1205.0, 0.1273193359375, 634.0, 1194.0, 0.142822265625, 617.5, 1213.0, 0.09375, 653.5, 1170.0, 0.07891845703125], 'scores': None}, '12': {'keypoints': [566.0, 1274.0, 0.006221771240234375, 563.5, 1274.0, 0.01120758056640625, 556.0, 1274.0, 0.019287109375, 505.25, 1274.0, 0.0217132568359375, 558.5, 1274.0, 0.07244873046875, 505.25, 1267.0, 0.3759765625, 540.5, 1249.0, 0.50634765625, 568.5, 1254.0, 0.264892578125, 583.5, 1208.0, 0.70263671875, 599.0, 1274.0, 0.41064453125, 624.0, 1231.0, 0.60986328125, 513.0, 1186.0, 0.1385498046875, 561.0, 1173.0, 0.148681640625, 609.0, 1244.0, 0.07440185546875, 634.5, 1201.0, 0.1240234375, 624.0, 1224.0, 0.08953857421875, 649.5, 1193.0, 0.07672119140625], 'scores': None}, '13': {'keypoints': [578.5, 1273.0, 0.015960693359375, 578.5, 1273.0, 0.01000213623046875, 576.0, 1265.0, 0.01154327392578125, 576.0, 1273.0, 0.0013828277587890625, 571.0, 1270.0, 0.060333251953125, 514.5, 1265.0, 0.359130859375, 549.0, 1250.0, 0.46728515625, 581.0, 1226.0, 0.134765625, 590.5, 1206.0, 0.70068359375, 610.5, 1255.0, 0.312255859375, 630.0, 1231.0, 0.56787109375, 512.0, 1186.0, 0.1767578125, 541.5, 1174.0, 0.21240234375, 571.0, 1263.0, 0.054595947265625, 644.5, 1191.0, 0.1258544921875, 625.0, 1221.0, 0.0772705078125, 649.5, 1194.0, 0.09576416015625], 'scores': None}, '14': {'keypoints': [583.5, 1272.0, 0.04290771484375, 576.0, 1272.0, 0.019256591796875, 578.5, 1272.0, 0.0283355712890625, 578.5, 1272.0, 0.0022449493408203125, 573.5, 1269.0, 0.060211181640625, 517.5, 1264.0, 0.337158203125, 551.5, 1250.0, 0.48388671875, 573.5, 1272.0, 0.1456298828125, 593.0, 1204.0, 0.7109375, 605.0, 1247.0, 0.490966796875, 632.0, 1223.0, 0.3984375, 517.5, 1187.0, 0.1983642578125, 542.0, 1175.0, 0.2127685546875, 646.5, 1209.0, 0.10101318359375, 646.5, 1192.0, 0.142333984375, 649.0, 1204.0, 0.05255126953125, 651.5, 1175.0, 0.061126708984375], 'scores': None}, '15': {'keypoints': [589.0, 1271.0, 0.194091796875, 587.0, 1271.0, 0.130615234375, 587.0, 1271.0, 0.1109619140625, 596.5, 1271.0, 0.0078887939453125, 584.5, 1268.0, 0.0882568359375, 518.5, 1263.0, 0.33544921875, 557.5, 1249.0, 0.41162109375, 584.5, 1266.0, 0.136962890625, 599.0, 1203.0, 0.73681640625, 613.5, 1251.0, 0.10870361328125, 638.0, 1225.0, 0.45654296875, 516.0, 1181.0, 0.2841796875, 535.5, 1179.0, 0.258056640625, 596.5, 1203.0, 0.06585693359375, 652.5, 1203.0, 0.1451416015625, 633.0, 1239.0, 0.0631103515625, 652.5, 1174.0, 0.0799560546875], 'scores': None}, '16': {'keypoints': [589.5, 1272.0, 0.22314453125, 587.0, 1272.0, 0.1173095703125, 585.0, 1272.0, 0.14306640625, 529.0, 1272.0, 0.0155029296875, 577.5, 1269.0, 0.09307861328125, 522.0, 1264.0, 0.388427734375, 565.5, 1245.0, 0.442626953125, 580.0, 1262.0, 0.27734375, 604.0, 1202.0, 0.68017578125, 616.0, 1255.0, 0.1553955078125, 640.5, 1230.0, 0.49462890625, 514.5, 1182.0, 0.2744140625, 536.5, 1175.0, 0.2445068359375, 599.5, 1199.0, 0.052093505859375, 652.5, 1204.0, 0.1259765625, 623.5, 1233.0, 0.073974609375, 643.0, 1235.0, 0.08087158203125], 'scores': None}, '17': {'keypoints': [593.0, 1273.0, 0.07647705078125, 593.0, 1273.0, 0.021087646484375, 590.5, 1273.0, 0.06024169921875, 528.5, 1273.0, 0.06988525390625, 575.5, 1273.0, 0.034912109375, 521.0, 1260.0, 0.43212890625, 568.0, 1246.0, 0.4111328125, 578.0, 1268.0, 0.144287109375, 607.5, 1199.0, 0.7080078125, 615.0, 1255.0, 0.160888671875, 644.5, 1221.0, 0.28173828125, 516.5, 1186.0, 0.349853515625, 536.0, 1174.0, 0.301513671875, 610.0, 1199.0, 0.051666259765625, 644.5, 1199.0, 0.1253662109375, 637.0, 1253.0, 0.051422119140625, 652.0, 1214.0, 0.08343505859375], 'scores': None}, '18': {'keypoints': [594.5, 1274.0, 0.164794921875, 594.5, 1274.0, 0.08551025390625, 594.5, 1274.0, 0.09259033203125, 536.5, 1274.0, 0.1773681640625, 569.5, 1274.0, 0.053192138671875, 521.5, 1256.0, 0.5087890625, 572.0, 1244.0, 0.42236328125, 569.5, 1244.0, 0.1307373046875, 609.5, 1198.0, 0.75244140625, 612.0, 1256.0, 0.25, 642.5, 1221.0, 0.439697265625, 516.5, 1186.0, 0.373046875, 536.5, 1174.0, 0.376220703125, 577.0, 1186.0, 0.0908203125, 645.0, 1204.0, 0.1708984375, 640.0, 1244.0, 0.0787353515625, 642.5, 1236.0, 0.12744140625], 'scores': None}, '19': {'keypoints': [596.0, 1274.0, 0.19384765625, 596.0, 1274.0, 0.10577392578125, 590.5, 1274.0, 0.14697265625, 537.0, 1274.0, 0.220703125, 585.5, 1274.0, 0.098876953125, 522.0, 1257.0, 0.55908203125, 570.5, 1244.0, 0.451171875, 580.5, 1249.0, 0.08697509765625, 611.0, 1198.0, 0.77783203125, 603.5, 1246.0, 0.124267578125, 644.0, 1224.0, 0.51611328125, 517.0, 1186.0, 0.353759765625, 537.0, 1173.0, 0.3994140625, 578.0, 1186.0, 0.1529541015625, 590.5, 1186.0, 0.205078125, 641.5, 1244.0, 0.054931640625, 644.0, 1236.0, 0.1307373046875], 'scores': None}, '20': {'keypoints': [597.0, 1274.0, 0.2176513671875, 597.0, 1274.0, 0.148681640625, 597.0, 1274.0, 0.1893310546875, 540.0, 1274.0, 0.2088623046875, 586.5, 1274.0, 0.18408203125, 522.0, 1256.0, 0.59033203125, 571.0, 1243.0, 0.473388671875, 571.0, 1259.0, 0.0814208984375, 612.5, 1197.0, 0.73388671875, 615.0, 1256.0, 0.1512451171875, 646.0, 1225.0, 0.46240234375, 512.0, 1187.0, 0.322998046875, 537.5, 1171.0, 0.3818359375, 579.0, 1184.0, 0.1572265625, 592.0, 1184.0, 0.257080078125, 646.0, 1236.0, 0.045196533203125, 643.5, 1236.0, 0.10546875], 'scores': None}, '21': {'keypoints': [596.5, 1271.0, 0.59619140625, 599.5, 1276.0, 0.362548828125, 594.0, 1276.0, 0.537109375, 538.5, 1276.0, 0.1917724609375, 589.0, 1276.0, 0.329833984375, 523.0, 1258.0, 0.529296875, 573.0, 1242.0, 0.515625, 586.0, 1240.0, 0.08306884765625, 615.0, 1198.0, 0.72900390625, 641.5, 1245.0, 0.1280517578125, 647.0, 1224.0, 0.51318359375, 512.0, 1184.0, 0.31640625, 538.5, 1171.0, 0.40771484375, 581.0, 1184.0, 0.1593017578125, 589.0, 1184.0, 0.248291015625, 644.0, 1234.0, 0.03985595703125, 649.5, 1237.0, 0.0914306640625], 'scores': None}, '22': {'keypoints': [598.0, 1271.0, 0.546875, 600.5, 1276.0, 0.3583984375, 595.0, 1276.0, 0.53173828125, 542.0, 1276.0, 0.2430419921875, 590.0, 1276.0, 0.340576171875, 523.0, 1258.0, 0.5673828125, 574.0, 1242.0, 0.5380859375, 584.5, 1236.0, 0.08966064453125, 614.0, 1199.0, 0.74658203125, 614.0, 1252.0, 0.373291015625, 646.0, 1220.0, 0.5751953125, 512.5, 1186.0, 0.345947265625, 539.0, 1175.0, 0.400634765625, 582.0, 1183.0, 0.167724609375, 584.5, 1183.0, 0.262451171875, 576.5, 1162.0, 0.033111572265625, 651.5, 1236.0, 0.07452392578125], 'scores': None}, '23': {'keypoints': [598.5, 1271.0, 0.61962890625, 598.5, 1276.0, 0.3701171875, 596.0, 1276.0, 0.5517578125, 542.0, 1276.0, 0.1710205078125, 587.5, 1276.0, 0.3544921875, 523.0, 1258.0, 0.55517578125, 574.0, 1242.0, 0.52587890625, 579.5, 1231.0, 0.0799560546875, 614.5, 1199.0, 0.75390625, 614.5, 1252.0, 0.25390625, 647.0, 1220.0, 0.6064453125, 512.5, 1186.0, 0.3564453125, 539.5, 1175.0, 0.398193359375, 582.5, 1183.0, 0.168701171875, 585.0, 1183.0, 0.2452392578125, 577.0, 1162.0, 0.04962158203125, 652.0, 1236.0, 0.047576904296875], 'scores': None}, '24': {'keypoints': [597.5, 1276.0, 0.54443359375, 600.0, 1276.0, 0.36572265625, 595.0, 1276.0, 0.45556640625, 541.0, 1274.0, 0.294921875, 584.0, 1276.0, 0.28759765625, 525.0, 1258.0, 0.52392578125, 573.0, 1242.0, 0.493408203125, 589.5, 1260.0, 0.063720703125, 613.5, 1199.0, 0.763671875, 646.0, 1226.0, 0.091552734375, 651.0, 1220.0, 0.533203125, 511.5, 1186.0, 0.343505859375, 538.5, 1170.0, 0.38427734375, 581.5, 1188.0, 0.1866455078125, 640.5, 1188.0, 0.1658935546875, 570.5, 1162.0, 0.0506591796875, 560.0, 1151.0, 0.036590576171875], 'scores': None}, '25': {'keypoints': [597.0, 1276.0, 0.578125, 599.5, 1276.0, 0.389404296875, 594.0, 1276.0, 0.50927734375, 541.0, 1274.0, 0.337158203125, 583.5, 1276.0, 0.28955078125, 522.0, 1258.0, 0.55517578125, 575.5, 1242.0, 0.5009765625, 583.5, 1220.0, 0.054962158203125, 613.0, 1199.0, 0.75341796875, 645.0, 1234.0, 0.12310791015625, 647.5, 1220.0, 0.650390625, 516.5, 1183.0, 0.370849609375, 538.0, 1170.0, 0.40576171875, 581.0, 1188.0, 0.18212890625, 589.0, 1183.0, 0.165771484375, 573.0, 1162.0, 0.055145263671875, 573.0, 1151.0, 0.041229248046875], 'scores': None}, '26': {'keypoints': [597.0, 1276.0, 0.5751953125, 599.5, 1276.0, 0.38330078125, 594.0, 1276.0, 0.50048828125, 541.0, 1274.0, 0.284912109375, 583.5, 1276.0, 0.290771484375, 522.0, 1258.0, 0.56005859375, 575.5, 1242.0, 0.49609375, 565.0, 1250.0, 0.06365966796875, 613.0, 1199.0, 0.7470703125, 607.5, 1226.0, 0.1309814453125, 650.5, 1220.0, 0.6396484375, 516.5, 1183.0, 0.3720703125, 538.0, 1170.0, 0.395751953125, 581.0, 1188.0, 0.1715087890625, 589.0, 1183.0, 0.1397705078125, 573.0, 1162.0, 0.046600341796875, 559.5, 1151.0, 0.0340576171875], 'scores': None}, '27': {'keypoints': [597.5, 1276.0, 0.5439453125, 600.0, 1276.0, 0.364501953125, 595.0, 1276.0, 0.4453125, 541.0, 1274.0, 0.1953125, 584.0, 1276.0, 0.278076171875, 522.0, 1258.0, 0.481689453125, 573.0, 1242.0, 0.482421875, 565.0, 1250.0, 0.068359375, 613.5, 1199.0, 0.76708984375, 597.5, 1239.0, 0.12017822265625, 651.0, 1223.0, 0.56787109375, 517.0, 1183.0, 0.362548828125, 538.5, 1170.0, 0.39697265625, 581.5, 1188.0, 0.1724853515625, 589.5, 1183.0, 0.137451171875, 573.0, 1258.0, 0.041534423828125, 659.0, 1212.0, 0.061737060546875], 'scores': None}, '28': {'keypoints': [599.5, 1276.0, 0.57470703125, 599.5, 1276.0, 0.373779296875, 594.0, 1276.0, 0.50048828125, 538.0, 1274.0, 0.2471923828125, 583.5, 1276.0, 0.297119140625, 522.0, 1258.0, 0.51220703125, 573.0, 1244.0, 0.498291015625, 591.5, 1215.0, 0.06475830078125, 613.0, 1199.0, 0.75341796875, 645.0, 1223.0, 0.123291015625, 650.5, 1223.0, 0.6015625, 516.5, 1183.0, 0.3779296875, 538.0, 1170.0, 0.404052734375, 581.0, 1188.0, 0.1846923828125, 589.0, 1183.0, 0.2235107421875, 658.5, 1212.0, 0.081787109375, 658.5, 1212.0, 0.06005859375], 'scores': None}, '29': {'keypoints': [597.0, 1276.0, 0.54150390625, 599.5, 1276.0, 0.34326171875, 594.0, 1276.0, 0.4501953125, 538.0, 1274.0, 0.45068359375, 583.5, 1276.0, 0.27587890625, 522.0, 1258.0, 0.49853515625, 573.0, 1244.0, 0.4775390625, 583.5, 1239.0, 0.0728759765625, 615.5, 1199.0, 0.74267578125, 597.0, 1239.0, 0.145263671875, 650.5, 1226.0, 0.529296875, 516.5, 1183.0, 0.374267578125, 538.0, 1170.0, 0.400146484375, 581.0, 1188.0, 0.176025390625, 589.0, 1183.0, 0.2283935546875, 658.5, 1212.0, 0.050933837890625, 653.0, 1236.0, 0.06732177734375], 'scores': None}, '30': {'keypoints': [597.0, 1276.0, 0.5224609375, 599.5, 1276.0, 0.321533203125, 594.0, 1276.0, 0.410400390625, 535.5, 1276.0, 0.35205078125, 583.5, 1276.0, 0.2548828125, 522.0, 1258.0, 0.56005859375, 573.0, 1247.0, 0.491455078125, 578.0, 1239.0, 0.0748291015625, 613.0, 1199.0, 0.755859375, 597.0, 1239.0, 0.3681640625, 647.5, 1226.0, 0.583984375, 516.5, 1183.0, 0.365234375, 538.0, 1170.0, 0.38720703125, 581.0, 1188.0, 0.191162109375, 589.0, 1183.0, 0.1795654296875, 655.5, 1212.0, 0.0767822265625, 650.5, 1236.0, 0.07733154296875], 'scores': None}, '31': {'keypoints': [597.0, 1276.0, 0.493896484375, 599.5, 1276.0, 0.29931640625, 591.5, 1276.0, 0.37109375, 537.5, 1276.0, 0.2391357421875, 583.5, 1276.0, 0.251220703125, 521.5, 1257.0, 0.5205078125, 572.5, 1246.0, 0.469970703125, 581.0, 1222.0, 0.0887451171875, 616.0, 1200.0, 0.79248046875, 613.0, 1225.0, 0.269287109375, 648.5, 1230.0, 0.3369140625, 510.5, 1187.0, 0.31103515625, 537.5, 1173.0, 0.383056640625, 581.0, 1187.0, 0.1947021484375, 589.0, 1182.0, 0.138671875, 572.5, 1257.0, 0.03692626953125, 656.5, 1211.0, 0.077880859375], 'scores': None}, '32': {'keypoints': [597.0, 1276.0, 0.390625, 599.5, 1276.0, 0.23681640625, 591.5, 1276.0, 0.263671875, 540.0, 1276.0, 0.1337890625, 583.5, 1276.0, 0.1888427734375, 521.5, 1257.0, 0.52685546875, 572.5, 1246.0, 0.4931640625, 591.5, 1265.0, 0.166748046875, 616.0, 1200.0, 0.7548828125, 645.5, 1236.0, 0.083740234375, 651.0, 1228.0, 0.49267578125, 516.0, 1187.0, 0.3095703125, 537.5, 1173.0, 0.40869140625, 581.0, 1187.0, 0.17236328125, 589.0, 1187.0, 0.1436767578125, 654.0, 1211.0, 0.043731689453125, 659.0, 1211.0, 0.0732421875], 'scores': None}, '33': {'keypoints': [597.0, 1277.0, 0.46142578125, 599.5, 1277.0, 0.313720703125, 594.5, 1277.0, 0.37939453125, 537.5, 1277.0, 0.1336669921875, 583.5, 1277.0, 0.252197265625, 521.5, 1258.0, 0.489013671875, 572.5, 1248.0, 0.489501953125, 589.0, 1261.0, 0.0919189453125, 616.0, 1201.0, 0.79833984375, 645.5, 1234.0, 0.103271484375, 651.0, 1229.0, 0.5078125, 510.5, 1185.0, 0.3271484375, 537.5, 1174.0, 0.402099609375, 581.0, 1188.0, 0.1649169921875, 589.0, 1188.0, 0.1920166015625, 572.5, 1161.0, 0.0465087890625, 659.0, 1212.0, 0.04913330078125], 'scores': None}, '34': {'keypoints': [598.0, 1276.0, 0.35693359375, 601.0, 1276.0, 0.2183837890625, 592.5, 1276.0, 0.259521484375, 541.0, 1276.0, 0.202880859375, 587.0, 1276.0, 0.2037353515625, 524.5, 1257.0, 0.461669921875, 571.0, 1246.0, 0.469482421875, 582.0, 1230.0, 0.114013671875, 614.5, 1200.0, 0.79833984375, 614.5, 1230.0, 0.64697265625, 647.5, 1225.0, 0.402099609375, 505.25, 1192.0, 0.313232421875, 535.5, 1179.0, 0.393310546875, 582.0, 1192.0, 0.1630859375, 584.5, 1187.0, 0.2449951171875, 601.0, 1160.0, 0.043914794921875, 653.0, 1236.0, 0.06280517578125], 'scores': None}, '35': {'keypoints': [600.5, 1277.0, 0.50439453125, 600.5, 1277.0, 0.3330078125, 595.0, 1277.0, 0.40185546875, 540.5, 1277.0, 0.10845947265625, 584.0, 1277.0, 0.26416015625, 524.0, 1258.0, 0.492919921875, 576.0, 1248.0, 0.50927734375, 578.5, 1231.0, 0.1412353515625, 614.0, 1201.0, 0.81396484375, 614.0, 1231.0, 0.55078125, 633.0, 1226.0, 0.46044921875, 505.25, 1193.0, 0.337158203125, 538.0, 1174.0, 0.3984375, 581.5, 1193.0, 0.1605224609375, 589.5, 1188.0, 0.329345703125, 576.0, 1161.0, 0.048919677734375, 652.0, 1237.0, 0.0775146484375], 'scores': None}, '36': {'keypoints': [596.0, 1277.0, 0.1817626953125, 604.0, 1277.0, 0.1580810546875, 596.0, 1277.0, 0.1468505859375, 539.0, 1277.0, 0.297607421875, 582.5, 1277.0, 0.1654052734375, 528.5, 1264.0, 0.49267578125, 569.0, 1248.0, 0.45751953125, 577.0, 1245.0, 0.190673828125, 612.0, 1201.0, 0.7783203125, 612.0, 1234.0, 0.52685546875, 650.0, 1229.0, 0.61865234375, 504.0, 1193.0, 0.363525390625, 536.5, 1180.0, 0.42236328125, 580.0, 1193.0, 0.153076171875, 588.0, 1188.0, 0.374755859375, 612.0, 1229.0, 0.04986572265625, 650.0, 1237.0, 0.054412841796875], 'scores': None}, '37': {'keypoints': [604.0, 1276.0, 0.145751953125, 604.0, 1276.0, 0.10009765625, 604.0, 1276.0, 0.11102294921875, 531.5, 1276.0, 0.07373046875, 590.5, 1276.0, 0.12200927734375, 529.0, 1268.0, 0.4501953125, 569.0, 1247.0, 0.428955078125, 582.5, 1244.0, 0.1802978515625, 609.0, 1202.0, 0.77197265625, 617.0, 1236.0, 0.30224609375, 646.5, 1228.0, 0.62255859375, 505.0, 1194.0, 0.342529296875, 537.0, 1180.0, 0.418701171875, 580.0, 1194.0, 0.1500244140625, 588.0, 1188.0, 0.328125, 612.0, 1228.0, 0.1536865234375, 665.5, 1212.0, 0.11322021484375], 'scores': None}, '38': {'keypoints': [601.0, 1277.0, 0.1700439453125, 603.5, 1277.0, 0.1121826171875, 595.5, 1277.0, 0.10986328125, 539.0, 1277.0, 0.0833740234375, 590.0, 1277.0, 0.1346435546875, 528.0, 1264.0, 0.423095703125, 571.0, 1248.0, 0.495849609375, 582.0, 1245.0, 0.240478515625, 609.0, 1203.0, 0.7861328125, 622.5, 1237.0, 0.364013671875, 646.5, 1229.0, 0.6689453125, 504.0, 1195.0, 0.32958984375, 536.5, 1181.0, 0.416748046875, 579.5, 1195.0, 0.1517333984375, 590.0, 1189.0, 0.32373046875, 611.5, 1227.0, 0.07958984375, 665.5, 1213.0, 0.0504150390625], 'scores': None}, '39': {'keypoints': [590.0, 1276.0, 0.10443115234375, 601.0, 1276.0, 0.0638427734375, 603.5, 1276.0, 0.08758544921875, 528.0, 1276.0, 0.0297393798828125, 590.0, 1276.0, 0.10076904296875, 528.0, 1268.0, 0.4208984375, 568.5, 1252.0, 0.438232421875, 582.0, 1244.0, 0.218994140625, 609.0, 1204.0, 0.751953125, 622.5, 1239.0, 0.3388671875, 646.5, 1231.0, 0.5205078125, 504.0, 1194.0, 0.313720703125, 536.5, 1180.0, 0.421630859375, 579.5, 1194.0, 0.1173095703125, 587.5, 1188.0, 0.2100830078125, 609.0, 1228.0, 0.0406494140625, 646.5, 1180.0, 0.07000732421875], 'scores': None}, '40': {'keypoints': [596.0, 1277.0, 0.1689453125, 596.0, 1277.0, 0.1026611328125, 596.0, 1277.0, 0.09320068359375, 528.5, 1277.0, 0.03936767578125, 585.0, 1277.0, 0.1240234375, 528.5, 1269.0, 0.419189453125, 569.0, 1250.0, 0.482177734375, 582.5, 1245.0, 0.220947265625, 607.0, 1204.0, 0.78759765625, 623.0, 1239.0, 0.3701171875, 647.5, 1231.0, 0.60986328125, 509.5, 1193.0, 0.3349609375, 536.5, 1180.0, 0.412841796875, 580.0, 1188.0, 0.142822265625, 588.0, 1188.0, 0.2626953125, 631.0, 1223.0, 0.043609619140625, 647.5, 1191.0, 0.0567626953125], 'scores': None}, '41': {'keypoints': [599.5, 1278.0, 0.304443359375, 605.0, 1278.0, 0.2147216796875, 596.5, 1278.0, 0.236083984375, 531.5, 1278.0, 0.023406982421875, 588.5, 1278.0, 0.2291259765625, 528.5, 1270.0, 0.39208984375, 572.0, 1251.0, 0.4892578125, 583.0, 1246.0, 0.229248046875, 607.5, 1205.0, 0.79931640625, 624.0, 1240.0, 0.2391357421875, 645.5, 1230.0, 0.63671875, 509.5, 1194.0, 0.349365234375, 534.0, 1181.0, 0.412353515625, 580.5, 1189.0, 0.1416015625, 629.0, 1189.0, 0.360107421875, 634.5, 1192.0, 0.04083251953125, 648.0, 1192.0, 0.05999755859375], 'scores': None}, '42': {'keypoints': [603.5, 1277.0, 0.243408203125, 603.5, 1277.0, 0.166015625, 595.5, 1277.0, 0.1669921875, 531.0, 1277.0, 0.0289306640625, 590.0, 1277.0, 0.241943359375, 528.0, 1269.0, 0.390869140625, 571.0, 1251.0, 0.486083984375, 582.0, 1245.0, 0.2122802734375, 606.0, 1205.0, 0.79443359375, 622.5, 1237.0, 0.24609375, 644.0, 1229.0, 0.544921875, 504.0, 1195.0, 0.313720703125, 536.5, 1181.0, 0.404296875, 587.5, 1195.0, 0.1595458984375, 587.5, 1189.0, 0.2578125, 657.0, 1235.0, 0.056793212890625, 646.5, 1192.0, 0.0494384765625], 'scores': None}, '43': {'keypoints': [600.0, 1277.0, 0.179931640625, 600.0, 1277.0, 0.0877685546875, 595.0, 1277.0, 0.0911865234375, 525.5, 1277.0, 0.041748046875, 589.5, 1277.0, 0.14501953125, 528.0, 1269.0, 0.430908203125, 568.0, 1251.0, 0.451904296875, 600.0, 1216.0, 0.08984375, 605.5, 1205.0, 0.744140625, 629.5, 1235.0, 0.13330078125, 645.5, 1235.0, 0.490478515625, 504.0, 1195.0, 0.2646484375, 536.0, 1181.0, 0.42333984375, 587.0, 1189.0, 0.165283203125, 621.5, 1189.0, 0.1590576171875, 656.5, 1235.0, 0.053863525390625, 664.5, 1213.0, 0.05181884765625], 'scores': None}, '44': {'keypoints': [590.5, 1278.0, 0.12109375, 590.5, 1278.0, 0.059539794921875, 596.0, 1278.0, 0.07489013671875, 528.5, 1278.0, 0.03570556640625, 590.5, 1278.0, 0.11614990234375, 528.5, 1270.0, 0.456787109375, 569.0, 1254.0, 0.47802734375, 577.0, 1243.0, 0.1441650390625, 607.0, 1205.0, 0.787109375, 596.0, 1243.0, 0.1988525390625, 644.5, 1232.0, 0.57080078125, 509.5, 1194.0, 0.304931640625, 531.0, 1181.0, 0.3857421875, 588.0, 1194.0, 0.17236328125, 623.0, 1189.0, 0.2431640625, 634.0, 1192.0, 0.0562744140625, 647.5, 1181.0, 0.072265625], 'scores': None}, '45': {'keypoints': [597.0, 1279.0, 0.1287841796875, 599.5, 1279.0, 0.09088134765625, 591.0, 1279.0, 0.11163330078125, 531.0, 1279.0, 0.02593994140625, 588.5, 1279.0, 0.2071533203125, 528.0, 1271.0, 0.4228515625, 566.5, 1254.0, 0.51953125, 583.0, 1244.0, 0.09490966796875, 602.0, 1208.0, 0.8154296875, 638.0, 1238.0, 0.1077880859375, 641.0, 1227.0, 0.6044921875, 503.25, 1194.0, 0.292724609375, 533.5, 1186.0, 0.38818359375, 580.0, 1194.0, 0.18310546875, 588.5, 1188.0, 0.350830078125, 635.0, 1197.0, 0.06585693359375, 649.0, 1191.0, 0.09759521484375], 'scores': None}, '46': {'keypoints': [595.0, 1278.0, 0.10888671875, 597.5, 1278.0, 0.059844970703125, 592.5, 1278.0, 0.08074951171875, 538.0, 1278.0, 0.0216217041015625, 589.5, 1278.0, 0.1416015625, 527.5, 1273.0, 0.36669921875, 568.0, 1254.0, 0.52001953125, 584.0, 1246.0, 0.224365234375, 600.5, 1208.0, 0.80517578125, 619.5, 1240.0, 0.294921875, 638.5, 1227.0, 0.53515625, 503.0, 1194.0, 0.277587890625, 535.5, 1181.0, 0.392822265625, 587.0, 1194.0, 0.16259765625, 622.0, 1189.0, 0.2403564453125, 643.5, 1194.0, 0.034027099609375, 646.5, 1192.0, 0.0765380859375], 'scores': None}, '47': {'keypoints': [592.0, 1276.0, 0.07403564453125, 587.0, 1276.0, 0.058319091796875, 592.0, 1276.0, 0.05279541015625, 537.0, 1276.0, 0.022918701171875, 584.0, 1276.0, 0.1397705078125, 526.5, 1271.0, 0.446533203125, 568.5, 1253.0, 0.52587890625, 579.0, 1245.0, 0.09576416015625, 600.0, 1208.0, 0.80908203125, 618.5, 1240.0, 0.17626953125, 639.5, 1229.0, 0.603515625, 503.0, 1195.0, 0.287109375, 534.5, 1182.0, 0.395751953125, 631.5, 1234.0, 0.1322021484375, 621.0, 1184.0, 0.34130859375, 652.5, 1234.0, 0.055023193359375, 642.0, 1182.0, 0.05908203125], 'scores': None}, '48': {'keypoints': [600.0, 1277.0, 0.10833740234375, 602.5, 1277.0, 0.08917236328125, 602.5, 1277.0, 0.1463623046875, 534.5, 1277.0, 0.0166473388671875, 587.0, 1277.0, 0.1048583984375, 534.5, 1275.0, 0.32373046875, 566.0, 1254.0, 0.442138671875, 581.5, 1277.0, 0.10968017578125, 600.0, 1209.0, 0.8251953125, 605.0, 1267.0, 0.07135009765625, 639.5, 1233.0, 0.59326171875, 503.0, 1196.0, 0.29345703125, 534.5, 1183.0, 0.433837890625, 587.0, 1196.0, 0.1258544921875, 621.0, 1186.0, 0.1824951171875, 642.0, 1235.0, 0.062408447265625, 642.0, 1191.0, 0.0753173828125], 'scores': None}, '49': {'keypoints': [601.0, 1278.0, 0.153076171875, 604.0, 1278.0, 0.1512451171875, 601.0, 1278.0, 0.2164306640625, 535.0, 1278.0, 0.019287109375, 601.0, 1278.0, 0.09515380859375, 535.0, 1278.0, 0.274169921875, 561.5, 1260.0, 0.403076171875, 582.5, 1278.0, 0.0985107421875, 598.5, 1209.0, 0.765625, 606.5, 1278.0, 0.09381103515625, 636.0, 1230.0, 0.462158203125, 503.0, 1196.0, 0.300537109375, 535.0, 1182.0, 0.427734375, 598.5, 1209.0, 0.166259765625, 625.0, 1190.0, 0.159423828125, 644.0, 1236.0, 0.1212158203125, 646.5, 1238.0, 0.08880615234375], 'scores': None}, '50': {'keypoints': [603.5, 1278.0, 0.14453125, 603.5, 1278.0, 0.1400146484375, 601.0, 1278.0, 0.2010498046875, 534.0, 1278.0, 0.01190185546875, 601.0, 1278.0, 0.11688232421875, 534.0, 1278.0, 0.27978515625, 566.0, 1260.0, 0.457275390625, 585.0, 1262.0, 0.1290283203125, 598.0, 1209.0, 0.76708984375, 606.0, 1273.0, 0.09649658203125, 635.5, 1233.0, 0.5615234375, 502.0, 1196.0, 0.270751953125, 534.0, 1182.0, 0.44091796875, 593.0, 1204.0, 0.1483154296875, 625.0, 1190.0, 0.2408447265625, 643.5, 1236.0, 0.08917236328125, 643.5, 1238.0, 0.0623779296875], 'scores': None}, '51': {'keypoints': [603.5, 1278.0, 0.1910400390625, 603.5, 1278.0, 0.1849365234375, 601.0, 1278.0, 0.260009765625, 603.5, 1278.0, 0.007175445556640625, 590.0, 1278.0, 0.158203125, 537.0, 1278.0, 0.265380859375, 569.0, 1254.0, 0.4716796875, 579.5, 1244.0, 0.11810302734375, 598.0, 1209.0, 0.78076171875, 619.5, 1244.0, 0.09136962890625, 635.5, 1230.0, 0.62353515625, 507.25, 1196.0, 0.255126953125, 534.0, 1182.0, 0.4345703125, 593.0, 1206.0, 0.1676025390625, 630.5, 1190.0, 0.271484375, 643.5, 1236.0, 0.1092529296875, 643.5, 1182.0, 0.063720703125], 'scores': None}, '52': {'keypoints': [601.5, 1278.0, 0.1923828125, 604.0, 1278.0, 0.2149658203125, 601.5, 1278.0, 0.2393798828125, 601.5, 1278.0, 0.0297393798828125, 591.0, 1278.0, 0.17578125, 534.5, 1278.0, 0.2479248046875, 566.5, 1260.0, 0.43994140625, 585.5, 1246.0, 0.150390625, 596.0, 1209.0, 0.81396484375, 620.5, 1246.0, 0.1341552734375, 634.0, 1228.0, 0.6484375, 507.5, 1196.0, 0.2496337890625, 534.5, 1182.0, 0.429931640625, 593.5, 1209.0, 0.2080078125, 625.5, 1190.0, 0.148681640625, 644.5, 1236.0, 0.0634765625, 644.5, 1188.0, 0.07281494140625], 'scores': None}, '53': {'keypoints': [604.0, 1280.0, 0.15185546875, 604.0, 1280.0, 0.139892578125, 604.0, 1280.0, 0.1766357421875, 604.0, 1280.0, 0.0248260498046875, 593.0, 1280.0, 0.197021484375, 535.0, 1280.0, 0.2327880859375, 568.0, 1258.0, 0.46435546875, 590.0, 1247.0, 0.158203125, 596.0, 1212.0, 0.7431640625, 623.0, 1258.0, 0.09808349609375, 634.0, 1231.0, 0.599609375, 513.0, 1195.0, 0.2685546875, 535.0, 1181.0, 0.447021484375, 593.0, 1209.0, 0.2269287109375, 623.0, 1190.0, 0.21875, 648.0, 1231.0, 0.081787109375, 648.0, 1192.0, 0.08184814453125], 'scores': None}, '54': {'keypoints': [601.0, 1280.0, 0.1630859375, 607.0, 1280.0, 0.1419677734375, 601.0, 1280.0, 0.173583984375, 604.0, 1280.0, 0.03387451171875, 593.0, 1280.0, 0.1812744140625, 535.0, 1280.0, 0.27001953125, 568.0, 1258.0, 0.49072265625, 590.0, 1247.0, 0.2056884765625, 596.0, 1212.0, 0.7421875, 618.0, 1258.0, 0.1370849609375, 634.0, 1228.0, 0.63427734375, 513.0, 1195.0, 0.268798828125, 535.0, 1181.0, 0.44140625, 593.0, 1209.0, 0.181396484375, 623.0, 1190.0, 0.259033203125, 648.0, 1236.0, 0.04656982421875, 648.0, 1192.0, 0.0694580078125], 'scores': None}, '55': {'keypoints': [599.5, 1280.0, 0.15869140625, 602.5, 1280.0, 0.09686279296875, 602.5, 1280.0, 0.10693359375, 605.0, 1280.0, 0.02294921875, 591.5, 1280.0, 0.195556640625, 535.5, 1280.0, 0.23779296875, 566.5, 1258.0, 0.50732421875, 580.0, 1247.0, 0.254150390625, 594.0, 1210.0, 0.78759765625, 616.5, 1241.0, 0.2169189453125, 636.0, 1230.0, 0.60595703125, 502.25, 1194.0, 0.253662109375, 535.5, 1180.0, 0.424072265625, 591.5, 1208.0, 0.1622314453125, 624.5, 1188.0, 0.2213134765625, 647.0, 1224.0, 0.0718994140625, 650.0, 1191.0, 0.07080078125], 'scores': None}, '56': {'keypoints': [600.5, 1280.0, 0.164794921875, 603.0, 1280.0, 0.1048583984375, 600.5, 1280.0, 0.12939453125, 541.5, 1280.0, 0.053009033203125, 592.0, 1280.0, 0.215576171875, 536.0, 1277.0, 0.272216796875, 566.5, 1258.0, 0.5, 592.0, 1244.0, 0.291015625, 594.5, 1210.0, 0.7744140625, 617.0, 1244.0, 0.216796875, 636.5, 1230.0, 0.60595703125, 502.5, 1194.0, 0.2344970703125, 536.0, 1180.0, 0.421142578125, 592.0, 1208.0, 0.181884765625, 620.0, 1188.0, 0.1375732421875, 650.5, 1230.0, 0.09716796875, 650.5, 1191.0, 0.06304931640625], 'scores': None}, '57': {'keypoints': [601.0, 1280.0, 0.1436767578125, 607.0, 1280.0, 0.10211181640625, 601.0, 1280.0, 0.12213134765625, 535.0, 1280.0, 0.03515625, 587.5, 1280.0, 0.181396484375, 535.0, 1280.0, 0.231201171875, 565.5, 1261.0, 0.5185546875, 590.0, 1244.0, 0.30224609375, 593.0, 1212.0, 0.76025390625, 620.5, 1247.0, 0.1558837890625, 637.0, 1228.0, 0.56689453125, 513.0, 1195.0, 0.1822509765625, 535.0, 1181.0, 0.359375, 593.0, 1214.0, 0.1907958984375, 615.0, 1190.0, 0.123291015625, 648.0, 1228.0, 0.058319091796875, 648.0, 1192.0, 0.0941162109375], 'scores': None}, '58': {'keypoints': [598.5, 1281.0, 0.2156982421875, 598.5, 1281.0, 0.185302734375, 598.5, 1281.0, 0.187255859375, 604.0, 1281.0, 0.04071044921875, 593.0, 1281.0, 0.28173828125, 532.0, 1281.0, 0.1402587890625, 565.5, 1262.0, 0.537109375, 590.5, 1245.0, 0.185791015625, 590.5, 1214.0, 0.75, 612.5, 1256.0, 0.16357421875, 640.5, 1228.0, 0.6103515625, 512.5, 1195.0, 0.220703125, 534.5, 1181.0, 0.338134765625, 587.5, 1214.0, 0.149169921875, 593.0, 1203.0, 0.15478515625, 607.0, 1167.0, 0.056182861328125, 649.0, 1192.0, 0.052520751953125], 'scores': None}, '59': {'keypoints': [591.5, 1281.0, 0.1951904296875, 594.5, 1281.0, 0.1678466796875, 591.5, 1281.0, 0.180419921875, 602.5, 1281.0, 0.0126190185546875, 589.0, 1281.0, 0.2322998046875, 530.5, 1281.0, 0.1451416015625, 564.0, 1262.0, 0.55810546875, 589.0, 1240.0, 0.1640625, 589.0, 1215.0, 0.7490234375, 597.0, 1243.0, 0.238525390625, 633.0, 1229.0, 0.5634765625, 511.25, 1196.0, 0.25146484375, 536.0, 1182.0, 0.292236328125, 586.0, 1215.0, 0.12060546875, 591.5, 1204.0, 0.1708984375, 633.0, 1188.0, 0.04205322265625, 647.0, 1193.0, 0.104248046875], 'scores': None}, '60': {'keypoints': [587.0, 1279.0, 0.04327392578125, 581.5, 1279.0, 0.03411865234375, 587.0, 1279.0, 0.0401611328125, 581.5, 1193.0, 0.056243896484375, 569.0, 1279.0, 0.0333251953125, 523.0, 1277.0, 0.14794921875, 556.0, 1267.0, 0.5166015625, 584.0, 1198.0, 0.1583251953125, 587.0, 1216.0, 0.66845703125, 599.5, 1279.0, 0.2137451171875, 635.5, 1231.0, 0.30615234375, 533.0, 1178.0, 0.21142578125, 543.0, 1183.0, 0.2266845703125, 589.5, 1216.0, 0.220458984375, 589.5, 1216.0, 0.29296875, 615.0, 1259.0, 0.050750732421875, 653.5, 1241.0, 0.06610107421875], 'scores': None}, '61': {'keypoints': [574.0, 1186.0, 0.0784912109375, 574.0, 1186.0, 0.10455322265625, 576.5, 1189.0, 0.0677490234375, 571.5, 1161.0, 0.163818359375, 494.5, 1189.0, 0.0914306640625, 533.0, 1184.0, 0.325439453125, 499.75, 1204.0, 0.26171875, 584.0, 1199.0, 0.355224609375, 584.0, 1222.0, 0.55615234375, 643.0, 1235.0, 0.1397705078125, 633.0, 1237.0, 0.25439453125, 546.0, 1270.0, 0.21875, 517.5, 1280.0, 0.189697265625, 584.0, 1217.0, 0.481201171875, 581.5, 1219.0, 0.127197265625, 597.0, 1280.0, 0.1488037109375, 597.0, 1280.0, 0.156005859375], 'scores': None}, '62': {'keypoints': [507.25, 1167.0, 0.0309295654296875, 576.0, 1190.0, 0.011383056640625, 507.25, 1162.0, 0.0045928955078125, 573.5, 1190.0, 0.1507568359375, 512.5, 1162.0, 0.032623291015625, 535.5, 1185.0, 0.26171875, 555.5, 1269.0, 0.401611328125, 581.0, 1266.0, 0.1939697265625, 586.0, 1223.0, 0.60986328125, 599.0, 1281.0, 0.2471923828125, 639.5, 1226.0, 0.2186279296875, 533.0, 1185.0, 0.1793212890625, 515.0, 1281.0, 0.1669921875, 583.5, 1223.0, 0.2451171875, 583.5, 1220.0, 0.11358642578125, 632.0, 1231.0, 0.11029052734375, 622.0, 1266.0, 0.07867431640625], 'scores': None}, '63': {'keypoints': [592.5, 1171.0, 0.028350830078125, 590.0, 1166.0, 0.037506103515625, 592.5, 1282.0, 0.01236724853515625, 583.0, 1203.0, 0.0689697265625, 511.5, 1166.0, 0.1168212890625, 529.0, 1282.0, 0.091796875, 553.5, 1272.0, 0.46826171875, 583.0, 1262.0, 0.2413330078125, 585.0, 1225.0, 0.6650390625, 600.0, 1282.0, 0.255859375, 632.0, 1245.0, 0.38720703125, 519.0, 1191.0, 0.12042236328125, 541.0, 1188.0, 0.26416015625, 590.0, 1223.0, 0.1300048828125, 585.0, 1223.0, 0.2073974609375, 629.5, 1269.0, 0.06353759765625, 624.5, 1267.0, 0.11163330078125], 'scores': None}, '64': {'keypoints': [595.5, 1172.0, 0.045013427734375, 591.0, 1167.0, 0.044830322265625, 591.0, 1167.0, 0.00643157958984375, 576.0, 1167.0, 0.1990966796875, 511.75, 1167.0, 0.021728515625, 516.5, 1283.0, 0.2454833984375, 553.5, 1275.0, 0.4228515625, 578.5, 1268.0, 0.19140625, 586.0, 1226.0, 0.69189453125, 600.5, 1283.0, 0.30126953125, 630.0, 1241.0, 0.185546875, 509.25, 1206.0, 0.2353515625, 541.5, 1189.0, 0.265869140625, 586.0, 1226.0, 0.0792236328125, 591.0, 1224.0, 0.208984375, 628.0, 1263.0, 0.07757568359375, 628.0, 1268.0, 0.2392578125], 'scores': None}, '65': {'keypoints': [587.5, 1231.0, 0.018402099609375, 607.0, 1282.0, 0.00945281982421875, 585.0, 1226.0, 0.004619598388671875, 578.0, 1168.0, 0.03656005859375, 592.5, 1282.0, 0.02197265625, 515.0, 1282.0, 0.2109375, 548.5, 1282.0, 0.4140625, 578.0, 1262.0, 0.350341796875, 590.0, 1228.0, 0.66455078125, 599.5, 1282.0, 0.342529296875, 631.0, 1257.0, 0.55078125, 510.0, 1209.0, 0.2403564453125, 539.0, 1195.0, 0.287109375, 590.0, 1228.0, 0.0914306640625, 590.0, 1231.0, 0.3603515625, 624.0, 1262.0, 0.0281219482421875, 631.0, 1265.0, 0.31298828125], 'scores': None}, '66': {'keypoints': [607.5, 1283.0, 0.0176544189453125, 607.5, 1283.0, 0.01461029052734375, 607.5, 1283.0, 0.01171875, 607.5, 1283.0, 0.001789093017578125, 607.5, 1283.0, 0.0012369155883789062, 513.0, 1283.0, 0.26416015625, 556.5, 1283.0, 0.37744140625, 590.5, 1283.0, 0.2213134765625, 593.0, 1229.0, 0.6708984375, 629.0, 1268.0, 0.11248779296875, 631.5, 1266.0, 0.57275390625, 513.0, 1212.0, 0.289794921875, 547.0, 1200.0, 0.306884765625, 590.5, 1283.0, 0.09100341796875, 593.0, 1237.0, 0.394775390625, 634.0, 1263.0, 0.03466796875, 634.0, 1266.0, 0.384521484375], 'scores': None}, '67': {'keypoints': [611.5, 1278.0, 0.375244140625, 614.0, 1283.0, 0.2247314453125, 604.0, 1283.0, 0.343017578125, 609.0, 1280.0, 0.034149169921875, 594.5, 1283.0, 0.1787109375, 515.5, 1283.0, 0.1431884765625, 566.0, 1283.0, 0.1441650390625, 590.0, 1230.0, 0.04510498046875, 594.5, 1233.0, 0.67431640625, 630.5, 1268.0, 0.20703125, 635.5, 1261.0, 0.61669921875, 515.5, 1214.0, 0.2490234375, 556.5, 1202.0, 0.24609375, 592.0, 1238.0, 0.1319580078125, 594.5, 1238.0, 0.27392578125, 633.0, 1271.0, 0.06744384765625, 635.5, 1266.0, 0.157958984375], 'scores': None}, '68': {'keypoints': [599.5, 1173.0, 0.203125, 597.5, 1173.0, 0.0192108154296875, 599.5, 1173.0, 0.01432037353515625, 581.0, 1173.0, 0.1319580078125, 595.0, 1283.0, 0.033447265625, 522.0, 1283.0, 0.0863037109375, 569.0, 1283.0, 0.06671142578125, 595.0, 1283.0, 0.0941162109375, 597.5, 1236.0, 0.71875, 633.0, 1269.0, 0.1552734375, 637.5, 1264.0, 0.385009765625, 524.0, 1220.0, 0.212158203125, 550.0, 1208.0, 0.336181640625, 602.0, 1243.0, 0.1953125, 597.5, 1217.0, 0.2022705078125, 637.5, 1271.0, 0.11297607421875, 637.5, 1276.0, 0.154296875], 'scores': None}, '69': {'keypoints': [507.0, 1175.0, 0.126220703125, 604.0, 1283.0, 0.042724609375, 602.0, 1283.0, 0.023895263671875, 602.0, 1283.0, 0.022918701171875, 602.0, 1283.0, 0.11810302734375, 525.5, 1283.0, 0.083984375, 572.0, 1283.0, 0.05938720703125, 597.0, 1248.0, 0.09112548828125, 599.5, 1239.0, 0.70947265625, 625.0, 1283.0, 0.1683349609375, 639.0, 1271.0, 0.488037109375, 532.5, 1214.0, 0.2193603515625, 558.0, 1209.0, 0.255615234375, 604.0, 1246.0, 0.1627197265625, 604.0, 1244.0, 0.151123046875, 641.0, 1276.0, 0.10650634765625, 639.0, 1276.0, 0.115966796875], 'scores': None}, '70': {'keypoints': [617.0, 1284.0, 0.0289764404296875, 614.5, 1284.0, 0.01462554931640625, 610.0, 1284.0, 0.0192718505859375, 520.0, 1284.0, 0.01335906982421875, 556.0, 1180.0, 0.023773193359375, 527.0, 1282.0, 0.07281494140625, 574.0, 1284.0, 0.0718994140625, 599.0, 1250.0, 0.07928466796875, 601.0, 1243.0, 0.7255859375, 635.0, 1277.0, 0.09771728515625, 639.0, 1272.0, 0.401123046875, 536.0, 1216.0, 0.2220458984375, 560.5, 1216.0, 0.2420654296875, 605.5, 1250.0, 0.178466796875, 610.0, 1248.0, 0.11541748046875, 639.0, 1279.0, 0.15771484375, 639.0, 1277.0, 0.09130859375], 'scores': None}, '71': {'keypoints': [605.5, 1251.0, 0.041107177734375, 514.0, 1275.0, 0.03387451171875, 599.0, 1251.0, 0.0154266357421875, 547.5, 1180.0, 0.0217742919921875, 547.5, 1182.0, 0.051361083984375, 529.5, 1284.0, 0.06109619140625, 576.5, 1284.0, 0.07135009765625, 601.0, 1251.0, 0.07720947265625, 603.5, 1244.0, 0.66748046875, 643.5, 1279.0, 0.11859130859375, 643.5, 1277.0, 0.32080078125, 538.5, 1218.0, 0.1971435546875, 565.5, 1213.0, 0.196044921875, 608.0, 1248.0, 0.249755859375, 608.0, 1251.0, 0.276123046875, 641.5, 1279.0, 0.2490234375, 641.5, 1282.0, 0.1588134765625], 'scores': None}, '72': {'keypoints': [612.5, 1250.0, 0.0653076171875, 520.0, 1184.0, 0.01251983642578125, 612.5, 1250.0, 0.030792236328125, 608.0, 1223.0, 0.08038330078125, 567.5, 1182.0, 0.0623779296875, 606.0, 1254.0, 0.1256103515625, 581.0, 1286.0, 0.07135009765625, 612.5, 1232.0, 0.1533203125, 603.5, 1252.0, 0.31884765625, 644.0, 1281.0, 0.14404296875, 642.0, 1284.0, 0.1424560546875, 540.5, 1218.0, 0.07928466796875, 561.0, 1225.0, 0.173095703125, 608.0, 1250.0, 0.489501953125, 608.0, 1252.0, 0.312255859375, 644.0, 1286.0, 0.20703125, 642.0, 1286.0, 0.1322021484375], 'scores': None}, '73': {'keypoints': [618.0, 1249.0, 0.022186279296875, 664.5, 1267.0, 0.0161590576171875, 662.0, 1251.0, 0.02001953125, 586.0, 1206.0, 0.02960205078125, 555.5, 1180.0, 0.0249176025390625, 609.0, 1258.0, 0.08642578125, 525.5, 1249.0, 0.05426025390625, 609.0, 1256.0, 0.032806396484375, 609.0, 1251.0, 0.375, 650.5, 1286.0, 0.04058837890625, 648.0, 1274.0, 0.1346435546875, 544.0, 1217.0, 0.0806884765625, 562.5, 1226.0, 0.11572265625, 613.5, 1256.0, 0.3408203125, 616.0, 1249.0, 0.051788330078125, 650.5, 1286.0, 0.061920166015625, 666.5, 1212.0, 0.0855712890625], 'scores': None}, '74': {'keypoints': [615.0, 1256.0, 0.01052093505859375, 595.0, 1170.0, 0.0162506103515625, 669.0, 1274.0, 0.005489349365234375, 523.0, 1287.0, 0.0289459228515625, 571.5, 1180.0, 0.0169830322265625, 528.0, 1289.0, 0.07379150390625, 615.0, 1259.0, 0.06488037109375, 582.0, 1277.0, 0.048492431640625, 615.0, 1256.0, 0.57177734375, 643.5, 1284.0, 0.058929443359375, 653.5, 1279.0, 0.413330078125, 541.0, 1231.0, 0.265380859375, 569.0, 1218.0, 0.2978515625, 615.0, 1262.0, 0.23388671875, 648.5, 1249.0, 0.11297607421875, 597.5, 1195.0, 0.0855712890625, 592.0, 1195.0, 0.06597900390625], 'scores': None}, '75': {'keypoints': [525.0, 1289.0, 0.0477294921875, 525.0, 1289.0, 0.034576416015625, 525.0, 1284.0, 0.057037353515625, 525.0, 1289.0, 0.0277557373046875, 525.0, 1284.0, 0.0150299072265625, 537.5, 1289.0, 0.05780029296875, 623.0, 1262.0, 0.021881103515625, 650.0, 1279.0, 0.04486083984375, 618.0, 1257.0, 0.64794921875, 655.0, 1281.0, 0.10992431640625, 655.0, 1281.0, 0.49609375, 542.0, 1234.0, 0.2255859375, 569.0, 1225.0, 0.227783203125, 618.0, 1257.0, 0.1226806640625, 652.5, 1230.0, 0.09417724609375, 584.0, 1207.0, 0.06732177734375, 679.5, 1230.0, 0.0628662109375], 'scores': None}, '76': {'keypoints': [525.0, 1290.0, 0.037353515625, 527.5, 1290.0, 0.0276031494140625, 525.0, 1285.0, 0.0498046875, 525.0, 1290.0, 0.02886962890625, 572.0, 1181.0, 0.04510498046875, 537.5, 1290.0, 0.059417724609375, 624.0, 1255.0, 0.0146331787109375, 619.0, 1253.0, 0.051483154296875, 619.0, 1258.0, 0.697265625, 651.0, 1285.0, 0.10736083984375, 656.0, 1282.0, 0.51806640625, 542.5, 1231.0, 0.296142578125, 574.5, 1226.0, 0.237060546875, 619.0, 1258.0, 0.1767578125, 646.0, 1231.0, 0.06903076171875, 584.0, 1208.0, 0.062103271484375, 680.5, 1231.0, 0.033416748046875], 'scores': None}, '77': {'keypoints': [525.0, 1290.0, 0.01861572265625, 525.0, 1290.0, 0.014617919921875, 525.0, 1290.0, 0.02716064453125, 525.0, 1287.0, 0.0117340087890625, 572.0, 1181.0, 0.019256591796875, 535.0, 1290.0, 0.0538330078125, 582.0, 1290.0, 0.01019287109375, 619.5, 1258.0, 0.053802490234375, 619.5, 1260.0, 0.73876953125, 649.0, 1287.0, 0.11328125, 654.0, 1282.0, 0.51708984375, 542.5, 1231.0, 0.287353515625, 574.5, 1221.0, 0.28662109375, 619.5, 1260.0, 0.1600341796875, 646.5, 1223.0, 0.0819091796875, 584.5, 1208.0, 0.07403564453125, 642.0, 1211.0, 0.0338134765625], 'scores': None}, '78': {'keypoints': [620.5, 1262.0, 0.0274810791015625, 525.0, 1290.0, 0.017303466796875, 525.0, 1290.0, 0.0167236328125, 525.0, 1290.0, 0.021575927734375, 571.5, 1179.0, 0.012298583984375, 530.0, 1290.0, 0.1092529296875, 613.5, 1260.0, 0.0134124755859375, 618.0, 1258.0, 0.0518798828125, 618.0, 1260.0, 0.76904296875, 650.0, 1285.0, 0.135009765625, 655.0, 1282.0, 0.515625, 542.0, 1228.0, 0.259765625, 574.0, 1221.0, 0.27392578125, 618.0, 1260.0, 0.15576171875, 652.5, 1231.0, 0.0775146484375, 584.0, 1206.0, 0.101806640625, 679.5, 1231.0, 0.05316162109375], 'scores': None}, '79': {'keypoints': [617.0, 1274.0, 0.0178070068359375, 591.0, 1278.0, 0.016143798828125, 591.0, 1288.0, 0.01116180419921875, 525.0, 1288.0, 0.018280029296875, 607.5, 1283.0, 0.01812744140625, 525.0, 1274.0, 0.148681640625, 572.0, 1285.0, 0.01148223876953125, 614.5, 1260.0, 0.042510986328125, 614.5, 1260.0, 0.7314453125, 655.0, 1288.0, 0.11199951171875, 657.0, 1285.0, 0.377685546875, 551.0, 1218.0, 0.22607421875, 574.5, 1222.0, 0.264404296875, 617.0, 1267.0, 0.2042236328125, 647.5, 1220.0, 0.0745849609375, 581.5, 1201.0, 0.11566162109375, 673.5, 1227.0, 0.06988525390625], 'scores': None}, '80': {'keypoints': [615.5, 1267.0, 0.0145111083984375, 597.0, 1171.0, 0.02288818359375, 597.0, 1171.0, 0.005596160888671875, 597.0, 1171.0, 0.0032062530517578125, 615.5, 1269.0, 0.01348876953125, 527.0, 1292.0, 0.1846923828125, 584.0, 1292.0, 0.00803375244140625, 581.5, 1282.0, 0.052215576171875, 613.0, 1264.0, 0.73193359375, 644.0, 1290.0, 0.0301361083984375, 649.0, 1292.0, 0.273193359375, 550.5, 1223.0, 0.255615234375, 576.5, 1225.0, 0.33251953125, 620.5, 1248.0, 0.10516357421875, 649.0, 1228.0, 0.09759521484375, 584.0, 1205.0, 0.11865234375, 623.0, 1181.0, 0.03179931640625], 'scores': None}, '81': {'keypoints': [610.0, 1273.0, 0.01934814453125, 641.5, 1246.0, 0.01216888427734375, 614.5, 1283.0, 0.01360321044921875, 519.0, 1288.0, 0.00926971435546875, 605.0, 1273.0, 0.03485107421875, 524.0, 1291.0, 0.1427001953125, 585.0, 1291.0, 0.0293426513671875, 612.0, 1273.0, 0.056396484375, 610.0, 1266.0, 0.63037109375, 649.0, 1281.0, 0.1055908203125, 649.0, 1268.0, 0.279052734375, 546.0, 1224.0, 0.2469482421875, 575.5, 1222.0, 0.242919921875, 612.0, 1268.0, 0.20751953125, 617.0, 1268.0, 0.12939453125, 644.0, 1207.0, 0.0802001953125, 673.5, 1232.0, 0.1282958984375], 'scores': None}, '82': {'keypoints': [603.5, 1276.0, 0.02679443359375, 593.0, 1171.0, 0.01343536376953125, 609.0, 1294.0, 0.009735107421875, 593.0, 1171.0, 0.0018701553344726562, 609.0, 1286.0, 0.0167388916015625, 522.0, 1294.0, 0.1668701171875, 580.0, 1294.0, 0.03192138671875, 598.5, 1276.0, 0.08819580078125, 603.5, 1271.0, 0.74072265625, 640.5, 1276.0, 0.07208251953125, 643.0, 1271.0, 0.348388671875, 546.0, 1231.0, 0.299560546875, 575.0, 1226.0, 0.30810546875, 640.5, 1218.0, 0.1265869140625, 653.5, 1231.0, 0.1263427734375, 682.5, 1226.0, 0.070068359375, 682.5, 1231.0, 0.09002685546875], 'scores': None}, '83': {'keypoints': [595.0, 1290.0, 0.0200653076171875, 603.0, 1293.0, 0.01064300537109375, 603.0, 1295.0, 0.0013179779052734375, 513.0, 1279.0, 0.003017425537109375, 592.5, 1284.0, 0.0102081298828125, 518.5, 1295.0, 0.155029296875, 578.5, 1295.0, 0.06219482421875, 587.0, 1276.0, 0.0867919921875, 598.0, 1274.0, 0.7548828125, 644.0, 1276.0, 0.0736083984375, 639.0, 1268.0, 0.5791015625, 543.0, 1236.0, 0.26806640625, 570.5, 1230.0, 0.340576171875, 619.5, 1255.0, 0.1710205078125, 655.0, 1263.0, 0.239013671875, 600.5, 1195.0, 0.1474609375, 685.5, 1228.0, 0.077392578125], 'scores': None}, '84': {'keypoints': [581.0, 1265.0, nan, 588.0, 1268.0, nan, 588.0, 1269.0, nan, 510.0, 1255.0, nan, 579.5, 1259.0, nan, 523.5, 1269.0, nan, 574.5, 1269.0, nan, 581.5, 1262.0, nan, 590.5, 1261.0, nan, 632.5, 1266.0, nan, 627.5, 1258.0, nan, 550.5, 1233.0, nan, 579.5, 1226.0, nan, 623.0, 1257.0, nan, 657.0, 1262.0, nan, 611.0, 1211.0, nan, 688.5, 1237.0, nan], 'scores': None}, '85': {'keypoints': [567.0, 1240.0, nan, 572.5, 1242.0, nan, 572.5, 1244.0, nan, 507.0, 1231.0, nan, 566.5, 1235.0, nan, 529.0, 1244.0, nan, 570.5, 1242.0, nan, 576.5, 1248.0, nan, 583.0, 1248.0, nan, 621.0, 1256.0, nan, 616.0, 1249.0, nan, 557.5, 1230.0, nan, 589.0, 1221.0, nan, 626.5, 1258.0, nan, 659.5, 1260.0, nan, 622.0, 1227.0, nan, 691.5, 1245.0, nan], 'scores': None}, '86': {'keypoints': [552.5, 1215.0, nan, 557.0, 1217.0, nan, 557.0, 1218.0, nan, 504.0, 1207.0, nan, 553.0, 1210.0, nan, 534.0, 1218.0, nan, 566.5, 1216.0, nan, 571.0, 1234.0, nan, 575.5, 1235.0, nan, 609.5, 1246.0, nan, 604.5, 1239.0, nan, 565.0, 1227.0, nan, 598.0, 1217.0, nan, 630.0, 1260.0, nan, 661.5, 1259.0, nan, 632.5, 1243.0, nan, 694.5, 1254.0, nan], 'scores': None}, '87': {'keypoints': [538.5, 1190.0, nan, 542.0, 1191.0, nan, 542.0, 1192.0, nan, 500.75, 1183.0, nan, 540.0, 1185.0, nan, 539.0, 1192.0, nan, 562.0, 1190.0, nan, 566.0, 1220.0, nan, 568.0, 1221.0, nan, 597.5, 1235.0, nan, 593.5, 1229.0, nan, 572.5, 1223.0, nan, 607.5, 1213.0, nan, 633.0, 1262.0, nan, 664.0, 1258.0, nan, 643.0, 1259.0, nan, 697.0, 1263.0, nan], 'scores': None}, '88': {'keypoints': [524.5, 1165.0, nan, 527.0, 1166.0, nan, 527.0, 1166.0, nan, 497.75, 1159.0, nan, 527.0, 1160.0, nan, 544.0, 1166.0, nan, 558.0, 1164.0, nan, 560.5, 1206.0, nan, 560.5, 1208.0, nan, 586.0, 1225.0, nan, 582.0, 1219.0, nan, 580.0, 1220.0, nan, 616.5, 1209.0, nan, 636.5, 1264.0, nan, 666.0, 1257.0, nan, 653.5, 1275.0, nan, 700.0, 1272.0, nan], 'scores': None}, '89': {'keypoints': [510.25, 1140.0, nan, 511.5, 1140.0, nan, 511.5, 1141.0, nan, 494.75, 1135.0, nan, 514.0, 1136.0, nan, 549.5, 1141.0, nan, 554.0, 1137.0, nan, 555.5, 1192.0, nan, 553.0, 1195.0, nan, 574.5, 1215.0, nan, 570.5, 1210.0, nan, 587.0, 1217.0, nan, 626.0, 1204.0, nan, 640.0, 1265.0, nan, 668.5, 1255.0, nan, 664.5, 1291.0, nan, 703.0, 1280.0, nan], 'scores': None}, '90': {'keypoints': [496.25, 1115.0, 0.11566162109375, 496.25, 1115.0, 0.1048583984375, 496.25, 1115.0, 0.17236328125, 491.75, 1111.0, 0.142333984375, 500.75, 1111.0, 0.0836181640625, 554.5, 1115.0, 0.172119140625, 550.0, 1111.0, 0.1455078125, 550.0, 1178.0, 0.367919921875, 545.5, 1182.0, 0.08721923828125, 563.0, 1205.0, 0.62451171875, 559.0, 1200.0, 0.32373046875, 594.5, 1214.0, 0.2459716796875, 635.0, 1200.0, 0.1405029296875, 643.5, 1267.0, 0.1707763671875, 670.5, 1254.0, 0.1463623046875, 675.0, 1307.0, 0.1641845703125, 706.0, 1289.0, 0.14990234375], 'scores': None}}\n",
      "\n",
      "============================================================\n",
      "GT (GROUND TRUTH) FOLDER\n",
      "============================================================\n",
      "GT folder does not exist yet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "base_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\data\\PoseLift\\pose\"\n",
    "\n",
    "# Check train folder\n",
    "train_path = os.path.join(base_path, \"train\")\n",
    "test_path = os.path.join(base_path, \"test\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAIN FOLDER\")\n",
    "print(\"=\" * 60)\n",
    "train_files = os.listdir(train_path)\n",
    "print(f\"Total files: {len(train_files)}\")\n",
    "print(f\"First 5 files: {train_files[:5]}\")\n",
    "\n",
    "if train_files:\n",
    "    sample_train = train_files[0]\n",
    "    train_file_path = os.path.join(train_path, sample_train)\n",
    "    print(f\"\\nSample file: {sample_train}\")\n",
    "    print(f\"File size: {os.path.getsize(train_file_path) / 1024:.2f} KB\")\n",
    "    \n",
    "    with open(train_file_path, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    print(f\"Type: {type(train_data)}\")\n",
    "    print(f\"Structure: {list(train_data.keys()) if isinstance(train_data, dict) else f'List of length {len(train_data)}'}\")\n",
    "    \n",
    "    # Print first entry\n",
    "    if isinstance(train_data, dict):\n",
    "        first_key = list(train_data.keys())[0]\n",
    "        print(f\"\\nFirst entry key: {first_key}\")\n",
    "        print(f\"First entry value: {train_data[first_key]}\")\n",
    "    else:\n",
    "        print(f\"\\nFirst item: {train_data[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST FOLDER\")\n",
    "print(\"=\" * 60)\n",
    "test_files = os.listdir(test_path)\n",
    "print(f\"Total files: {len(test_files)}\")\n",
    "print(f\"First 5 files: {test_files[:5]}\")\n",
    "\n",
    "if test_files:\n",
    "    sample_test = test_files[0]\n",
    "    test_file_path = os.path.join(test_path, sample_test)\n",
    "    print(f\"\\nSample file: {sample_test}\")\n",
    "    print(f\"File size: {os.path.getsize(test_file_path) / 1024:.2f} KB\")\n",
    "    \n",
    "    with open(test_file_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    print(f\"Type: {type(test_data)}\")\n",
    "    print(f\"Structure: {list(test_data.keys()) if isinstance(test_data, dict) else f'List of length {len(test_data)}'}\")\n",
    "    \n",
    "    # Print first entry\n",
    "    if isinstance(test_data, dict):\n",
    "        first_key = list(test_data.keys())[0]\n",
    "        print(f\"\\nFirst entry key: {first_key}\")\n",
    "        print(f\"First entry value: {test_data[first_key]}\")\n",
    "    else:\n",
    "        print(f\"\\nFirst item: {test_data[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GT (GROUND TRUTH) FOLDER\")\n",
    "print(\"=\" * 60)\n",
    "gt_path = os.path.join(base_path.replace(\"pose\", \"gt\"))\n",
    "if os.path.exists(gt_path):\n",
    "    gt_files = os.listdir(gt_path)\n",
    "    print(f\"GT files: {gt_files[:5]}\")\n",
    "else:\n",
    "    print(\"GT folder does not exist yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05cc90dd-27e5-416e-a21f-9990a7fe1ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAIN DATASET\n",
      "============================================================\n",
      "[TRAIN] Found 104 JSON files in C:\\Users\\Krish\\PoseLift\\STG-NF\\data\\PoseLift\\pose\\train\n",
      "Sample shape: torch.Size([30, 17, 2])\n",
      "Video ID: 01_097_alphapose_tracked_person.json\n",
      "Sample min/max: 0.00 / 406.00\n",
      "NaN count: 0\n",
      "\n",
      "============================================================\n",
      "TEST DATASET\n",
      "============================================================\n",
      "[TEST] Found 47 JSON files in C:\\Users\\Krish\\PoseLift\\STG-NF\\data\\PoseLift\\pose\\test\n",
      "Sample shape: torch.Size([30, 17, 2])\n",
      "Video ID: 01_0222_alphapose_tracked_person.json\n",
      "Sample min/max: 0.00 / 1280.00\n",
      "NaN count: 0\n",
      "\n",
      "============================================================\n",
      "BATCH TEST\n",
      "============================================================\n",
      "Batch shape: torch.Size([4, 30, 17, 2])\n",
      "Batch video IDs: ('04_233_alphapose_tracked_person.json', '02_217_alphapose_tracked_person.json')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PoseLiftDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PoseLift dataset for STG-NF anomaly detection.\n",
    "    \n",
    "    Structure:\n",
    "    - JSON files contain: {frame_id: {person_id: {\"keypoints\": [51 floats], \"scores\": None}}}\n",
    "    - Keypoints list has 51 floats (17 joints  3: x, y, confidence)\n",
    "    - We extract (x, y) for each of 17 joints  (T, 17, 2) tensors per person\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pose_dir, split='train', num_frames=30, num_joints=17):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pose_dir: path to pose folder (e.g., data/PoseLift/pose/train)\n",
    "            split: 'train' or 'test'\n",
    "            num_frames: sequence length (we'll pad/truncate to this)\n",
    "            num_joints: number of joints (17 for COCO)\n",
    "        \"\"\"\n",
    "        self.pose_dir = pose_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.num_joints = num_joints\n",
    "        self.split = split\n",
    "        \n",
    "        # Get all JSON files\n",
    "        self.json_files = sorted([f for f in os.listdir(pose_dir) if f.endswith('.json')])\n",
    "        \n",
    "        if not self.json_files:\n",
    "            raise FileNotFoundError(f\"No JSON files found in {pose_dir}\")\n",
    "        \n",
    "        print(f\"[{split.upper()}] Found {len(self.json_files)} JSON files in {pose_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.json_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a sequence tensor for the first person in this video.\n",
    "        \n",
    "        Returns:\n",
    "            - sequence: (T, J, 2) tensor for a single person\n",
    "            - video_id: filename for reference\n",
    "        \"\"\"\n",
    "        json_path = os.path.join(self.pose_dir, self.json_files[idx])\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            video_data = json.load(f)\n",
    "        \n",
    "        # video_data = {frame_id: {person_id: {\"keypoints\": [51 floats], \"scores\": None}}}\n",
    "        \n",
    "        # Collect all unique person IDs\n",
    "        person_ids = set()\n",
    "        for frame_id, persons in video_data.items():\n",
    "            person_ids.update(persons.keys())\n",
    "        \n",
    "        person_ids = sorted(list(person_ids))\n",
    "        \n",
    "        if not person_ids:\n",
    "            # Empty video  return zero tensor\n",
    "            return torch.zeros(self.num_frames, self.num_joints, 2), self.json_files[idx]\n",
    "        \n",
    "        # Use the first person\n",
    "        person_id = person_ids[0]\n",
    "        pose_sequence = []\n",
    "        \n",
    "        # Sort frames by frame ID (convert to int if needed)\n",
    "        frame_ids_sorted = sorted(video_data.keys(), \n",
    "                                   key=lambda x: int(x) if isinstance(x, str) else x)\n",
    "        \n",
    "        for frame_id in frame_ids_sorted:\n",
    "            if person_id in video_data[frame_id]:\n",
    "                person_data = video_data[frame_id][person_id]\n",
    "                \n",
    "                # Extract keypoints list from dict\n",
    "                if isinstance(person_data, dict) and \"keypoints\" in person_data:\n",
    "                    keypoints_raw = person_data[\"keypoints\"]\n",
    "                else:\n",
    "                    # Fallback: might be a list directly\n",
    "                    keypoints_raw = person_data\n",
    "                \n",
    "                # Convert to numpy array\n",
    "                keypoints_raw = np.array(keypoints_raw, dtype=np.float32)\n",
    "                \n",
    "                # Handle NaN and reshape\n",
    "                # Filter out NaN values before reshaping\n",
    "                valid_keypoints = np.isnan(keypoints_raw) == False\n",
    "                \n",
    "                if len(keypoints_raw) == self.num_joints * 3:\n",
    "                    # Reshape to (17, 3) and extract x, y only\n",
    "                    keypoints_xy = keypoints_raw.reshape(self.num_joints, 3)[:, :2]  # (17, 2)\n",
    "                else:\n",
    "                    # Unexpected format  zero padding\n",
    "                    keypoints_xy = np.zeros((self.num_joints, 2), dtype=np.float32)\n",
    "            else:\n",
    "                # Missing detection  zero padding\n",
    "                keypoints_xy = np.zeros((self.num_joints, 2), dtype=np.float32)\n",
    "            \n",
    "            pose_sequence.append(keypoints_xy)\n",
    "        \n",
    "        # Convert to (T, 17, 2)\n",
    "        pose_sequence = np.array(pose_sequence, dtype=np.float32)  # (T, 17, 2)\n",
    "        \n",
    "        # Handle NaN in the sequence\n",
    "        pose_sequence = np.nan_to_num(pose_sequence, nan=0.0)\n",
    "        \n",
    "        # Pad or truncate to num_frames\n",
    "        if pose_sequence.shape[0] < self.num_frames:\n",
    "            # Pad with zeros\n",
    "            padding = np.zeros((self.num_frames - pose_sequence.shape[0], \n",
    "                               self.num_joints, 2), dtype=np.float32)\n",
    "            pose_sequence = np.vstack([pose_sequence, padding])\n",
    "        else:\n",
    "            # Truncate\n",
    "            pose_sequence = pose_sequence[:self.num_frames]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        pose_tensor = torch.from_numpy(pose_sequence).float()  # (T, 17, 2)\n",
    "        \n",
    "        return pose_tensor, self.json_files[idx]\n",
    "\n",
    "\n",
    "# Test the dataset\n",
    "if __name__ == \"__main__\":\n",
    "    train_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\data\\PoseLift\\pose\\train\"\n",
    "    test_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\data\\PoseLift\\pose\\test\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAIN DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    train_dataset = PoseLiftDataset(train_path, split='train')\n",
    "    sample, vid_id = train_dataset[0]\n",
    "    print(f\"Sample shape: {sample.shape}\")  # Should be (30, 17, 2)\n",
    "    print(f\"Video ID: {vid_id}\")\n",
    "    print(f\"Sample min/max: {sample.min():.2f} / {sample.max():.2f}\")\n",
    "    print(f\"NaN count: {torch.isnan(sample).sum().item()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    test_dataset = PoseLiftDataset(test_path, split='test')\n",
    "    sample, vid_id = test_dataset[0]\n",
    "    print(f\"Sample shape: {sample.shape}\")  # Should be (30, 17, 2)\n",
    "    print(f\"Video ID: {vid_id}\")\n",
    "    print(f\"Sample min/max: {sample.min():.2f} / {sample.max():.2f}\")\n",
    "    print(f\"NaN count: {torch.isnan(sample).sum().item()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BATCH TEST\")\n",
    "    print(\"=\" * 60)\n",
    "    from torch.utils.data import DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    batch_poses, batch_ids = next(iter(train_loader))\n",
    "    print(f\"Batch shape: {batch_poses.shape}\")  # Should be (4, 30, 17, 2)\n",
    "    print(f\"Batch video IDs: {batch_ids[:2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09ee0dc7-15e7-4bbf-9ff8-9270ce54967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset_poselift.py in STG-NF directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create dataset_poselift.py in your STG-NF directory\n",
    "dataset_code = '''import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PoseLiftDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PoseLift dataset for STG-NF anomaly detection.\n",
    "    \n",
    "    Structure:\n",
    "    - JSON files contain: {frame_id: {person_id: {\"keypoints\": [51 floats], \"scores\": None}}}\n",
    "    - Keypoints list has 51 floats (17 joints x 3: x, y, confidence)\n",
    "    - We extract (x, y) for each of 17 joints -> (T, 17, 2) tensors per person\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pose_dir, split='train', num_frames=30, num_joints=17):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pose_dir: path to pose folder (e.g., data/PoseLift/pose/train)\n",
    "            split: 'train' or 'test'\n",
    "            num_frames: sequence length (we'll pad/truncate to this)\n",
    "            num_joints: number of joints (17 for COCO)\n",
    "        \"\"\"\n",
    "        self.pose_dir = pose_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.num_joints = num_joints\n",
    "        self.split = split\n",
    "        \n",
    "        # Get all JSON files\n",
    "        self.json_files = sorted([f for f in os.listdir(pose_dir) if f.endswith('.json')])\n",
    "        \n",
    "        if not self.json_files:\n",
    "            raise FileNotFoundError(f\"No JSON files found in {pose_dir}\")\n",
    "        \n",
    "        print(f\"[{split.upper()}] Found {len(self.json_files)} JSON files in {pose_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.json_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a sequence tensor for the first person in this video.\n",
    "        \n",
    "        Returns:\n",
    "            - sequence: (T, J, 2) tensor for a single person\n",
    "            - video_id: filename for reference\n",
    "        \"\"\"\n",
    "        json_path = os.path.join(self.pose_dir, self.json_files[idx])\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            video_data = json.load(f)\n",
    "        \n",
    "        # video_data = {frame_id: {person_id: {\"keypoints\": [51 floats], \"scores\": None}}}\n",
    "        \n",
    "        # Collect all unique person IDs\n",
    "        person_ids = set()\n",
    "        for frame_id, persons in video_data.items():\n",
    "            person_ids.update(persons.keys())\n",
    "        \n",
    "        person_ids = sorted(list(person_ids))\n",
    "        \n",
    "        if not person_ids:\n",
    "            # Empty video -> return zero tensor\n",
    "            return torch.zeros(self.num_frames, self.num_joints, 2), self.json_files[idx]\n",
    "        \n",
    "        # Use the first person\n",
    "        person_id = person_ids[0]\n",
    "        pose_sequence = []\n",
    "        \n",
    "        # Sort frames by frame ID (convert to int if needed)\n",
    "        frame_ids_sorted = sorted(video_data.keys(), \n",
    "                                   key=lambda x: int(x) if isinstance(x, str) else x)\n",
    "        \n",
    "        for frame_id in frame_ids_sorted:\n",
    "            if person_id in video_data[frame_id]:\n",
    "                person_data = video_data[frame_id][person_id]\n",
    "                \n",
    "                # Extract keypoints list from dict\n",
    "                if isinstance(person_data, dict) and \"keypoints\" in person_data:\n",
    "                    keypoints_raw = person_data[\"keypoints\"]\n",
    "                else:\n",
    "                    # Fallback: might be a list directly\n",
    "                    keypoints_raw = person_data\n",
    "                \n",
    "                # Convert to numpy array\n",
    "                keypoints_raw = np.array(keypoints_raw, dtype=np.float32)\n",
    "                \n",
    "                if len(keypoints_raw) == self.num_joints * 3:\n",
    "                    # Reshape to (17, 3) and extract x, y only\n",
    "                    keypoints_xy = keypoints_raw.reshape(self.num_joints, 3)[:, :2]  # (17, 2)\n",
    "                else:\n",
    "                    # Unexpected format -> zero padding\n",
    "                    keypoints_xy = np.zeros((self.num_joints, 2), dtype=np.float32)\n",
    "            else:\n",
    "                # Missing detection -> zero padding\n",
    "                keypoints_xy = np.zeros((self.num_joints, 2), dtype=np.float32)\n",
    "            \n",
    "            pose_sequence.append(keypoints_xy)\n",
    "        \n",
    "        # Convert to (T, 17, 2)\n",
    "        pose_sequence = np.array(pose_sequence, dtype=np.float32)  # (T, 17, 2)\n",
    "        \n",
    "        # Handle NaN in the sequence\n",
    "        pose_sequence = np.nan_to_num(pose_sequence, nan=0.0)\n",
    "        \n",
    "        # Pad or truncate to num_frames\n",
    "        if pose_sequence.shape[0] < self.num_frames:\n",
    "            # Pad with zeros\n",
    "            padding = np.zeros((self.num_frames - pose_sequence.shape[0], \n",
    "                               self.num_joints, 2), dtype=np.float32)\n",
    "            pose_sequence = np.vstack([pose_sequence, padding])\n",
    "        else:\n",
    "            # Truncate\n",
    "            pose_sequence = pose_sequence[:self.num_frames]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        pose_tensor = torch.from_numpy(pose_sequence).float()  # (T, 17, 2)\n",
    "        \n",
    "        return pose_tensor, self.json_files[idx]\n",
    "'''\n",
    "\n",
    "# Write to file with UTF-8 encoding\n",
    "stg_nf_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\"\n",
    "with open(os.path.join(stg_nf_path, \"dataset_poselift.py\"), 'w', encoding='utf-8') as f:\n",
    "    f.write(dataset_code)\n",
    "\n",
    "print(\"Created dataset_poselift.py in STG-NF directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d38046be-2101-4539-a38f-2ba5dc52e139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Found 104 JSON files in C:\\Users\\Krish\\PoseLift\\STG-NF\\data\\PoseLift\\pose\\train\n",
      "[TEST] Found 47 JSON files in C:\\Users\\Krish\\PoseLift\\STG-NF\\data\\PoseLift\\pose\\test\n",
      "Train dataset: 104 videos\n",
      "Test dataset: 47 videos\n",
      "Batch shape: torch.Size([8, 30, 17, 2])\n",
      "Sample IDs: ('05_027_alphapose_tracked_person.json', '01_258_alphapose_tracked_person.json', '02_323_alphapose_tracked_person.json')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r\"C:\\Users\\Krish\\PoseLift\\STG-NF\")\n",
    "\n",
    "from dataset_poselift import PoseLiftDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\data\\PoseLift\\pose\\train\"\n",
    "test_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\data\\PoseLift\\pose\\test\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = PoseLiftDataset(train_path, split='train', num_frames=30)\n",
    "test_dataset = PoseLiftDataset(test_path, split='test', num_frames=30)\n",
    "\n",
    "# Create loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "# Get a batch\n",
    "batch_poses, batch_ids = next(iter(train_loader))\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} videos\")\n",
    "print(f\"Test dataset: {len(test_dataset)} videos\")\n",
    "print(f\"Batch shape: {batch_poses.shape}\")  # Should be (8, 30, 17, 2)\n",
    "print(f\"Sample IDs: {batch_ids[:3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d96f5e95-0ca0-4c33-8a0f-be5188fdc7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updated args.py - added PoseLift to choices\n"
     ]
    }
   ],
   "source": [
    "# Read args.py\n",
    "args_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\args.py\"\n",
    "with open(args_path, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Replace line 41-42\n",
    "content = content.replace(\n",
    "    \"choices=['ShanghaiTech', 'ShanghaiTech-HR', 'UBnormal']\",\n",
    "    \"choices=['ShanghaiTech', 'ShanghaiTech-HR', 'UBnormal', 'PoseLift']\"\n",
    ")\n",
    "\n",
    "# Write back\n",
    "with open(args_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\" Updated args.py - added PoseLift to choices\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93f49dbd-2e7c-44b9-9939-85d26391f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updated args.py - added PoseLift handling in init_sub_args()\n"
     ]
    }
   ],
   "source": [
    "# Read args.py\n",
    "args_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\args.py\"\n",
    "with open(args_path, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Replace init_sub_args function\n",
    "old_func = '''def init_sub_args(args):\n",
    "    dataset = \"UBnormal\" if args.dataset == \"UBnormal\" else \"ShanghaiTech\"\n",
    "    if args.vid_path_train and args.vid_path_test and args.pose_path_train and args.pose_path_test:'''\n",
    "\n",
    "new_func = '''def init_sub_args(args):\n",
    "    dataset = \"UBnormal\" if args.dataset == \"UBnormal\" else \"ShanghaiTech\"\n",
    "    if args.dataset == \"PoseLift\":\n",
    "        dataset = \"PoseLift\"\n",
    "    if args.vid_path_train and args.vid_path_test and args.pose_path_train and args.pose_path_test:'''\n",
    "\n",
    "content = content.replace(old_func, new_func)\n",
    "\n",
    "# Write back\n",
    "with open(args_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\" Updated args.py - added PoseLift handling in init_sub_args()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96163cf7-d266-4b44-bf7d-bda0584c52b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updated dataset.py - added PoseLift support\n"
     ]
    }
   ],
   "source": [
    "# Read dataset.py\n",
    "dataset_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\dataset.py\"\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Find the get_dataset_and_loader function and add PoseLift check at the start\n",
    "poselift_code = '''def get_dataset_and_loader(args, trans_list, only_test=False):\n",
    "    # POSELIFT SUPPORT\n",
    "    if args.dataset == \"PoseLift\":\n",
    "        from dataset_poselift import PoseLiftDataset\n",
    "        from torch.utils.data import DataLoader\n",
    "        loader_args = {'batch_size': args.batch_size, 'num_workers': args.num_workers, 'pin_memory': True}\n",
    "        dataset, loader = dict(), dict()\n",
    "        splits = ['train', 'test'] if not only_test else ['test']\n",
    "        \n",
    "        for split in splits:\n",
    "            dataset[split] = PoseLiftDataset(args.pose_path[split], split=split, num_frames=args.seg_len)\n",
    "            loader[split] = DataLoader(dataset[split], **loader_args, shuffle=(split == 'train'))\n",
    "        \n",
    "        if only_test:\n",
    "            loader['train'] = None\n",
    "        return dataset, loader\n",
    "    \n",
    "    # Original STG-NF code continues below:\n",
    "    '''\n",
    "\n",
    "# Replace the function definition\n",
    "old_func_start = \"def get_dataset_and_loader(args, trans_list, only_test=False):\\n    loader_args = {'batch_size': args.batch_size, 'num_workers': args.num_workers, 'pin_memory': True}\"\n",
    "\n",
    "new_func_start = poselift_code + \"\\n    loader_args = {'batch_size': args.batch_size, 'num_workers': args.num_workers, 'pin_memory': True}\"\n",
    "\n",
    "content = content.replace(old_func_start, new_func_start)\n",
    "\n",
    "# Write back\n",
    "with open(dataset_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\" Updated dataset.py - added PoseLift support\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0100737-2b07-40be-8825-2a19a4d72826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFYING CHANGES\n",
      "============================================================\n",
      " args.py updated correctly\n",
      " dataset.py updated correctly\n",
      "\n",
      "All changes complete!\n"
     ]
    }
   ],
   "source": [
    "# Verify all changes\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFYING CHANGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check args.py\n",
    "with open(r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\args.py\", 'r') as f:\n",
    "    if \"PoseLift\" in f.read():\n",
    "        print(\" args.py updated correctly\")\n",
    "    else:\n",
    "        print(\" args.py NOT updated\")\n",
    "\n",
    "# Check dataset.py\n",
    "with open(r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\dataset.py\", 'r') as f:\n",
    "    if \"dataset_poselift\" in f.read():\n",
    "        print(\" dataset.py updated correctly\")\n",
    "    else:\n",
    "        print(\" dataset.py NOT updated\")\n",
    "\n",
    "print(\"\\nAll changes complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb4be1ea-093f-4df4-a08b-5fdc20e955a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "STDOUT:\n",
      "\n",
      "\n",
      "STDERR:\n",
      "2025-12-10 13:12:19.237167: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-10 13:12:20.352580: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\style\\core.py\", line 137, in use\n",
      "    style = _rc_params_in_file(style)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py\", line 883, in _rc_params_in_file\n",
      "    with _open_file_or_url(fname) as fd:\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py\", line 137, in __enter__\n",
      "    return next(self.gen)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py\", line 860, in _open_file_or_url\n",
      "    with open(fname, encoding='utf-8') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'seaborn-ticks'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\train_eval.py\", line 11, in <module>\n",
      "    from dataset import get_dataset_and_loader\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\dataset.py\", line 9, in <module>\n",
      "    from utils.pose_utils import gen_clip_seg_data_np, get_ab_labels\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\utils\\pose_utils.py\", line 9, in <module>\n",
      "    plt.style.use('seaborn-ticks')\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\style\\core.py\", line 139, in use\n",
      "    raise OSError(\n",
      "OSError: 'seaborn-ticks' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)\n",
      "\n",
      "\n",
      "Exit code: 1\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\Krish\\PoseLift\\STG-NF\")\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"train_eval.py\",\n",
    "    \"--dataset\", \"PoseLift\",\n",
    "    \"--pose_path_train\", \"data/PoseLift/pose/train\",\n",
    "    \"--pose_path_test\", \"data/PoseLift/pose/test\",\n",
    "    \"--batch_size\", \"16\",\n",
    "    \"--seg_len\", \"30\",\n",
    "    \"--epochs\", \"20\",\n",
    "    \"--model_lr\", \"0.001\",\n",
    "    \"--num_workers\", \"0\",\n",
    "    \"--device\", \"cuda:0\"\n",
    "]\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "# Capture output\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "print(\"STDOUT:\")\n",
    "print(result.stdout)\n",
    "\n",
    "if result.stderr:\n",
    "    print(\"\\nSTDERR:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "print(f\"\\nExit code: {result.returncode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78b1cf0b-d6dc-44ea-aa8d-af5a717a7cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fixed pose_utils.py line 9\n",
      " 1: import os\n",
      " 2: import re\n",
      " 3: \n",
      " 4: import numpy as np\n",
      " 5: from PIL import Image\n",
      " 6: from matplotlib import pyplot as plt\n",
      " 7: from tqdm import tqdm\n",
      " 8: \n",
      " 9: # plt.style.use('seaborn-ticks')  # Commented out - style not available in this matplotlib version\n",
      "10: \n",
      "11: \n",
      "12: def get_ab_labels(global_data_np_ab, segs_meta_ab, path_to_vid_dir='', segs_root=''):\n",
      "13:     pose_segs_root = segs_root\n",
      "14:     clip_list = os.listdir(pose_segs_root)\n",
      "15:     clip_list = sorted(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pose_utils_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\utils\\pose_utils.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(pose_utils_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Replace line 9 (index 8)\n",
    "lines[8] = \"# plt.style.use('seaborn-ticks')  # Commented out - style not available in this matplotlib version\\n\"\n",
    "\n",
    "# Write back\n",
    "with open(pose_utils_path, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "print(\" Fixed pose_utils.py line 9\")\n",
    "\n",
    "# Verify\n",
    "with open(pose_utils_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f.readlines()[:15], 1):\n",
    "        print(f\"{i:2d}: {line.rstrip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5e8e972-66fc-4679-b4dd-a85330fb2c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rewrote dataset_poselift.py with correct STG-NF format\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# REWRITE dataset_poselift.py with correct output format\n",
    "dataset_code = '''import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PoseLiftDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PoseLift dataset for STG-NF anomaly detection.\n",
    "    Returns data in STG-NF format: (C, T, V) where C=2 (x,y), T=frames, V=17 joints\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pose_dir, split='train', num_frames=30, num_joints=17):\n",
    "        self.pose_dir = pose_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.num_joints = num_joints\n",
    "        self.split = split\n",
    "        \n",
    "        self.json_files = sorted([f for f in os.listdir(pose_dir) if f.endswith('.json')])\n",
    "        \n",
    "        if not self.json_files:\n",
    "            raise FileNotFoundError(f\"No JSON files found in {pose_dir}\")\n",
    "        \n",
    "        print(f\"[{split.upper()}] Found {len(self.json_files)} JSON files in {pose_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.json_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        json_path = os.path.join(self.pose_dir, self.json_files[idx])\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            video_data = json.load(f)\n",
    "        \n",
    "        # Collect all unique person IDs\n",
    "        person_ids = set()\n",
    "        for frame_id, persons in video_data.items():\n",
    "            person_ids.update(persons.keys())\n",
    "        \n",
    "        person_ids = sorted(list(person_ids))\n",
    "        \n",
    "        if not person_ids:\n",
    "            # Empty video - return zero tensor in STG-NF format: (C, T, V)\n",
    "            return torch.zeros(2, self.num_frames, self.num_joints).float(), 0, self.json_files[idx]\n",
    "        \n",
    "        person_id = person_ids[0]\n",
    "        pose_sequence = []\n",
    "        \n",
    "        # Sort frames by frame ID\n",
    "        frame_ids_sorted = sorted(video_data.keys(), \n",
    "                                   key=lambda x: int(x) if isinstance(x, str) else x)\n",
    "        \n",
    "        for frame_id in frame_ids_sorted:\n",
    "            if person_id in video_data[frame_id]:\n",
    "                person_data = video_data[frame_id][person_id]\n",
    "                \n",
    "                if isinstance(person_data, dict) and \"keypoints\" in person_data:\n",
    "                    keypoints_raw = person_data[\"keypoints\"]\n",
    "                else:\n",
    "                    keypoints_raw = person_data\n",
    "                \n",
    "                keypoints_raw = np.array(keypoints_raw, dtype=np.float32)\n",
    "                \n",
    "                if len(keypoints_raw) == self.num_joints * 3:\n",
    "                    keypoints_xy = keypoints_raw.reshape(self.num_joints, 3)[:, :2]  # (17, 2)\n",
    "                else:\n",
    "                    keypoints_xy = np.zeros((self.num_joints, 2), dtype=np.float32)\n",
    "            else:\n",
    "                keypoints_xy = np.zeros((self.num_joints, 2), dtype=np.float32)\n",
    "            \n",
    "            pose_sequence.append(keypoints_xy)\n",
    "        \n",
    "        # Convert to (T, 17, 2)\n",
    "        pose_sequence = np.array(pose_sequence, dtype=np.float32)\n",
    "        pose_sequence = np.nan_to_num(pose_sequence, nan=0.0)\n",
    "        \n",
    "        # Pad or truncate to num_frames\n",
    "        if pose_sequence.shape[0] < self.num_frames:\n",
    "            padding = np.zeros((self.num_frames - pose_sequence.shape[0], \n",
    "                               self.num_joints, 2), dtype=np.float32)\n",
    "            pose_sequence = np.vstack([pose_sequence, padding])\n",
    "        else:\n",
    "            pose_sequence = pose_sequence[:self.num_frames]\n",
    "        \n",
    "        # Convert to STG-NF format: (C, T, V) = (2, 30, 17)\n",
    "        # Transpose from (T, V, C) to (C, T, V)\n",
    "        pose_tensor = torch.from_numpy(pose_sequence).float()  # (T, 17, 2)\n",
    "        pose_tensor = pose_tensor.permute(2, 0, 1)  # (2, T, 17)\n",
    "        \n",
    "        # Return: (data, score, metadata)\n",
    "        # STG-NF expects: data tensor, confidence score, and metadata\n",
    "        return pose_tensor, 1.0, self.json_files[idx]\n",
    "'''\n",
    "\n",
    "# Write to file\n",
    "stg_nf_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\"\n",
    "with open(os.path.join(stg_nf_path, \"dataset_poselift.py\"), 'w', encoding='utf-8') as f:\n",
    "    f.write(dataset_code)\n",
    "\n",
    "print(\" Rewrote dataset_poselift.py with correct STG-NF format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47917e4f-3cfa-48e8-bfe6-7ec8dbefd9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Found 104 JSON files in C:\\Users\\Krish\\PoseLift\\STG-NF\\data\\PoseLift\\pose\\train\n",
      "Batch type: <class 'list'>\n",
      "Batch length: 3\n",
      "Data shape: torch.Size([4, 2, 30, 17])\n",
      "Scores shape: torch.Size([4])\n",
      "Metadata: ('04_066_alphapose_tracked_person.json', '01_243_alphapose_tracked_person.json')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r\"C:\\Users\\Krish\\PoseLift\\STG-NF\")\n",
    "\n",
    "# Force reload\n",
    "import importlib\n",
    "if 'dataset_poselift' in sys.modules:\n",
    "    del sys.modules['dataset_poselift']\n",
    "\n",
    "from dataset_poselift import PoseLiftDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\data\\PoseLift\\pose\\train\"\n",
    "train_dataset = PoseLiftDataset(train_path, split='train', num_frames=30)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch type: {type(batch)}\")\n",
    "print(f\"Batch length: {len(batch)}\")\n",
    "print(f\"Data shape: {batch[0].shape}\")  # Should be (4, 2, 30, 17)\n",
    "print(f\"Scores shape: {batch[1].shape}\")  # Should be (4,)\n",
    "print(f\"Metadata: {batch[2][:2]}\")  # First 2 IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bda72e54-0d41-4bfc-b047-9e0cb02295c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Args:\n",
      "  vid_path_train: None\n",
      "  pose_path_train_abnormal: None\n",
      "  pose_path_train: data/PoseLift/pose/train\n",
      "  vid_path_test: None\n",
      "  pose_path_test: data/PoseLift/pose/test\n",
      "  dataset: PoseLift\n",
      "  vid_res: None\n",
      "  device: cuda:0\n",
      "  seed: 999\n",
      "  verbose: 1\n",
      "  data_dir: data/\n",
      "  exp_dir: data/exp_dir\n",
      "  num_workers: 8\n",
      "  plot_vid: 0\n",
      "  only_test: False\n",
      "  num_transform: 2\n",
      "  headless: False\n",
      "  norm_scale: 0\n",
      "  prop_norm_scale: 1\n",
      "  train_seg_conf_th: 0.0\n",
      "  seg_len: 30\n",
      "  seg_stride: 6\n",
      "  specific_clip: None\n",
      "  global_pose_segs: True\n",
      "  checkpoint: None\n",
      "  batch_size: 16\n",
      "  epochs: 20\n",
      "  model_optimizer: adamx\n",
      "  model_sched: exp_decay\n",
      "  model_lr: 0.0005\n",
      "  model_weight_decay: 5e-05\n",
      "  model_lr_decay: 0.99\n",
      "  model_hidden_dim: 0\n",
      "  model_confidence: False\n",
      "  K: 8\n",
      "  L: 1\n",
      "  R: 3.0\n",
      "  temporal_kernel: None\n",
      "  edge_importance: False\n",
      "  flow_permutation: permute\n",
      "  adj_strategy: uniform\n",
      "  max_hops: 8\n",
      "  vid_path: {'train': 'data/PoseLift\\\\train/images/', 'test': 'data/PoseLift\\\\test/frames/'}\n",
      "  pose_path: {'train': 'data/PoseLift\\\\pose\\\\train/', 'test': 'data/PoseLift\\\\pose\\\\test/', 'train_abnormal': None}\n",
      "  ckpt_dir: None\n",
      "  optimizer: adamx\n",
      "  sched: exp_decay\n",
      "  lr: 0.0005\n",
      "  weight_decay: 5e-05\n",
      "  lr_decay: 0.99\n",
      "  hidden_dim: 0\n",
      "  confidence: False\n"
     ]
    }
   ],
   "source": [
    "# Check what model_args are being set\n",
    "import sys\n",
    "sys.path.insert(0, r\"C:\\Users\\Krish\\PoseLift\\STG-NF\")\n",
    "\n",
    "from args import init_parser, init_sub_args\n",
    "\n",
    "parser = init_parser()\n",
    "args = parser.parse_args([\n",
    "    '--dataset', 'PoseLift',\n",
    "    '--pose_path_train', 'data/PoseLift/pose/train',\n",
    "    '--pose_path_test', 'data/PoseLift/pose/test',\n",
    "    '--batch_size', '16',\n",
    "    '--seg_len', '30',\n",
    "    '--epochs', '20'\n",
    "])\n",
    "\n",
    "args, model_args = init_sub_args(args)\n",
    "\n",
    "print(\"Model Args:\")\n",
    "for key, value in model_args.__dict__.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2af5f49-95d5-4c2f-8f15-1d3b61d08aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting STG-NF training on PoseLift with temporal_kernel=3...\n",
      "\n",
      "============================================================\n",
      "TRAINING OUTPUT (last 4000 chars)\n",
      "============================================================\n",
      "Experiment directories created\n",
      "[TRAIN] Found 104 JSON files in data/PoseLift\\pose\\train/\n",
      "[TEST] Found 47 JSON files in data/PoseLift\\pose\\test/\n",
      "Number of params in net: 0.296K\n",
      "Starting Epoch 1 / 20\n",
      "\n",
      "\n",
      "STDERR (last 1500 chars):\n",
      "2025-12-10 13:17:39.860317: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-10 13:17:40.783979: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\train_eval.py\", line 60, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\train_eval.py\", line 47, in main\n",
      "    trainer.train(log_writer=writer)\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\training.py\", line 118, in train\n",
      "    data = [data.to(self.args.device, non_blocking=True) for data in data_arr]\n",
      "            ^^^^^^^\n",
      "AttributeError: 'list' object has no attribute 'to'\n",
      "\n",
      "\n",
      "Exit code: 1\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\Krish\\PoseLift\\STG-NF\")\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"train_eval.py\",\n",
    "    \"--dataset\", \"PoseLift\",\n",
    "    \"--pose_path_train\", \"data/PoseLift/pose/train\",\n",
    "    \"--pose_path_test\", \"data/PoseLift/pose/test\",\n",
    "    \"--batch_size\", \"16\",\n",
    "    \"--seg_len\", \"30\",\n",
    "    \"--epochs\", \"20\",\n",
    "    \"--model_lr\", \"0.001\",\n",
    "    \"--num_workers\", \"0\",\n",
    "    \"--device\", \"cuda:0\",\n",
    "    \"--temporal_kernel\", \"3\"  # ADD THIS - must be odd number\n",
    "]\n",
    "\n",
    "print(\"Starting STG-NF training on PoseLift with temporal_kernel=3...\\n\")\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "# Show output\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING OUTPUT (last 4000 chars)\")\n",
    "print(\"=\" * 60)\n",
    "if result.stdout:\n",
    "    output = result.stdout[-4000:]\n",
    "    print(output)\n",
    "else:\n",
    "    print(\"(no stdout)\")\n",
    "\n",
    "if result.stderr:\n",
    "    print(\"\\nSTDERR (last 1500 chars):\")\n",
    "    print(result.stderr[-1500:])\n",
    "\n",
    "print(f\"\\nExit code: {result.returncode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55627d6f-2be9-48e9-935a-343e1a3364a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines 110-130 of training.py:\n",
      "110:         key_break = False\n",
      "111:         for epoch in range(start_epoch, num_epochs):\n",
      "112:             if key_break:\n",
      "113:                 break\n",
      "114:             print(\"Starting Epoch {} / {}\".format(epoch + 1, num_epochs))\n",
      "115:             pbar = tqdm(self.train_loader)\n",
      "116:             for itern, data_arr in enumerate(pbar):\n",
      "117:                 try:\n",
      "118:                     data = [data.to(self.args.device, non_blocking=True) for data in data_arr]\n",
      "119:                     score = data[-2].amin(dim=-1)\n",
      "120:                     label = data[-1]\n",
      "121:                     if self.args.model_confidence:\n",
      "122:                         samp = data[0]\n",
      "123:                     else:\n",
      "124:                         samp = data[0][:, :2]\n",
      "125:                     z, nll = self.model(samp.float(), label=label, score=score)\n",
      "126:                     if nll is None:\n",
      "127:                         continue\n",
      "128:                     if self.args.model_confidence:\n",
      "129:                         nll = nll * score\n",
      "130:                     losses = compute_loss(nll, reduction=\"mean\")[\"total_loss\"]\n"
     ]
    }
   ],
   "source": [
    "# Check what trainer.train expects\n",
    "training_path = r\"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\training.py\"\n",
    "\n",
    "with open(training_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Show lines around line 118\n",
    "print(\"Lines 110-130 of training.py:\")\n",
    "for i in range(109, min(130, len(lines))):\n",
    "    print(f\"{i+1:3d}: {lines[i].rstrip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f8eb69d-587d-4a9f-8a72-b99def2cf8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Krish\\PoseLift\\STG-NF\n",
      "\n",
      "Files in current directory:\n",
      "  .git\n",
      "  .gitignore\n",
      "  args.py\n",
      "  checkpoints\n",
      "  data\n",
      "  dataset.py\n",
      "  dataset_poselift.py\n",
      "  environment.yml\n",
      "  gen_data.py\n",
      "  LICENSE\n",
      "  models\n",
      "  README.md\n",
      "  runs\n",
      "  STG-NF\n",
      "  train_eval.py\n",
      "  utils\n",
      "  __pycache__\n",
      "\n",
      "=== Searching for JSON files ===\n",
      "Found: .\\data\\PoseLift\\pose\\test\\01_0222_alphapose_tracked_person.json\n",
      "Found: .\\data\\PoseLift\\pose\\test\\01_0225_alphapose_tracked_person.json\n",
      "Found: .\\data\\PoseLift\\pose\\test\\01_0240_alphapose_tracked_person.json\n",
      "Found: .\\data\\PoseLift\\pose\\test\\01_0242_alphapose_tracked_person.json\n",
      "Found: .\\data\\PoseLift\\pose\\test\\01_0245_alphapose_tracked_person.json\n",
      "Found: .\\data\\PoseLift\\pose\\test\\01_0251_alphapose_tracked_person.json\n",
      "Found: .\\data\\PoseLift\\pose\\test\\01_0257_alphapose_tracked_person.json\n",
      "Found: .\\data\\PoseLift\\pose\\test\\01_0272_alphapose_tracked_person.json\n",
      "Found: .\\data\\PoseLift\\pose\\test\\01_0273_alphapose_tracked_person.json\n",
      "Found: .\\data\\PoseLift\\pose\\test\\01_0282_alphapose_tracked_person.json\n",
      "\n",
      "Total JSON files found: 151\n",
      "\n",
      "'data' folder exists. Contents:\n",
      "  arch.png\n",
      "  exp_dir\n",
      "  PoseLift\n",
      "  ShanghaiTech\n",
      "  UBnormal\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Check what's in your current directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nFiles in current directory:\")\n",
    "for item in os.listdir('.'):\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "# Find all JSON files recursively from current directory\n",
    "print(\"\\n=== Searching for JSON files ===\")\n",
    "json_files = []\n",
    "for root, dirs, files in os.walk('.'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            json_files.append(os.path.join(root, file))\n",
    "            if len(json_files) <= 10:\n",
    "                print(f\"Found: {os.path.join(root, file)}\")\n",
    "\n",
    "print(f\"\\nTotal JSON files found: {len(json_files)}\")\n",
    "\n",
    "# Also check if data folder exists\n",
    "if os.path.exists('data'):\n",
    "    print(\"\\n'data' folder exists. Contents:\")\n",
    "    for item in os.listdir('data'):\n",
    "        print(f\"  {item}\")\n",
    "    \n",
    "    if os.path.exists('data/pose'):\n",
    "        print(\"\\n'data/pose' folder exists. Contents:\")\n",
    "        for item in os.listdir('data/pose'):\n",
    "            print(f\"  {item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e116b525-47a7-4f10-b078-ca0ec287a2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing PoseLiftDataset ===\n",
      "Loaded 104 pose sequences from ./data/PoseLift/pose/train\n",
      "Dataset size: 104\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 158\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# Test a single sample\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample 0 structure:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(sample)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[46], line 133\u001b[0m, in \u001b[0;36mPoseLiftDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    130\u001b[0m pose_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpose_files[idx]\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Load pose sequence\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m pose_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pose_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpose_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# trans_index: 0 means no transformation applied\u001b[39;00m\n\u001b[0;32m    136\u001b[0m trans_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[46], line 75\u001b[0m, in \u001b[0;36mPoseLiftDataset._load_pose_sequence\u001b[1;34m(self, json_file)\u001b[0m\n\u001b[0;32m     72\u001b[0m     keypoints \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(keypoints, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Handle different keypoint formats\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mkeypoints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:  \u001b[38;5;66;03m# [x, y, confidence]\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     keypoints \u001b[38;5;241m=\u001b[39m keypoints[:, :\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Take only x, y\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m keypoints\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:  \u001b[38;5;66;03m# [x, y]\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class PoseLiftDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading PoseLift JSON pose sequences.\n",
    "    Returns data in STG-NF compatible format: [pose_tensor, trans_index, score, label]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pose_json_dir, normalize=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pose_json_dir: Path to directory containing pose JSON files\n",
    "            normalize: Whether to normalize pose sequences\n",
    "        \"\"\"\n",
    "        self.pose_json_dir = pose_json_dir\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # Get all JSON files\n",
    "        self.pose_files = sorted([\n",
    "            os.path.join(pose_json_dir, f) \n",
    "            for f in os.listdir(pose_json_dir) \n",
    "            if f.endswith('.json')\n",
    "        ])\n",
    "        \n",
    "        print(f\"Loaded {len(self.pose_files)} pose sequences from {pose_json_dir}\")\n",
    "        \n",
    "        if len(self.pose_files) == 0:\n",
    "            raise ValueError(f\"No JSON files found in {pose_json_dir}\")\n",
    "    \n",
    "    def _load_pose_sequence(self, json_file):\n",
    "        \"\"\"\n",
    "        Load pose sequence from JSON file.\n",
    "        Returns tensor of shape (2, T, 17) where:\n",
    "        - 2 = [x, y] coordinates\n",
    "        - T = temporal dimension (frame count)\n",
    "        - 17 = COCO keypoints\n",
    "        \"\"\"\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Handle different JSON structures\n",
    "        if isinstance(data, dict):\n",
    "            if 'people' in data:\n",
    "                people = data['people']\n",
    "            elif 'keypoints' in data:\n",
    "                people = data['keypoints']\n",
    "            else:\n",
    "                # Try to extract first person's keypoints\n",
    "                people = [data]\n",
    "        else:\n",
    "            people = data\n",
    "        \n",
    "        # Extract keypoints from all frames\n",
    "        pose_sequence = []\n",
    "        \n",
    "        for frame_data in people:\n",
    "            if isinstance(frame_data, dict):\n",
    "                if 'keypoints' in frame_data:\n",
    "                    keypoints = frame_data['keypoints']\n",
    "                else:\n",
    "                    # Assume frame_data directly contains keypoints\n",
    "                    keypoints = frame_data\n",
    "            else:\n",
    "                keypoints = frame_data\n",
    "            \n",
    "            # Convert to numpy array if needed\n",
    "            if isinstance(keypoints, list):\n",
    "                keypoints = np.array(keypoints, dtype=np.float32)\n",
    "            \n",
    "            # Handle different keypoint formats\n",
    "            if keypoints.shape[-1] == 3:  # [x, y, confidence]\n",
    "                keypoints = keypoints[:, :2]  # Take only x, y\n",
    "            elif keypoints.shape[-1] == 2:  # [x, y]\n",
    "                pass\n",
    "            else:\n",
    "                # Try to reshape\n",
    "                keypoints = keypoints.reshape(-1, 2)\n",
    "            \n",
    "            pose_sequence.append(keypoints)\n",
    "        \n",
    "        # Convert list of frames to array\n",
    "        pose_array = np.array(pose_sequence, dtype=np.float32)  # Shape: (T, 17, 2)\n",
    "        \n",
    "        # Ensure we have correct shape\n",
    "        if pose_array.shape[1] != 17 or pose_array.shape[2] != 2:\n",
    "            raise ValueError(f\"Unexpected pose shape: {pose_array.shape}. Expected (T, 17, 2)\")\n",
    "        \n",
    "        # Normalize if requested\n",
    "        if self.normalize:\n",
    "            # Normalize to 0-1 range per frame\n",
    "            for t in range(pose_array.shape[0]):\n",
    "                frame = pose_array[t]\n",
    "                x_min, x_max = frame[:, 0].min(), frame[:, 0].max()\n",
    "                y_min, y_max = frame[:, 1].min(), frame[:, 1].max()\n",
    "                \n",
    "                if x_max > x_min:\n",
    "                    pose_array[t, :, 0] = (frame[:, 0] - x_min) / (x_max - x_min + 1e-6)\n",
    "                if y_max > y_min:\n",
    "                    pose_array[t, :, 1] = (frame[:, 1] - y_min) / (y_max - y_min + 1e-6)\n",
    "        \n",
    "        # Handle NaN values\n",
    "        pose_array = np.nan_to_num(pose_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Transpose to (2, T, 17) format expected by STG-NF\n",
    "        # Input: (T, 17, 2)\n",
    "        # Output: (2, T, 17)\n",
    "        pose_tensor = torch.from_numpy(pose_array).float()  # (T, 17, 2)\n",
    "        pose_tensor = pose_tensor.permute(2, 0, 1)  # (2, T, 17)\n",
    "        \n",
    "        return pose_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pose_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns data in STG-NF compatible format:\n",
    "        [pose_tensor, trans_index, score, label]\n",
    "        \n",
    "        Where:\n",
    "        - pose_tensor: shape (2, T, 17) - the pose sequence\n",
    "        - trans_index: 0 (no transformation)\n",
    "        - score: tensor of shape (1,) - confidence score (1.0 for all)\n",
    "        - label: tensor - class label (0 for normal, 1 for anomaly)\n",
    "        \"\"\"\n",
    "        pose_file = self.pose_files[idx]\n",
    "        \n",
    "        # Load pose sequence\n",
    "        pose_tensor = self._load_pose_sequence(pose_file)\n",
    "        \n",
    "        # trans_index: 0 means no transformation applied\n",
    "        trans_index = 0\n",
    "        \n",
    "        # score: confidence score as tensor\n",
    "        # Using 1.0 for all samples (you can modify based on your ground truth)\n",
    "        score = torch.ones(1, dtype=torch.float32)\n",
    "        \n",
    "        # label: class label as tensor\n",
    "        # Assuming all samples are normal (label=0)\n",
    "        # Change this if you have ground truth labels\n",
    "        label = torch.tensor(0, dtype=torch.long)\n",
    "        \n",
    "        # Return as 4-element LIST (NOT tuple) matching STG-NF format\n",
    "        # Trainer expects: data[-2] = score, data[-1] = label\n",
    "        return [pose_tensor, trans_index, score, label]\n",
    "\n",
    "\n",
    "# Test the dataset\n",
    "print(\"\\n=== Testing PoseLiftDataset ===\")\n",
    "train_dataset = PoseLiftDataset('./data/PoseLift/pose/train')\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "\n",
    "# Test a single sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample 0 structure:\")\n",
    "print(f\"  Type: {type(sample)}\")\n",
    "print(f\"  Length: {len(sample)}\")\n",
    "print(f\"  pose_tensor shape: {sample[0].shape} (expected: torch.Size([2, T, 17]))\")\n",
    "print(f\"  trans_index: {sample[1]} (type: {type(sample[1])})\")\n",
    "print(f\"  score shape: {sample[2].shape}, value: {sample[2].item()}\")\n",
    "print(f\"  label: {sample[3].item()}\")\n",
    "\n",
    "# Test DataLoader compatibility\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for PoseLift data\"\"\"\n",
    "    # batch is a list of samples\n",
    "    # Each sample is [pose_tensor, trans_index, score, label]\n",
    "    \n",
    "    poses = [item[0] for item in batch]\n",
    "    trans_indices = [item[1] for item in batch]\n",
    "    scores = torch.cat([item[2] for item in batch], dim=0)\n",
    "    labels = torch.stack([item[3] for item in batch], dim=0)\n",
    "    \n",
    "    return poses, trans_indices, scores, labels\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Testing DataLoader ===\")\n",
    "for batch_idx, (poses, trans_indices, scores, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"  Number of poses: {len(poses)}\")\n",
    "    print(f\"  First pose shape: {poses[0].shape}\")\n",
    "    print(f\"  Scores shape: {scores.shape}, values: {scores.tolist()}\")\n",
    "    print(f\"  Labels shape: {labels.shape}, values: {labels.tolist()}\")\n",
    "    break\n",
    "\n",
    "print(\"\\n Dataset test completed successfully!\")\n",
    "\n",
    "# Now create both train and test datasets\n",
    "print(\"\\n=== Creating Train and Test Datasets ===\")\n",
    "train_dataset = PoseLiftDataset('./data/PoseLift/pose/train')\n",
    "test_dataset = PoseLiftDataset('./data/PoseLift/pose/test')\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain loader: {len(train_loader)} batches\")\n",
    "print(f\"Test loader: {len(test_loader)} batches\")\n",
    "print(\" Ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a1c69a2-6e1a-439d-aa5e-ed2523ae8c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 104\n",
      "\n",
      "First 5 files:\n",
      "  01_097_alphapose_tracked_person.json\n",
      "  01_126_alphapose_tracked_person.json\n",
      "  01_127_alphapose_tracked_person.json\n",
      "  01_132_alphapose_tracked_person.json\n",
      "  01_134_alphapose_tracked_person.json\n",
      "\n",
      "Opening: ./data/PoseLift/pose/train\\01_097_alphapose_tracked_person.json\n",
      "\n",
      "JSON Structure:\n",
      "Type: <class 'dict'>\n",
      "Keys: dict_keys(['167', '47', '54', '89', '93'])\n",
      "\n",
      "Key: 167\n",
      "Value type: <class 'dict'>\n",
      "Content (first 300 chars): {'1350': {'keypoints': [631.5, 1266.0, 0.712890625, 631.5, 1278.0, 0.71240234375, 624.0, 1270.0, 0.6513671875, 594.0, 1304.0, 0.1063232421875, 582.5, 1304.0, 0.1431884765625, 586.0, 1304.0, 0.054473876953125, 586.0, 1304.0, 0.07830810546875, 628.0, 1304.0, 0.59375, 631.5, 1304.0, 0.288818359375, 597\n",
      "\n",
      "Key: 47\n",
      "Value type: <class 'dict'>\n",
      "Content (first 300 chars): {'465': {'keypoints': [90.0, 761.0, 0.90576171875, 86.0, 763.0, 0.9189453125, 86.0, 761.0, 0.904296875, 90.0, 773.0, 0.87353515625, 86.0, 761.0, 0.40478515625, 104.0, 779.0, 0.787109375, 104.0, 765.0, 0.75732421875, 120.0, 791.0, 0.67578125, 128.0, 761.0, 0.452392578125, 94.0, 791.0, 0.3525390625, 1\n",
      "\n",
      "Key: 54\n",
      "Value type: <class 'dict'>\n",
      "Content (first 300 chars): {'533': {'keypoints': [578.0, 1318.0, 0.056976318359375, 578.0, 1325.0, 0.060546875, 578.0, 1318.0, 0.076416015625, 633.0, 1224.0, 0.03216552734375, 578.0, 1314.0, 0.0277862548828125, 699.0, 1238.0, 0.047760009765625, 685.0, 1325.0, 0.044891357421875, 592.0, 1314.0, 0.3310546875, 592.0, 1321.0, 0.05\n",
      "\n",
      "Key: 89\n",
      "Value type: <class 'dict'>\n",
      "Content (first 300 chars): {'832': {'keypoints': [80.5625, 762.0, 0.8486328125, 78.5, 764.0, 0.85888671875, 76.375, 762.0, 0.79736328125, 80.5625, 772.0, 0.76513671875, 80.5625, 774.0, 0.5029296875, 101.3125, 774.0, 0.78857421875, 99.25, 770.0, 0.6474609375, 126.25, 770.0, 0.76416015625, 122.125, 768.0, 0.337158203125, 122.12\n",
      "\n",
      "Key: 93\n",
      "Value type: <class 'dict'>\n",
      "Content (first 300 chars): {'900': {'keypoints': [136.75, 307.0, 0.68115234375, 132.375, 311.5, 0.67431640625, 132.375, 307.0, 0.6474609375, 136.75, 317.75, 0.64208984375, 136.75, 320.0, 0.42822265625, 158.125, 324.0, 0.76953125, 153.75, 324.0, 0.5419921875, 188.0, 332.5, 0.58447265625, 185.875, 328.5, 0.392333984375, 196.625\n",
      "\n",
      "\n",
      "=== Full JSON (first 1500 chars) ===\n",
      "{\n",
      "  \"167\": {\n",
      "    \"1350\": {\n",
      "      \"keypoints\": [\n",
      "        631.5,\n",
      "        1266.0,\n",
      "        0.712890625,\n",
      "        631.5,\n",
      "        1278.0,\n",
      "        0.71240234375,\n",
      "        624.0,\n",
      "        1270.0,\n",
      "        0.6513671875,\n",
      "        594.0,\n",
      "        1304.0,\n",
      "        0.1063232421875,\n",
      "        582.5,\n",
      "        1304.0,\n",
      "        0.1431884765625,\n",
      "        586.0,\n",
      "        1304.0,\n",
      "        0.054473876953125,\n",
      "        586.0,\n",
      "        1304.0,\n",
      "        0.07830810546875,\n",
      "        628.0,\n",
      "        1304.0,\n",
      "        0.59375,\n",
      "        631.5,\n",
      "        1304.0,\n",
      "        0.288818359375,\n",
      "        597.5,\n",
      "        1274.0,\n",
      "        0.751953125,\n",
      "        597.5,\n",
      "        1274.0,\n",
      "        0.193603515625,\n",
      "        696.0,\n",
      "        1289.0,\n",
      "        0.20947265625,\n",
      "        684.5,\n",
      "        1255.0,\n",
      "        0.338134765625,\n",
      "        752.5,\n",
      "        1274.0,\n",
      "        0.09234619140625,\n",
      "        756.5,\n",
      "        1274.0,\n",
      "        0.275634765625,\n",
      "        794.0,\n",
      "        1198.0,\n",
      "        0.057159423828125,\n",
      "        794.0,\n",
      "        1141.0,\n",
      "        0.09429931640625\n",
      "      ],\n",
      "      \"scores\": null\n",
      "    },\n",
      "    \"1351\": {\n",
      "      \"keypoints\": [\n",
      "        626.5,\n",
      "        1255.0,\n",
      "        0.71142578125,\n",
      "        622.5,\n",
      "        1270.0,\n",
      "        0.71240234375,\n",
      "        619.0,\n",
      "        1262.0,\n",
      "        0.59619140625,\n",
      "        680.0,\n",
      "        1304.0,\n",
      "        0.1600341796875,\n",
      "        619.0,\n",
      "        1255.0,\n",
      "        0.12005615234375,\n",
      "        722.0,\n",
      "        1274.0,\n",
      "        0.1009521484375,\n",
      "        683.5,\n",
      "        1247.0,\n",
      "        0.2406005859375,\n",
      "        622.5,\n",
      "        1300.0,\n",
      "        0.258056640625,\n",
      "       \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# List all files in the train directory\n",
    "train_dir = './data/PoseLift/pose/train'\n",
    "files = os.listdir(train_dir)\n",
    "print(f\"Total files: {len(files)}\")\n",
    "print(f\"\\nFirst 5 files:\")\n",
    "for f in sorted(files)[:5]:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Get the first file\n",
    "first_file = os.path.join(train_dir, sorted(files)[0])\n",
    "print(f\"\\nOpening: {first_file}\")\n",
    "\n",
    "# Inspect the first JSON file\n",
    "with open(first_file, 'r') as f:\n",
    "    sample_data = json.load(f)\n",
    "\n",
    "print(\"\\nJSON Structure:\")\n",
    "print(f\"Type: {type(sample_data)}\")\n",
    "print(f\"Keys: {sample_data.keys() if isinstance(sample_data, dict) else 'N/A'}\")\n",
    "\n",
    "# Print first few elements\n",
    "if isinstance(sample_data, dict):\n",
    "    for key in list(sample_data.keys())[:5]:\n",
    "        print(f\"\\nKey: {key}\")\n",
    "        print(f\"Value type: {type(sample_data[key])}\")\n",
    "        if isinstance(sample_data[key], (list, dict)):\n",
    "            val_str = str(sample_data[key])\n",
    "            print(f\"Content (first 300 chars): {val_str[:300]}\")\n",
    "elif isinstance(sample_data, list):\n",
    "    print(f\"\\nList length: {len(sample_data)}\")\n",
    "    print(f\"First item type: {type(sample_data[0])}\")\n",
    "    print(f\"First item: {str(sample_data[0])[:500]}\")\n",
    "\n",
    "# Try to pretty print\n",
    "print(\"\\n\\n=== Full JSON (first 1500 chars) ===\")\n",
    "print(json.dumps(sample_data, indent=2)[:1500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c59c8cc6-632c-4f28-8ff9-6cdba2a34bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing PoseLiftDataset ===\n",
      "Loaded 104 pose sequences from ./data/PoseLift/pose/train\n",
      "Dataset size: 104\n",
      "\n",
      "Sample 0 structure:\n",
      "  Type: <class 'list'>\n",
      "  Length: 4\n",
      "  pose_tensor shape: torch.Size([2, 1006, 17]) (expected: torch.Size([2, T, 17]))\n",
      "  trans_index: 0 (type: <class 'int'>)\n",
      "  score shape: torch.Size([1]), value: 1.0\n",
      "  label: 0\n",
      "\n",
      "=== Testing DataLoader ===\n",
      "Batch 0:\n",
      "  Number of poses: 4\n",
      "  First pose shape: torch.Size([2, 1172, 17])\n",
      "  Scores shape: torch.Size([4]), values: [1.0, 1.0, 1.0, 1.0]\n",
      "  Labels shape: torch.Size([4]), values: [0, 0, 0, 0]\n",
      "\n",
      " Dataset test completed successfully!\n",
      "\n",
      "=== Creating Train and Test Datasets ===\n",
      "Loaded 104 pose sequences from ./data/PoseLift/pose/train\n",
      "Loaded 47 pose sequences from ./data/PoseLift/pose/test\n",
      "\n",
      "Train loader: 4 batches\n",
      "Test loader: 2 batches\n",
      " Ready for training!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class PoseLiftDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading PoseLift JSON pose sequences.\n",
    "    Returns data in STG-NF compatible format: [pose_tensor, trans_index, score, label]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pose_json_dir, normalize=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pose_json_dir: Path to directory containing pose JSON files\n",
    "            normalize: Whether to normalize pose sequences\n",
    "        \"\"\"\n",
    "        self.pose_json_dir = pose_json_dir\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # Get all JSON files\n",
    "        self.pose_files = sorted([\n",
    "            os.path.join(pose_json_dir, f) \n",
    "            for f in os.listdir(pose_json_dir) \n",
    "            if f.endswith('.json')\n",
    "        ])\n",
    "        \n",
    "        print(f\"Loaded {len(self.pose_files)} pose sequences from {pose_json_dir}\")\n",
    "        \n",
    "        if len(self.pose_files) == 0:\n",
    "            raise ValueError(f\"No JSON files found in {pose_json_dir}\")\n",
    "    \n",
    "    def _load_pose_sequence(self, json_file):\n",
    "        \"\"\"\n",
    "        Load pose sequence from JSON file.\n",
    "        \n",
    "        JSON structure:\n",
    "        {\n",
    "            \"person_id\": {\n",
    "                \"frame_id\": {\n",
    "                    \"keypoints\": [x1, y1, conf1, x2, y2, conf2, ..., x17, y17, conf17],\n",
    "                    \"scores\": null\n",
    "                },\n",
    "                ...\n",
    "            },\n",
    "            ...\n",
    "        }\n",
    "        \n",
    "        Returns tensor of shape (2, T, 17) where:\n",
    "        - 2 = [x, y] coordinates\n",
    "        - T = temporal dimension (total frames across all people)\n",
    "        - 17 = COCO keypoints\n",
    "        \"\"\"\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        pose_sequence = []\n",
    "        \n",
    "        # Iterate through all people\n",
    "        for person_id in sorted(data.keys(), key=lambda x: int(x)):\n",
    "            person_data = data[person_id]\n",
    "            \n",
    "            # Iterate through all frames for this person\n",
    "            for frame_id in sorted(person_data.keys(), key=lambda x: int(x)):\n",
    "                frame_data = person_data[frame_id]\n",
    "                \n",
    "                # Extract keypoints\n",
    "                keypoints_flat = frame_data['keypoints']\n",
    "                \n",
    "                if keypoints_flat is None:\n",
    "                    continue\n",
    "                \n",
    "                # Convert flat list to numpy array\n",
    "                keypoints_array = np.array(keypoints_flat, dtype=np.float32)\n",
    "                \n",
    "                # Reshape from [x1, y1, conf1, ..., x17, y17, conf17] to (17, 3)\n",
    "                keypoints_array = keypoints_array.reshape(-1, 3)  # (17, 3)\n",
    "                \n",
    "                # Take only x, y (drop confidence)\n",
    "                keypoints_xy = keypoints_array[:, :2]  # (17, 2)\n",
    "                \n",
    "                pose_sequence.append(keypoints_xy)\n",
    "        \n",
    "        if len(pose_sequence) == 0:\n",
    "            raise ValueError(f\"No valid keypoints found in {json_file}\")\n",
    "        \n",
    "        # Convert list of frames to array\n",
    "        pose_array = np.array(pose_sequence, dtype=np.float32)  # Shape: (T, 17, 2)\n",
    "        \n",
    "        # Ensure we have correct shape\n",
    "        if pose_array.shape[1] != 17 or pose_array.shape[2] != 2:\n",
    "            raise ValueError(f\"Unexpected pose shape: {pose_array.shape}. Expected (T, 17, 2)\")\n",
    "        \n",
    "        # Normalize if requested\n",
    "        if self.normalize:\n",
    "            # Normalize to 0-1 range per frame\n",
    "            for t in range(pose_array.shape[0]):\n",
    "                frame = pose_array[t]\n",
    "                x_min, x_max = frame[:, 0].min(), frame[:, 0].max()\n",
    "                y_min, y_max = frame[:, 1].min(), frame[:, 1].max()\n",
    "                \n",
    "                if x_max > x_min:\n",
    "                    pose_array[t, :, 0] = (frame[:, 0] - x_min) / (x_max - x_min + 1e-6)\n",
    "                if y_max > y_min:\n",
    "                    pose_array[t, :, 1] = (frame[:, 1] - y_min) / (y_max - y_min + 1e-6)\n",
    "        \n",
    "        # Handle NaN values\n",
    "        pose_array = np.nan_to_num(pose_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Transpose to (2, T, 17) format expected by STG-NF\n",
    "        # Input: (T, 17, 2)\n",
    "        # Output: (2, T, 17)\n",
    "        pose_tensor = torch.from_numpy(pose_array).float()  # (T, 17, 2)\n",
    "        pose_tensor = pose_tensor.permute(2, 0, 1)  # (2, T, 17)\n",
    "        \n",
    "        return pose_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pose_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns data in STG-NF compatible format:\n",
    "        [pose_tensor, trans_index, score, label]\n",
    "        \n",
    "        Where:\n",
    "        - pose_tensor: shape (2, T, 17) - the pose sequence\n",
    "        - trans_index: 0 (no transformation)\n",
    "        - score: tensor of shape (1,) - confidence score (1.0 for all)\n",
    "        - label: tensor - class label (0 for normal, 1 for anomaly)\n",
    "        \"\"\"\n",
    "        pose_file = self.pose_files[idx]\n",
    "        \n",
    "        # Load pose sequence\n",
    "        pose_tensor = self._load_pose_sequence(pose_file)\n",
    "        \n",
    "        # trans_index: 0 means no transformation applied\n",
    "        trans_index = 0\n",
    "        \n",
    "        # score: confidence score as tensor\n",
    "        # Using 1.0 for all samples (you can modify based on your ground truth)\n",
    "        score = torch.ones(1, dtype=torch.float32)\n",
    "        \n",
    "        # label: class label as tensor\n",
    "        # Assuming all samples are normal (label=0)\n",
    "        # Change this if you have ground truth labels\n",
    "        label = torch.tensor(0, dtype=torch.long)\n",
    "        \n",
    "        # Return as 4-element LIST (NOT tuple) matching STG-NF format\n",
    "        # Trainer expects: data[-2] = score, data[-1] = label\n",
    "        return [pose_tensor, trans_index, score, label]\n",
    "\n",
    "\n",
    "# Test the dataset\n",
    "print(\"\\n=== Testing PoseLiftDataset ===\")\n",
    "train_dataset = PoseLiftDataset('./data/PoseLift/pose/train')\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "\n",
    "# Test a single sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample 0 structure:\")\n",
    "print(f\"  Type: {type(sample)}\")\n",
    "print(f\"  Length: {len(sample)}\")\n",
    "print(f\"  pose_tensor shape: {sample[0].shape} (expected: torch.Size([2, T, 17]))\")\n",
    "print(f\"  trans_index: {sample[1]} (type: {type(sample[1])})\")\n",
    "print(f\"  score shape: {sample[2].shape}, value: {sample[2].item()}\")\n",
    "print(f\"  label: {sample[3].item()}\")\n",
    "\n",
    "# Test DataLoader compatibility\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for PoseLift data\"\"\"\n",
    "    # batch is a list of samples\n",
    "    # Each sample is [pose_tensor, trans_index, score, label]\n",
    "    \n",
    "    poses = [item[0] for item in batch]\n",
    "    trans_indices = [item[1] for item in batch]\n",
    "    scores = torch.cat([item[2] for item in batch], dim=0)\n",
    "    labels = torch.stack([item[3] for item in batch], dim=0)\n",
    "    \n",
    "    return poses, trans_indices, scores, labels\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Testing DataLoader ===\")\n",
    "for batch_idx, (poses, trans_indices, scores, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"  Number of poses: {len(poses)}\")\n",
    "    print(f\"  First pose shape: {poses[0].shape}\")\n",
    "    print(f\"  Scores shape: {scores.shape}, values: {scores.tolist()}\")\n",
    "    print(f\"  Labels shape: {labels.shape}, values: {labels.tolist()}\")\n",
    "    break\n",
    "\n",
    "print(\"\\n Dataset test completed successfully!\")\n",
    "\n",
    "# Now create both train and test datasets\n",
    "print(\"\\n=== Creating Train and Test Datasets ===\")\n",
    "train_dataset = PoseLiftDataset('./data/PoseLift/pose/train')\n",
    "test_dataset = PoseLiftDataset('./data/PoseLift/pose/test')\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain loader: {len(train_loader)} batches\")\n",
    "print(f\"Test loader: {len(test_loader)} batches\")\n",
    "print(\" Ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec6bc31a-cfc9-45b6-afe6-4f41e115dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PoseLiftDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading PoseLift JSON pose sequences.\n",
    "    Returns data in STG-NF compatible format: [pose_tensor, trans_index, score, label]\n",
    "    \n",
    "    JSON Structure:\n",
    "    {\n",
    "        \"person_id\": {\n",
    "            \"frame_id\": {\n",
    "                \"keypoints\": [x1, y1, conf1, ..., x17, y17, conf17],\n",
    "                \"scores\": null\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pose_json_dir, normalize=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pose_json_dir: Path to directory containing pose JSON files\n",
    "            normalize: Whether to normalize pose sequences (0-1 range per frame)\n",
    "        \"\"\"\n",
    "        self.pose_json_dir = pose_json_dir\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # Get all JSON files\n",
    "        self.pose_files = sorted([\n",
    "            os.path.join(pose_json_dir, f) \n",
    "            for f in os.listdir(pose_json_dir) \n",
    "            if f.endswith('.json')\n",
    "        ])\n",
    "        \n",
    "        print(f\"[PoseLiftDataset] Loaded {len(self.pose_files)} sequences from {pose_json_dir}\")\n",
    "        \n",
    "        if len(self.pose_files) == 0:\n",
    "            raise ValueError(f\"No JSON files found in {pose_json_dir}\")\n",
    "    \n",
    "    def _load_pose_sequence(self, json_file):\n",
    "        \"\"\"\n",
    "        Load pose sequence from JSON file and return as tensor (2, T, 17).\n",
    "        \n",
    "        Format:\n",
    "        - 2: x, y coordinates\n",
    "        - T: temporal frames\n",
    "        - 17: COCO keypoints\n",
    "        \"\"\"\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        pose_sequence = []\n",
    "        \n",
    "        # Iterate through all people\n",
    "        for person_id in sorted(data.keys(), key=lambda x: int(x)):\n",
    "            person_data = data[person_id]\n",
    "            \n",
    "            # Iterate through all frames for this person (sorted by frame ID)\n",
    "            for frame_id in sorted(person_data.keys(), key=lambda x: int(x)):\n",
    "                frame_data = person_data[frame_id]\n",
    "                \n",
    "                # Extract keypoints: [x1, y1, conf1, ..., x17, y17, conf17]\n",
    "                keypoints_flat = frame_data['keypoints']\n",
    "                \n",
    "                if keypoints_flat is None:\n",
    "                    continue\n",
    "                \n",
    "                # Convert to numpy array and reshape\n",
    "                keypoints_array = np.array(keypoints_flat, dtype=np.float32)\n",
    "                keypoints_array = keypoints_array.reshape(-1, 3)  # (17, 3) -> [x, y, conf]\n",
    "                \n",
    "                # Take only x, y (drop confidence)\n",
    "                keypoints_xy = keypoints_array[:, :2]  # (17, 2)\n",
    "                pose_sequence.append(keypoints_xy)\n",
    "        \n",
    "        if len(pose_sequence) == 0:\n",
    "            raise ValueError(f\"No valid keypoints found in {json_file}\")\n",
    "        \n",
    "        # Convert to numpy array: (T, 17, 2)\n",
    "        pose_array = np.array(pose_sequence, dtype=np.float32)\n",
    "        \n",
    "        # Validate shape\n",
    "        if pose_array.shape[1] != 17 or pose_array.shape[2] != 2:\n",
    "            raise ValueError(f\"Unexpected pose shape: {pose_array.shape}. Expected (T, 17, 2)\")\n",
    "        \n",
    "        # Normalize per frame if requested\n",
    "        if self.normalize:\n",
    "            for t in range(pose_array.shape[0]):\n",
    "                frame = pose_array[t]\n",
    "                x_min, x_max = frame[:, 0].min(), frame[:, 0].max()\n",
    "                y_min, y_max = frame[:, 1].min(), frame[:, 1].max()\n",
    "                \n",
    "                if x_max > x_min:\n",
    "                    pose_array[t, :, 0] = (frame[:, 0] - x_min) / (x_max - x_min + 1e-6)\n",
    "                if y_max > y_min:\n",
    "                    pose_array[t, :, 1] = (frame[:, 1] - y_min) / (y_max - y_min + 1e-6)\n",
    "        \n",
    "        # Handle NaN values\n",
    "        pose_array = np.nan_to_num(pose_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Convert to tensor and transpose: (T, 17, 2) -> (2, T, 17)\n",
    "        pose_tensor = torch.from_numpy(pose_array).float()\n",
    "        pose_tensor = pose_tensor.permute(2, 0, 1)  # (2, T, 17)\n",
    "        \n",
    "        return pose_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pose_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns: [pose_tensor, trans_index, score, label]\n",
    "        \n",
    "        STG-NF trainer expects:\n",
    "        - pose_tensor: shape (2, T, 17)\n",
    "        - trans_index: int (0 = no transformation)\n",
    "        - score: torch.Tensor shape (1,) - confidence score\n",
    "        - label: torch.Tensor - class label\n",
    "        \"\"\"\n",
    "        pose_file = self.pose_files[idx]\n",
    "        pose_tensor = self._load_pose_sequence(pose_file)\n",
    "        \n",
    "        trans_index = 0  # No transformation\n",
    "        score = torch.ones(1, dtype=torch.float32)  # Default score = 1.0\n",
    "        label = torch.tensor(0, dtype=torch.long)  # Default label = 0 (normal)\n",
    "        \n",
    "        return [pose_tensor, trans_index, score, label]\n",
    "\n",
    "\n",
    "def collate_fn_poselift(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for PoseLift batches.\n",
    "    Pads sequences to same length and returns trainer-compatible format.\n",
    "    \n",
    "    Input: List of [pose_tensor, trans_index, score, label]\n",
    "    Output: [poses_padded_tensor, trans_indices, scores, labels]\n",
    "    \"\"\"\n",
    "    poses = [item[0] for item in batch]  # List of (2, T, 17)\n",
    "    trans_indices = [item[1] for item in batch]\n",
    "    scores = torch.cat([item[2] for item in batch], dim=0)  # (batch_size,)\n",
    "    labels = torch.stack([item[3] for item in batch], dim=0)  # (batch_size,)\n",
    "    \n",
    "    # Find max temporal dimension\n",
    "    max_T = max(p.shape[1] for p in poses)\n",
    "    \n",
    "    # Pad all sequences to max_T\n",
    "    poses_padded = []\n",
    "    for pose in poses:\n",
    "        T = pose.shape[1]\n",
    "        if T < max_T:\n",
    "            pad = torch.zeros(2, max_T - T, 17, dtype=pose.dtype)\n",
    "            pose_padded = torch.cat([pose, pad], dim=1)\n",
    "        else:\n",
    "            pose_padded = pose\n",
    "        poses_padded.append(pose_padded)\n",
    "    \n",
    "    # Stack into tensor: (batch_size, 2, max_T, 17)\n",
    "    poses_tensor = torch.stack(poses_padded, dim=0)\n",
    "    \n",
    "    # Return in trainer format\n",
    "    return [poses_tensor, trans_indices, scores, labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18822ffb-7ea7-488e-96c8-018ad2bb820a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STG-NF Training with PoseLift Dataset (JUPYTER)\n",
      "================================================================================\n",
      "\n",
      "[1/5] Creating Datasets...\n",
      "--------------------------------------------------------------------------------\n",
      "[PoseLiftDataset] Loaded 104 sequences from ./data/PoseLift/pose/train\n",
      "[PoseLiftDataset] Loaded 47 sequences from ./data/PoseLift/pose/test\n",
      "\n",
      "[2/5] Creating DataLoaders...\n",
      "--------------------------------------------------------------------------------\n",
      " Train: 104 samples, 4 batches\n",
      " Test: 47 samples, 2 batches\n",
      "\n",
      "[3/5] Creating Arguments...\n",
      "--------------------------------------------------------------------------------\n",
      " Device: cuda\n",
      " Batch size: 32\n",
      " Learning rate: 0.001\n",
      " Epochs: 100\n",
      " Model: K=8, L=3, hidden_channels=64\n",
      " Temporal kernel (via args): 3 (MUST be odd)\n",
      "\n",
      "[4/5] Initializing Model and Trainer...\n",
      "--------------------------------------------------------------------------------\n",
      "  Creating STG-NF model...\n",
      " AssertionError: \n",
      "   Likely cause: temporal_kernel must be odd (1, 3, 5, 7, etc.)\n",
      "\n",
      "================================================================================\n",
      "Done!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Temp\\ipykernel_16920\\2539475651.py\", line 240, in <module>\n",
      "    model = STG_NF(\n",
      "            ^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 300, in __init__\n",
      "    self.flow = FlowNet(\n",
      "                ^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 241, in __init__\n",
      "    FlowStep(\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 114, in __init__\n",
      "    self.block = get_stgcn(in_channels // 2, hidden_channels, in_channels,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 45, in get_stgcn\n",
      "    st_gcn(in_channels, hidden_channels, kernel_size, 1, residual=(not first)),\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\stgcn.py\", line 97, in __init__\n",
      "    assert kernel_size[0] % 2 == 1\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete training script for STG-NF with PoseLift dataset - JUPYTER VERSION\n",
    "(Correct: temporal_kernel via args, not model init)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "# ============================================================================\n",
    "# DEFINE DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class PoseLiftDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for loading PoseLift JSON pose sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, pose_json_dir, normalize=True):\n",
    "        self.pose_json_dir = pose_json_dir\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        self.pose_files = sorted([\n",
    "            os.path.join(pose_json_dir, f) \n",
    "            for f in os.listdir(pose_json_dir) \n",
    "            if f.endswith('.json')\n",
    "        ])\n",
    "        \n",
    "        print(f\"[PoseLiftDataset] Loaded {len(self.pose_files)} sequences from {pose_json_dir}\")\n",
    "        \n",
    "        if len(self.pose_files) == 0:\n",
    "            raise ValueError(f\"No JSON files found in {pose_json_dir}\")\n",
    "    \n",
    "    def _load_pose_sequence(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        pose_sequence = []\n",
    "        \n",
    "        for person_id in sorted(data.keys(), key=lambda x: int(x)):\n",
    "            person_data = data[person_id]\n",
    "            \n",
    "            for frame_id in sorted(person_data.keys(), key=lambda x: int(x)):\n",
    "                frame_data = person_data[frame_id]\n",
    "                keypoints_flat = frame_data['keypoints']\n",
    "                \n",
    "                if keypoints_flat is None:\n",
    "                    continue\n",
    "                \n",
    "                keypoints_array = np.array(keypoints_flat, dtype=np.float32)\n",
    "                keypoints_array = keypoints_array.reshape(-1, 3)\n",
    "                keypoints_xy = keypoints_array[:, :2]\n",
    "                pose_sequence.append(keypoints_xy)\n",
    "        \n",
    "        if len(pose_sequence) == 0:\n",
    "            raise ValueError(f\"No valid keypoints found in {json_file}\")\n",
    "        \n",
    "        pose_array = np.array(pose_sequence, dtype=np.float32)\n",
    "        \n",
    "        if pose_array.shape[1] != 17 or pose_array.shape[2] != 2:\n",
    "            raise ValueError(f\"Unexpected pose shape: {pose_array.shape}. Expected (T, 17, 2)\")\n",
    "        \n",
    "        if self.normalize:\n",
    "            for t in range(pose_array.shape[0]):\n",
    "                frame = pose_array[t]\n",
    "                x_min, x_max = frame[:, 0].min(), frame[:, 0].max()\n",
    "                y_min, y_max = frame[:, 1].min(), frame[:, 1].max()\n",
    "                \n",
    "                if x_max > x_min:\n",
    "                    pose_array[t, :, 0] = (frame[:, 0] - x_min) / (x_max - x_min + 1e-6)\n",
    "                if y_max > y_min:\n",
    "                    pose_array[t, :, 1] = (frame[:, 1] - y_min) / (y_max - y_min + 1e-6)\n",
    "        \n",
    "        pose_array = np.nan_to_num(pose_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        pose_tensor = torch.from_numpy(pose_array).float()\n",
    "        pose_tensor = pose_tensor.permute(2, 0, 1)\n",
    "        \n",
    "        return pose_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pose_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pose_file = self.pose_files[idx]\n",
    "        pose_tensor = self._load_pose_sequence(pose_file)\n",
    "        \n",
    "        trans_index = 0\n",
    "        score = torch.ones(1, dtype=torch.float32)\n",
    "        label = torch.tensor(0, dtype=torch.long)\n",
    "        \n",
    "        return [pose_tensor, trans_index, score, label]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DEFINE COLLATE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def collate_fn_poselift(batch):\n",
    "    \"\"\"Custom collate function for PoseLift batches with padding.\"\"\"\n",
    "    poses = [item[0] for item in batch]\n",
    "    trans_indices = [item[1] for item in batch]\n",
    "    scores = torch.cat([item[2] for item in batch], dim=0)\n",
    "    labels = torch.stack([item[3] for item in batch], dim=0)\n",
    "    \n",
    "    max_T = max(p.shape[1] for p in poses)\n",
    "    \n",
    "    poses_padded = []\n",
    "    for pose in poses:\n",
    "        T = pose.shape[1]\n",
    "        if T < max_T:\n",
    "            pad = torch.zeros(2, max_T - T, 17, dtype=pose.dtype)\n",
    "            pose_padded = torch.cat([pose, pad], dim=1)\n",
    "        else:\n",
    "            pose_padded = pose\n",
    "        poses_padded.append(pose_padded)\n",
    "    \n",
    "    poses_tensor = torch.stack(poses_padded, dim=0)\n",
    "    \n",
    "    return [poses_tensor, trans_indices, scores, labels]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TRAINING SCRIPT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STG-NF Training with PoseLift Dataset (JUPYTER)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# STEP 1: Create datasets\n",
    "print(\"\\n[1/5] Creating Datasets...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "train_dataset = PoseLiftDataset('./data/PoseLift/pose/train', normalize=True)\n",
    "test_dataset = PoseLiftDataset('./data/PoseLift/pose/test', normalize=True)\n",
    "\n",
    "# STEP 2: Create dataloaders\n",
    "print(\"\\n[2/5] Creating DataLoaders...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_poselift,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_poselift,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\" Train: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
    "print(f\" Test: {len(test_dataset)} samples, {len(test_loader)} batches\")\n",
    "\n",
    "# STEP 3: Create complete args namespace\n",
    "print(\"\\n[3/5] Creating Arguments...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "args = Namespace(\n",
    "    # Dataset\n",
    "    dataset='poselift',\n",
    "    \n",
    "    # CRITICAL: Temporal kernel (MUST be ODD) - passed via args, NOT to model init\n",
    "    temporal_kernel=3,       # Must be 1, 3, 5, 7, etc.\n",
    "    \n",
    "    # Model architecture parameters (STG_NF requires these)\n",
    "    hidden_channels=64,      # Hidden channels for flow\n",
    "    K=8,                     # Number of flow steps\n",
    "    L=3,                     # Number of squeeze layers\n",
    "    actnorm_scale=1.0,       # Actnorm scale\n",
    "    flow_permutation='invconv',  # 'invconv' or 'shuffle' or 'reverse'\n",
    "    flow_coupling='affine',  # 'affine' or 'additive'\n",
    "    LU_decomposed=True,      # LU decomposition\n",
    "    learn_top=False,         # Learn top layer\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    lr=0.001,\n",
    "    weight_decay=0.0,\n",
    "    \n",
    "    # Model\n",
    "    model_confidence=False,\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer='adam',\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler='none',\n",
    "    \n",
    "    # Device\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Seed\n",
    "    seed=999,\n",
    "    \n",
    "    # Paths\n",
    "    cpt_dir='./checkpoints',\n",
    "    exp_dir='./runs',\n",
    "    dataset_dir='./data',\n",
    "    \n",
    "    # Model parameters\n",
    "    num_workers=0,\n",
    "    model_lr=0.001,\n",
    "    model_momentum=0.9,\n",
    "    model_weight_decay=0.0,\n",
    "    \n",
    "    # Additional params for trainer\n",
    "    evaluate=False,\n",
    "    pretrained='',\n",
    ")\n",
    "\n",
    "print(f\" Device: {args.device}\")\n",
    "print(f\" Batch size: {args.batch_size}\")\n",
    "print(f\" Learning rate: {args.lr}\")\n",
    "print(f\" Epochs: {args.epochs}\")\n",
    "print(f\" Model: K={args.K}, L={args.L}, hidden_channels={args.hidden_channels}\")\n",
    "print(f\" Temporal kernel (via args): {args.temporal_kernel} (MUST be odd)\")\n",
    "\n",
    "# STEP 4: Initialize model and trainer\n",
    "print(\"\\n[4/5] Initializing Model and Trainer...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "trainer = None\n",
    "try:\n",
    "    from models.STG_NF.model_pose import STG_NF\n",
    "    from models.training import Trainer\n",
    "    \n",
    "    # Initialize model with correct parameters\n",
    "    # NOTE: temporal_kernel is NOT passed to STG_NF, it's used by the trainer\n",
    "    print(\"  Creating STG-NF model...\")\n",
    "    model = STG_NF(\n",
    "        pose_shape=(2, 30, 17),  # (channels, temporal, keypoints)\n",
    "        hidden_channels=args.hidden_channels,\n",
    "        K=args.K,\n",
    "        L=args.L,\n",
    "        actnorm_scale=args.actnorm_scale,\n",
    "        flow_permutation=args.flow_permutation,\n",
    "        flow_coupling=args.flow_coupling,\n",
    "        LU_decomposed=args.LU_decomposed,\n",
    "        learn_top=args.learn_top\n",
    "        # NOTE: temporal_kernel is passed via args to Trainer, not here\n",
    "    )\n",
    "    print(f\"   Model created\")\n",
    "    print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(args.device)\n",
    "    print(f\"   Model moved to {args.device}\")\n",
    "    \n",
    "    # Initialize trainer with model\n",
    "    # temporal_kernel in args will be used by the trainer\n",
    "    print(\"  Creating Trainer...\")\n",
    "    trainer = Trainer(\n",
    "        args=args,\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        optimizer_f=None,\n",
    "        scheduler_f=None\n",
    "    )\n",
    "    print(\" Trainer initialized successfully\")\n",
    "    \n",
    "except AssertionError as e:\n",
    "    print(f\" AssertionError: {e}\")\n",
    "    print(\"   Likely cause: temporal_kernel must be odd (1, 3, 5, 7, etc.)\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "except Exception as e:\n",
    "    print(f\" Error initializing trainer: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# STEP 5: Train\n",
    "if trainer is not None:\n",
    "    print(\"\\n[5/5] Starting Training...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"\\n Training completed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # BONUS: Evaluate\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Evaluating on Test Set\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        scores = trainer.test()\n",
    "        print(f\" Test evaluation completed\")\n",
    "        print(f\"  Scores: {scores}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Testing failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Done!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb176ff1-39d6-405f-aa1e-078b0e104f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7fdb021-770e-4d83-91e7-3e7591d5487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STG_NF.__init__ signature:\n",
      "(self, pose_shape, hidden_channels, K, L, actnorm_scale, flow_permutation, flow_coupling, LU_decomposed, learn_top, R=0, edge_importance=False, temporal_kernel_size=None, strategy='uniform', max_hops=8, device='cuda:0')\n",
      "\n",
      "Parameters:\n",
      "  self: REQUIRED\n",
      "  pose_shape: REQUIRED\n",
      "  hidden_channels: REQUIRED\n",
      "  K: REQUIRED\n",
      "  L: REQUIRED\n",
      "  actnorm_scale: REQUIRED\n",
      "  flow_permutation: REQUIRED\n",
      "  flow_coupling: REQUIRED\n",
      "  LU_decomposed: REQUIRED\n",
      "  learn_top: REQUIRED\n",
      "  R: 0\n",
      "  edge_importance: False\n",
      "  temporal_kernel_size: None\n",
      "  strategy: uniform\n",
      "  max_hops: 8\n",
      "  device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check the actual STG_NF __init__ signature\n",
    "import inspect\n",
    "from models.STG_NF.model_pose import STG_NF\n",
    "\n",
    "sig = inspect.signature(STG_NF.__init__)\n",
    "print(\"STG_NF.__init__ signature:\")\n",
    "print(sig)\n",
    "print(\"\\nParameters:\")\n",
    "for param_name, param in sig.parameters.items():\n",
    "    print(f\"  {param_name}: {param.default if param.default != inspect.Parameter.empty else 'REQUIRED'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e9beed5-d5a9-4504-9ea5-26deb22a2730",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_kernel_size=(3, 1),  # (temporal_kernel=3, spatial_kernel=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87878a6f-0171-4a24-ba58-2c1f1f961c06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STG-NF Training with PoseLift Dataset\n",
      "================================================================================\n",
      "\n",
      "[1/3] Loading data...\n",
      "[PoseLiftDataset] Loaded 104 sequences\n",
      "[PoseLiftDataset] Loaded 47 sequences\n",
      " Train: 104 samples\n",
      " Test: 47 samples\n",
      " Device: cuda:0\n",
      "\n",
      "[2/3] Creating model...\n",
      " Model created: 321,384 params\n",
      "\n",
      "[3/3] Training...\n",
      "Starting Epoch 1 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Error: einsum(): subscript v has size 18 for operand 1 which does not broadcast with previously seen size 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Temp\\ipykernel_16920\\3052481817.py\", line 175, in <module>\n",
      "    trainer.train()\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\training.py\", line 125, in train\n",
      "    z, nll = self.model(samp.float(), label=label, score=score)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 378, in forward\n",
      "    return self.normal_flow(x, label, score)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 383, in normal_flow\n",
      "    z, objective = self.flow(x, reverse=False)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 263, in forward\n",
      "    return self.encode(input, logdet)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 268, in encode\n",
      "    z, logdet = layer(z, logdet, reverse=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 128, in forward\n",
      "    return self.normal_flow(input, logdet)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 152, in normal_flow\n",
      "    h, _ = gcn(h, self.A * importance)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\stgcn.py\", line 136, in forward\n",
      "    x, A = self.gcn(x, A)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\stgcn.py\", line 63, in forward\n",
      "    x = torch.einsum('nkctv,kvw->nctw', (x, A))\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py\", line 402, in einsum\n",
      "    return einsum(equation, *_operands)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py\", line 407, in einsum\n",
      "    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: einsum(): subscript v has size 18 for operand 1 which does not broadcast with previously seen size 17\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " FINAL WORKING CODE - FIXED KEYPOINT EXTRACTION \n",
    "STG-NF Training with PoseLift Dataset\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class PoseLiftDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pose_json_dir, normalize=True):\n",
    "        self.pose_json_dir = pose_json_dir\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        self.pose_files = sorted([\n",
    "            os.path.join(pose_json_dir, f) \n",
    "            for f in os.listdir(pose_json_dir) \n",
    "            if f.endswith('.json')\n",
    "        ])\n",
    "        \n",
    "        print(f\"[PoseLiftDataset] Loaded {len(self.pose_files)} sequences\")\n",
    "        \n",
    "        if len(self.pose_files) == 0:\n",
    "            raise ValueError(f\"No JSON files found in {pose_json_dir}\")\n",
    "    \n",
    "    def _load_pose_sequence(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        pose_sequence = []\n",
    "        \n",
    "        for person_id in sorted(data.keys(), key=lambda x: int(x)):\n",
    "            for frame_id in sorted(data[person_id].keys(), key=lambda x: int(x)):\n",
    "                keypoints_flat = data[person_id][frame_id]['keypoints']\n",
    "                \n",
    "                if keypoints_flat is None:\n",
    "                    continue\n",
    "                \n",
    "                # keypoints_flat is [x1, y1, conf1, x2, y2, conf2, ..., x17, y17, conf17]\n",
    "                # Reshape to (17, 3) then take only (x, y) -> (17, 2)\n",
    "                keypoints_array = np.array(keypoints_flat, dtype=np.float32)\n",
    "                \n",
    "                # COCO has 17 keypoints, so flat array should be 17*3=51 elements\n",
    "                if len(keypoints_array) != 51:\n",
    "                    print(f\"Warning: Expected 51 elements (17*3), got {len(keypoints_array)} in {json_file}\")\n",
    "                    # Try to handle it gracefully\n",
    "                    num_kpts = len(keypoints_array) // 3\n",
    "                    keypoints_array = keypoints_array[:num_kpts*3]\n",
    "                \n",
    "                keypoints_array = keypoints_array.reshape(-1, 3)  # (17, 3)\n",
    "                keypoints_xy = keypoints_array[:17, :2]  # Take first 17 keypoints, x,y only -> (17, 2)\n",
    "                \n",
    "                if keypoints_xy.shape != (17, 2):\n",
    "                    continue  # Skip if shape is wrong\n",
    "                \n",
    "                pose_sequence.append(keypoints_xy)\n",
    "        \n",
    "        if len(pose_sequence) == 0:\n",
    "            raise ValueError(f\"No valid keypoints in {json_file}\")\n",
    "        \n",
    "        pose_array = np.array(pose_sequence, dtype=np.float32)  # (T, 17, 2)\n",
    "        \n",
    "        if self.normalize:\n",
    "            for t in range(pose_array.shape[0]):\n",
    "                frame = pose_array[t]\n",
    "                x_min, x_max = frame[:, 0].min(), frame[:, 0].max()\n",
    "                y_min, y_max = frame[:, 1].min(), frame[:, 1].max()\n",
    "                \n",
    "                if x_max > x_min:\n",
    "                    pose_array[t, :, 0] = (frame[:, 0] - x_min) / (x_max - x_min + 1e-6)\n",
    "                if y_max > y_min:\n",
    "                    pose_array[t, :, 1] = (frame[:, 1] - y_min) / (y_max - y_min + 1e-6)\n",
    "        \n",
    "        pose_array = np.nan_to_num(pose_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Transpose to (2, T, 17): channels=2 (x,y), temporal=T, keypoints=17\n",
    "        pose_tensor = torch.from_numpy(pose_array).float().permute(2, 0, 1)\n",
    "        \n",
    "        assert pose_tensor.shape[0] == 2, f\"Expected shape (2, T, 17), got {pose_tensor.shape}\"\n",
    "        assert pose_tensor.shape[2] == 17, f\"Expected 17 keypoints, got {pose_tensor.shape[2]}\"\n",
    "        \n",
    "        return pose_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pose_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pose_tensor = self._load_pose_sequence(self.pose_files[idx])\n",
    "        return [pose_tensor, 0, torch.ones(1), torch.tensor(0, dtype=torch.long)]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Return list where all elements are tensors\"\"\"\n",
    "    poses = [item[0] for item in batch]\n",
    "    trans_indices = [item[1] for item in batch]\n",
    "    scores = torch.cat([item[2] for item in batch], dim=0)\n",
    "    labels = torch.stack([item[3] for item in batch], dim=0)\n",
    "    \n",
    "    # Pad poses to same length\n",
    "    max_T = max(p.shape[1] for p in poses)\n",
    "    poses_padded = []\n",
    "    for pose in poses:\n",
    "        T = pose.shape[1]\n",
    "        if T < max_T:\n",
    "            # Pad: (2, T, 17) -> (2, max_T, 17)\n",
    "            pose = torch.cat([pose, torch.zeros(2, max_T - T, 17)], dim=1)\n",
    "        poses_padded.append(pose)\n",
    "    \n",
    "    poses_tensor = torch.stack(poses_padded, dim=0)  # (batch, 2, max_T, 17)\n",
    "    trans_indices_tensor = torch.tensor(trans_indices, dtype=torch.long)\n",
    "    \n",
    "    return [poses_tensor, trans_indices_tensor, scores, labels]\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STG-NF Training with PoseLift Dataset\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"[1/3] Loading data...\")\n",
    "train_dataset = PoseLiftDataset('./data/PoseLift/pose/train')\n",
    "test_dataset = PoseLiftDataset('./data/PoseLift/pose/test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "print(f\" Train: {len(train_dataset)} samples\")\n",
    "print(f\" Test: {len(test_dataset)} samples\")\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\" Device: {device}\\n\")\n",
    "\n",
    "print(\"[2/3] Creating model...\")\n",
    "try:\n",
    "    from models.STG_NF.model_pose import STG_NF\n",
    "    from models.training import Trainer\n",
    "    \n",
    "    model = STG_NF(\n",
    "        pose_shape=(2, 30, 18),  # (channels, temporal, keypoints)\n",
    "        hidden_channels=64,\n",
    "        K=8, L=3,\n",
    "        actnorm_scale=1.0,\n",
    "        flow_permutation='invconv',\n",
    "        flow_coupling='affine',\n",
    "        LU_decomposed=True,\n",
    "        learn_top=False,\n",
    "        temporal_kernel_size=3,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\" Model created: {sum(p.numel() for p in model.parameters()):,} params\\n\")\n",
    "    \n",
    "    print(\"[3/3] Training...\")\n",
    "    args = Namespace(\n",
    "        dataset='poselift',\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        device=device,\n",
    "        lr=0.001,\n",
    "        weight_decay=0.0,\n",
    "        optimizer='adam',\n",
    "        scheduler='none',\n",
    "        temporal_kernel=3,\n",
    "        seed=999,\n",
    "        cpt_dir='./checkpoints',\n",
    "        exp_dir='./runs',\n",
    "        evaluate=False,\n",
    "        pretrained='',\n",
    "        model_confidence=False,\n",
    "        momentum=0.9,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(args=args, model=model, train_loader=train_loader, test_loader=test_loader)\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n Training complete! \\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3fc774fa-d009-414c-8f27-488c45daa94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STG-NF Training with PoseLift Dataset\n",
      "================================================================================\n",
      "\n",
      "[1/3] Loading data...\n",
      "[PoseLiftDataset] Loaded 104 sequences\n",
      "[PoseLiftDataset] Loaded 47 sequences\n",
      " Train: 104 samples\n",
      " Test: 47 samples\n",
      " Device: cuda:0\n",
      "\n",
      "[2/3] Creating model...\n",
      " Model created: 321,384 params\n",
      "\n",
      "[3/3] Training...\n",
      "Starting Epoch 1 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Error: einsum(): subscript v has size 18 for operand 1 which does not broadcast with previously seen size 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Temp\\ipykernel_16920\\1262337874.py\", line 170, in <module>\n",
      "    trainer.train()\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\training.py\", line 125, in train\n",
      "    z, nll = self.model(samp.float(), label=label, score=score)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 378, in forward\n",
      "    return self.normal_flow(x, label, score)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 383, in normal_flow\n",
      "    z, objective = self.flow(x, reverse=False)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 263, in forward\n",
      "    return self.encode(input, logdet)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 268, in encode\n",
      "    z, logdet = layer(z, logdet, reverse=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 128, in forward\n",
      "    return self.normal_flow(input, logdet)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\model_pose.py\", line 152, in normal_flow\n",
      "    h, _ = gcn(h, self.A * importance)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\stgcn.py\", line 136, in forward\n",
      "    x, A = self.gcn(x, A)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\PoseLift\\STG-NF\\models\\STG_NF\\stgcn.py\", line 63, in forward\n",
      "    x = torch.einsum('nkctv,kvw->nctw', (x, A))\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py\", line 402, in einsum\n",
      "    return einsum(equation, *_operands)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py\", line 407, in einsum\n",
      "    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: einsum(): subscript v has size 18 for operand 1 which does not broadcast with previously seen size 17\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " FINAL CORRECTED CODE - PROPER 17 KEYPOINT HANDLING \n",
    "STG-NF Training with PoseLift Dataset\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class PoseLiftDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pose_json_dir, normalize=True):\n",
    "        self.pose_json_dir = pose_json_dir\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        self.pose_files = sorted([\n",
    "            os.path.join(pose_json_dir, f) \n",
    "            for f in os.listdir(pose_json_dir) \n",
    "            if f.endswith('.json')\n",
    "        ])\n",
    "        \n",
    "        print(f\"[PoseLiftDataset] Loaded {len(self.pose_files)} sequences\")\n",
    "        \n",
    "        if len(self.pose_files) == 0:\n",
    "            raise ValueError(f\"No JSON files found in {pose_json_dir}\")\n",
    "    \n",
    "    def _load_pose_sequence(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        pose_sequence = []\n",
    "        \n",
    "        for person_id in sorted(data.keys(), key=lambda x: int(x)):\n",
    "            for frame_id in sorted(data[person_id].keys(), key=lambda x: int(x)):\n",
    "                keypoints_flat = data[person_id][frame_id]['keypoints']\n",
    "                \n",
    "                if keypoints_flat is None:\n",
    "                    continue\n",
    "                \n",
    "                # keypoints_flat: [x1, y1, conf1, ..., x17, y17, conf17] = 51 elements\n",
    "                keypoints_array = np.array(keypoints_flat, dtype=np.float32)\n",
    "                \n",
    "                # Reshape to (17, 3) - should have exactly 51 elements\n",
    "                if len(keypoints_array) % 3 != 0:\n",
    "                    print(f\"Warning: keypoints length {len(keypoints_array)} not divisible by 3\")\n",
    "                    continue\n",
    "                \n",
    "                num_kpts = len(keypoints_array) // 3\n",
    "                keypoints_array = keypoints_array.reshape(num_kpts, 3)\n",
    "                \n",
    "                # Take only x, y (first 2 columns), and only first 17 keypoints\n",
    "                keypoints_xy = keypoints_array[:17, :2].copy()  # (17, 2)\n",
    "                \n",
    "                if keypoints_xy.shape != (17, 2):\n",
    "                    print(f\"Warning: Expected (17, 2), got {keypoints_xy.shape}\")\n",
    "                    continue\n",
    "                \n",
    "                pose_sequence.append(keypoints_xy)\n",
    "        \n",
    "        if len(pose_sequence) == 0:\n",
    "            raise ValueError(f\"No valid keypoints in {json_file}\")\n",
    "        \n",
    "        pose_array = np.array(pose_sequence, dtype=np.float32)  # (T, 17, 2)\n",
    "        \n",
    "        if self.normalize:\n",
    "            for t in range(pose_array.shape[0]):\n",
    "                frame = pose_array[t]\n",
    "                x_min, x_max = frame[:, 0].min(), frame[:, 0].max()\n",
    "                y_min, y_max = frame[:, 1].min(), frame[:, 1].max()\n",
    "                \n",
    "                if x_max > x_min:\n",
    "                    pose_array[t, :, 0] = (frame[:, 0] - x_min) / (x_max - x_min + 1e-6)\n",
    "                if y_max > y_min:\n",
    "                    pose_array[t, :, 1] = (frame[:, 1] - y_min) / (y_max - y_min + 1e-6)\n",
    "        \n",
    "        pose_array = np.nan_to_num(pose_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Transpose to (2, T, 17)\n",
    "        pose_tensor = torch.from_numpy(pose_array).float().permute(2, 0, 1)\n",
    "        \n",
    "        return pose_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pose_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pose_tensor = self._load_pose_sequence(self.pose_files[idx])\n",
    "        return [pose_tensor, 0, torch.ones(1), torch.tensor(0, dtype=torch.long)]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    poses = [item[0] for item in batch]\n",
    "    trans_indices = [item[1] for item in batch]\n",
    "    scores = torch.cat([item[2] for item in batch], dim=0)\n",
    "    labels = torch.stack([item[3] for item in batch], dim=0)\n",
    "    \n",
    "    max_T = max(p.shape[1] for p in poses)\n",
    "    poses_padded = []\n",
    "    for pose in poses:\n",
    "        T = pose.shape[1]\n",
    "        if T < max_T:\n",
    "            pose = torch.cat([pose, torch.zeros(2, max_T - T, 17)], dim=1)\n",
    "        poses_padded.append(pose)\n",
    "    \n",
    "    poses_tensor = torch.stack(poses_padded, dim=0)\n",
    "    trans_indices_tensor = torch.tensor(trans_indices, dtype=torch.long)\n",
    "    \n",
    "    return [poses_tensor, trans_indices_tensor, scores, labels]\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STG-NF Training with PoseLift Dataset\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"[1/3] Loading data...\")\n",
    "train_dataset = PoseLiftDataset('./data/PoseLift/pose/train')\n",
    "test_dataset = PoseLiftDataset('./data/PoseLift/pose/test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "print(f\" Train: {len(train_dataset)} samples\")\n",
    "print(f\" Test: {len(test_dataset)} samples\")\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\" Device: {device}\\n\")\n",
    "\n",
    "print(\"[2/3] Creating model...\")\n",
    "try:\n",
    "    from models.STG_NF.model_pose import STG_NF\n",
    "    from models.training import Trainer\n",
    "    \n",
    "    model = STG_NF(\n",
    "        pose_shape=(2, 30, 17),  # Correct: 17 keypoints\n",
    "        hidden_channels=64,\n",
    "        K=8, L=3,\n",
    "        actnorm_scale=1.0,\n",
    "        flow_permutation='invconv',\n",
    "        flow_coupling='affine',\n",
    "        LU_decomposed=True,\n",
    "        learn_top=False,\n",
    "        temporal_kernel_size=3,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\" Model created: {sum(p.numel() for p in model.parameters()):,} params\\n\")\n",
    "    \n",
    "    print(\"[3/3] Training...\")\n",
    "    args = Namespace(\n",
    "        dataset='poselift',\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        device=device,\n",
    "        lr=0.001,\n",
    "        weight_decay=0.0,\n",
    "        optimizer='adam',\n",
    "        scheduler='none',\n",
    "        temporal_kernel=3,\n",
    "        seed=999,\n",
    "        cpt_dir='./checkpoints',\n",
    "        exp_dir='./runs',\n",
    "        evaluate=False,\n",
    "        pretrained='',\n",
    "        model_confidence=False,\n",
    "        momentum=0.9,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(args=args, model=model, train_loader=train_loader, test_loader=test_loader)\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n Training complete! \\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8affd07-9edb-4c75-b748-0960d4f32170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
